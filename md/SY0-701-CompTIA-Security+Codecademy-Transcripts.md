# SY0-701: CompTIA Security+

[TOC]

## General Security Concepts

In this track, we will delve into critical aspects of cybersecurity that are essential for any professional in the field. We’ll start by  comparing and contrasting various types of security controls, providing a clear understanding of their unique roles and applications. From there, we’ll summarize fundamental security concepts that form the backbone of effective security practices. We’ll also explore the vital importance  of change management processes, discussing how they impact security and  the overall integrity of systems. Lastly, we’ll emphasize the significance of selecting appropriate  cryptographic solutions, highlighting how they protect sensitive  information in our increasingly digital world. Join us as we navigate  these essential topics and enhance your cybersecurity expertise!

### Security Goals & Control

#### Course Overview

Security Goals and Controls. In this course, we’ll learn about confidentiality,integrity, availability, and non-repudiation. We’ll explore authentication,authorization, and accounting or AAA. We'll describe control categories and define control types.

#### The CIA Triad

In this first lesson, we're going to look at the CIA triad,which stands for confidentiality, integrity, and availability. Let's start with confidentiality. Confidentiality measures an attacker's ability to get unauthorized access to data or information from an application or a system. It involves using techniques, often cryptography,to allow only approved subjects or principals with the ability to view the information. Confidentiality includes preserving authorized restrictions on information access and disclosure: data in transit,data at rest and data in use. Confidentiality is a means for protecting personal privacy and proprietary information. Confidential information can include passwords, cryptographic keys,personally identifiable information (PII), personal health information (PHI), intellectual property (IP), or other sensitive information. Some examples of confidentiality would be: using an IPsec virtual private network (VPN); leveraging mutual Transport Layer Security (mTLS) between a web browser,for example, Google Chrome, and a web server,or a controller running in the cloud;storing sensitive data or credentials in a mobile device partition or secure enclave;and implementing AES encryption on data at rest in storage(file, block, object storage, and databases).The “I” of the CIA triad is integrity. Integrity involves safeguarding against improper information modification or even destruction. It is a property that data or information has not been altered or damaged in an unauthorized way. Integrity is the quality of an IT system that reflects: the logical correctness and reliability of the operating system;the logical completeness of the hardware and software that implements the protection mechanisms;and the consistency of the data structures and occurrence of the stored data. Examples of integrity would be: an operating system performing a mathematical checksum when a file is moved or copied from one volume to another;a frame check sequence conducted on an Ethernet frame when sent from one MAC address to another;a hashed message authentication code, or HMAC, applied to advertisements sent between neighbor systems such as routers and gateways;or implementing a mandatory access model technique such as Biba or Clark-Wilson. Availability. Availability is the process of ensuring timely and reliable access to and use of information. It is a property of data, information, applications,systems, or services that are accessible and usable upon demand by an authorized subject.“High availability” is a failover feature to ensure availability during device or component interruptions, both planned and unplanned. Availability examples include implementing security controls that protect systems and services from spoofing, flooding,denial-of-service or distributed denial-of-service, poisoning,and other attacks that negatively affect the ability to deliver data, content, or services. Vulnerabilities that impact availability can affect hardware,software and network resources, such as flooding network bandwidth,consuming large amounts of memory, CPU cycles, or unnecessary power consumption. Availability is also assuring that technical controls such as firewalls, IPS sensors, anti-virus, and endpoint protection are always reliable and deployed in a failover group or cluster. It could be determining the best disaster recovery site solution-a warm site or cloud DRP, disaster recovery planning for every scenario, or a situation for an organization. In the next lesson, we'll look at another property of security that is often added to the CIA triad to make it CIAN, and that is non-repudiation.

#### Non-repudiation

In this short lesson, we're going to learn about non-repudiation. Non-repudiation is the opposite of repudiation. It’s enforcing the inability of a subject or a principal to deny that they participated in a digital transaction, agreement,contract, or a communication such as an email. This capability was critical in the early days of the World Wide Web,when entities wanted to start doing transactions over the Internet. Non-repudiation is the property of agreeing to adhere to an obligation,and not repudiating that at some further date and time. More specifically, it is the inability to refute responsibility. For example, if you take a pen and signed a legal document,your signature is a non-repudiation device. In information technology, non-repudiation is usually accomplished with a public/private key pair cryptosystem and digitally signed certificates between the sending and receiving parties. In fact, this happens all the time when you use your web browser on the Internet. With repudiation, the sender, for example, when going to a online banking site or an online brokerage firm,cannot say, “I did not send that transfer”because their private key was used to sign the cryptographic hash of the transaction.

#### Authentication, Authorization, and Accounting

If you want to verify when certain subjects like users and applications or systems access resource objects,historically, we call that AAA- authentication,authorization and accounting. Authentication is the process of validating that an entity- an application,and end user, or a system, is who or what they claim to be. Fundamentally, we call that origin authentication. Authorization is the process of granting an authenticated entity permission to access a resource or perform a specific function. Accounting is basically: when did the entity begin,when did it end, and how long did they do it?When looking at AAA services, it’s important to realize the difference between character mode and packet mode. Character mode sends keystrokes and commands or characters to a network admission device, for example, a perimeter router, server,or a firewall for the purpose of configuration or administration on that same device. Packet or network mode occurs when the network admission device serves as an authentication proxy on behalf of services in other networks, zones, or VLANs such as web servers,FTP servers, DNS servers, SharePoint, and more. In this diagram, let's say the end user wants to get a web page from a web server in the public access zone on the right. The network access device, for example,a perimeter router or firewall is going to do packet mode AAA. They'll receive the datagram or the packet, perform the authentication,authorization and optional accounting locally,or they could send it back to a centralized authentication server. We'll talk more about that later. Once authenticated and authorized,they then send the packet to the zone, VLAN, or network. If the remote end user, however,is an administrator and wants to manage the network access device, for example,reconfigure it, then the AAA service will use character or administrator mode. Let's talk more about authentication. Authenticating subjects is technically mandatory,even if using open or anonymous techniques. For example, open authentication and 802.11 wireless or anonymous FTP. Historically, clients would initiate a TCP three-way communication handshake and then set up a connection before the authentication process. Although still done, it’s considered sub-optimal and a violation of “zero trust” principles. In that, the authentication and authorization should occur first before the connection is set up. Authorization is technically optional for authenticated entities,but it's really mandatory from a practical policy standpoint. In modern security deployments,it is desirable to implement session-based or token-based and robust attribute-based authorization mechanisms. Accounting is generally implemented for two use cases: monitoring, visibility,and reporting; and billing, chargeback, and reporting. RADIUS is one of the most popular IETF (Internet Engineering Task Force)-based AAA services, and it’s known for exceptional accounting capabilities. DIAMETER is the next generation of RADIUS.

#### Authenticating People

In this short lesson, let’s talk specifically about authenticating people as a subject. Authenticating a person entity means confirming that they are who they claim to be. This is tightly associated with identity. This confirms only those with authorized credentials can gain access to secure systems, applications, and services. Today, the most common factor for authenticating people is still a username and/or webmail and/or email with a password or passphrase. However, there should always be another robust factor added to a simple credential today. Common ways to authenticate people would be a password,a Personal Identification Number (PIN), or passphrase that they know,preferably memorized and not written down or documented;a smart card token such as Gemalto or YubiKey,or a USB fob that they possess; an X.509 v3 digital certificate in software or hardware that they present to be authenticated and authorized;or a biometric attribute such as a fingerprint or an iris or retina ocular scan;or a QR or other code that they present on an iPhone or Android device.

#### Authenticating Systems

Authenticating devices and systems.There are many different types of entitiesor principals that can be authenticated other than people.These subjects are often called “non-person entities” or NPEs: laptops and pads,mobile devices, gateways, and load balancers, robotics systems,embedded devices and Internet of Things (IoT) endpoints- including controllers,embedded systems, and special programable logic controllers.Endpoint or device authentication,is a security technique designed to ensure that only authorized devices canconnect to a given network, site, or service.Endpoint security management is rapidly emerging as an important area in the machine-to-machine (M2M) communications and the IoT (Internet of Things).On the exam, remember the term “endpoint fingerprinting”.This is one way to enable authentication of non-traditional network endpoints such as smart card readers, HVAC systems, medical equipment,and IP-enabled door locks. Some common device or endpoint authentication methods would be a shared secret key stored on the endpoint, for example,a wireless or infrastructure device;an X.509 v3 device certificate stored in a software application;a cryptographic key, certificate,or other credentials stored at the hardware level in a TPM,a trusted platform module that’s part of a trusted execution environment, or TEE; it could be a key stored in a hardware security module (HSM);or even a protected access file (PAC) in a Cisco EAP-FAST wireless infrastructure.

#### Authorization Models

In this lesson, let's talk about authorization models you must know about on the Security+ exam, starting with the discretionary access control model or DAC.DAC grants access control decisions to the resource owners and custodians. Each resource typically has an owner who determines the access permissions and shares. Now when we talk about resources, these could be endpoint devices, provisioned devices, the very commonly data. The owner can grant or revoke access rights for other users or groups if allowed in the model. DAC offers flexibility and allows resource owners to have fine-grained control over access, but it can also result in inconsistent access control decisions without proper visibility, automation, and orchestration. In fact, discretionary access control models are the most prone to "privilege creep". RBAC stands for role-based access control. In this model, you’re granting access based on a predefined job role or a job title.Users are assigned roles, and access rights are associated with these roles. For example, your job title in an organizational chart. Instead of directly assigning permissions to individual users, in an RBAC model,permissions are assigned to roles and users inherit the access rights associated with their assigned roles. For example, various roles in a hospital or medical center,for example, medical doctor, registered nurse, medical technician,and orderly; built-in roles in a database management system,for example, database administrator,backup operator, restore operator, or others. RBAC streamlines access control administration by grouping userswith similar job functions and offeringa scalable approach to access management. Remember, we don't have to use these models exclusively. For example, you can be in an environment-let’s say Active Directory- that uses a DAC model and havea global database administrators group. Then, within the relational database management system- let’s say Microsoft SQL-you could have additional role-based access controls. There's no one-size-fits-all. A mandatory access control (MAC) model is the most strict model. It’s a well-defined mathematical model where access to resources is determined by the system based on predefined security labels and rules. MAC principals are assigned security clearances or classification levels suchas top secret, secret, confidential, and others. The resource objects, files- both physical and/or logical,are labeled with sensitivity levels. Access is then granted or denied by comparing these labels and rules,ensuring strict control, and preventing unauthorized access. A MAC model is a “non-discretionary” model. For example, subjects and resource objects cannot change or grant themselves permissions at their discretion. In other words, there is no owner; everything’s done by committee. ABAC stands for attribute-based access control. Here, you grant access based on a combination of characteristics or attributes associated with users, resources, and environmental conditions. Here, you can use elements of role-based or discretionary. For example, attributes can include user attributes like job title or department; resource attributes-sensitivity level or classification; and environmental attributes-the time of access and the location or the method used,a VPN, no VPN, or other mechanism. Authorization policies are then defined using these combinations, and decisions are made based on evaluating the attributes against the defined policies. ABDAC (attribute-based dynamic access control) combines the principles of attribute-based access control (ABAC) with a dynamic access control model. It considers dynamic factors such as your risk assessment,for example, a qualitative model on a scale of 1 to 5 or 1 to 10;various user attributes; resource attributes;and additional contextual information to make access control decisions,often with an automated decision tree in real time. ABDAC provides a more fine-grained and context-aware access control,the kind needed in a “zero trust” environment,especially when compared to traditional static access control models. ABDAC may also include dynamic machine learning techniques suchas user behavioral analytics (UBA) in a next-generation environment. Next, we have rule-based access control. Rule-based access control, which may also have the same acronym as role-based,uses rules to determine access. Access control rules define conditions or criteria that must be met for access to be granted. These rules can be based on several factors such as user attributes,resource attributes, time of access, and more. But generally, they're based on location and IP address and other elements like services and port numbers. With rule-based, access decisions are made by comparing these rules against the context of the access request-usually IP transport and network layer header metadata. Here’s an example of a rule-based access control list,where you're allowing traffic based on protocol, port numbers,source and destination IP version 4 or IP version 6 address,and then of course an allow or deny decision. In addition, rule-based access control lists are often numbered starting with 100 and then going to 105 or 110, starting with the lowest number first. And once there's a match, the allow or deny decision is activated.

#### Control Categories

Security control categories: technical, managerial, operational, and physical. Let's take a look at these one at a time, starting with technical. Technical controls are security mechanisms that the specific systems run- either manually or, more often,automated and orchestrated. These controls deliver confidentiality, integrity,different levels of authenticity, and availability protections, to name a few. Technical controls defend against unauthorized access or misuse. They also facilitate the detection of security violations and support security requirements for applications and data. Some common technical controls would be infrastructure security and device hardening, identity and access (IAM) management engines,cryptographic key management and hardware security modules (HSMs),cloud-based threat modeling tools, and SIEM and SOAR systems. Managerial controls, also referred to as administrative, define policies,procedures, standard operating procedures, best practices guidelines, and security governance. Managerial controls are typically more logical in nature. A common example would be a published or printed definition of policies. For example, no piggybacking or tailgating. You have to always use your token or your badge,and you can't piggyback on somebody else's badge;acceptable use policies, for example,how you can use social media on a corporate device;best practices and guidelines,for example, awareness of common phishing attacks in corporate email;password policies- the length, the strength, the history; screening,hiring and, termination procedures; mandatory vacations;and training and awareness programs. Operational controls support the ongoing maintenance,the continual due care, and continual improvement. For example, optimizing your change and configuration management database or shifting from a relational database to a NoSQL database;performing tested patch management;the actual conducting of the security and training awareness initiative;monitoring physical and environmental controls;incident response testing and disaster planning drills and scenarios; performing software assurance initiatives;and finally, another example, your ongoing mobile device and mobile application management. Physical controls. These are introduced to protect the campus,the facility, the environment, and of course, people. It could involve various physical barriers;security guards and security teams; cameras and surveillance equipment;different types of sensors and alarms; locking mechanisms; secure safes,cabinets, cages, and secure areas; mantraps and Faraday cages; fire detection and suppression systems; and environmental controls. Make sure to memorize these categories for the Security+ exam. In the next lesson, we’ll look at different security control types.

#### Control Types

In the previous lesson, we looked at security control categories. Now we’re going to look at the six main security control types to be aware of on the exam. First, we have preventative, deterrent,the detective, corrective, compensating, and directive. Let's explore these one at a time. Preventative attacks stop an attacker from successfully conducting an exploit or advanced persistent threat. Classic examples of preventative controls would be fences, gates,and locking mechanisms. And although, practically speaking,a fence or a gate or even a lock can be overcome by brute force,we still consider them preventative. A deterrent discourages an attacker from initiating or continuing an attack. This could be a sign, a bollard, a sticker on a window,or the very presence of a security guard. A detective attack identifies an attack that is occurring as well as the steps of a kill chain. In other words, the steps that an attacker takes against a system. An intrusion detection system could be tactical, like an IPS or IDS sensor,or it could be physical, like a camera or some other monitoring system. A corrective control restores a system to a state before the negative event occurred. We call that the recovery point or the recovery point objective,and it can simply rectify or correct an identified problem. For example, after a ransomware attack,you can reimage the machine and recover the data from a backup. That’s a corrective control. A compensating control aids control that are already in place,or it's to provide a temporary stopgap solution. So, for example, it could be a next-generation endpoint protection system such as Palo Alto Cortex XDR, or it could be some stopgap measure that you put in place until you have the full budget for a comprehensive solution. A directive control is basically mandatory policies and regulations that are in place to maintain consistency and compliance. For example, a directive that states how an end user can use their provisioned pad or mobile device,or company car or truck.

#### Course Summary

In this course, you learned about confidentiality,integrity, availability, and non-repudiation. We explored authentication,authorization, and accounting or AAA. And we examined control categories and control types. Coming up in the next course, we'll look at fundamental security concepts.

### Fundamental Security Concepts

#### Course Overview

Fundamental security concepts. In this course, we will learn about gap analysis,define zero trust initiatives,explore deception technologies,examine preventive and detective physical controls,look at change management business and technical processes,and describe documentation and version control.

#### Gap Analysis

In this lesson, we’re going to talk about the term gap analysis. And gap analysis can obviously apply to a lot of different things. It just simply means the difference between an initial state and another subsequent state. Here, we're talking about gap analysis in the context of security. In order to know where you are and where you need to go as a secure organization, you must conduct gap analysis. This technique will be applied to several security projects, plans,programs, and initiatives throughout an entire security practitioner career. Information security gap analysis is a comprehensive appraisal that helps organizations determine the difference between the current state of their information security to specific industry requirements guidance and best practices,otherwise known as guidelines. When performing a security gap analysis,one will better understand the status of the cybersecurity risks and vulnerabilities in the organization. This type of risk assessment indicates where the technical, physical,managerial, and continuing operation controls need to be deployed. Gap analysis involves knowing what the residual risks are and what further physical and logical safeguards (if any)need to be acquired and implemented. Some common security gaps would be: weak and/or shared credentials;a lack of tested patch management;violation of the least privileged principle or need to know;having no/unenforced acceptable use policies; poor physical security or security gaps; configuration and deployment errors due to lack of change in configuration management;poor visibility and lack of proper auditing. In the next lesson, we'll look at one of the main activities that involves gap analysis,and that is assisting in implementing a zero trust environment.

#### Zero Trust Control Plane

Zero trust. Zero trust, or ZT, is the term for an evolving set of cybersecurity initiatives that move defenses from static, network-based perimeters (for example, based on membership in a VLAN or a subnet and your IP address)to rather focus on users, assets, and resources as the perimeter. Zero trust assumes there is no implicit trust granted to assets or user accounts based solely on their physical or network location or based on asset ownership. This is a departure from the traditional trust but verify mentality. Authentication and authorization(both subject and object) are discrete functions performed before a session toan enterprise resource is established (for example, with TCP/TLS). Zero trust embraces the principle of least privilege consistently across all resource classes and locations,both physical and logical. Segregation or separation of duties and high visibility, for instance,with a SIEM and or SOAR system are also highly emphasized in a zero trust environment, also referred to by the way, as ZTNA. Let's talk next about the zero trust adaptive identity,which is another key zero trust component. It is also called adaptive authentication or risk-based authentication. It’s a method of access to data that matches the user credentials with the risk of the requested authorization. For example, are they accessing sensitive data or intellectual property as opposed to information that’s readily available on the intranet human resources site. Adaptive identity delivers support for multiple classes of consumers and participants, whose roles and identity may evolve to meet rapidly changing ecosystems and environments. It offers ease of maintenance and operations while still being agile and easy to modify. Another main goal of ZTNA or ZT is threat scope reduction and risk avoidance. Reduce scope of threats to support agility and to support complex environments. This includes increased complexity and number of communication patterns,increasing difficulty of addressing through a data and asset-centric approach. Zero trust control plane. The ZT control plane is separate from the data plane,and it contains the policy decision point, or PDP,which includes: the policy engine (PE)which uses the enterprise policy-driven access control such as Cisco Identity Service Engine (ISE),(as well as input from external sources) to grant,deny, or revoke access to resource objects; the policy administrator (PA),enables and/or shuts down the communication path between a subject and a resource via commands to associated policy enforcement points, or PEPs; the policy administrator communicates with the PEP when creating the communication path via the control plane. Here, we can visualize the ZT architecture with the separate data plane and the separate control plane. On the left-hand side, we have the subject that could be a user, a system, it could be what’s referred to as an NPE, a non-person entity, which is sending information to a system. At this point, it's still untrusted until the policy enforcement point. For example, a multilayer switch redirects to the policy engine and policy administrator running on the PDP. Once it's authenticated and authorized,it is now trusted and can access the enterprise resource based on its authorization.

#### Zero Trust Data Plane

The zero trust data plane is defined by explicit trust zones,these trust zones could include: data centers; DMZs (demilitarized zones or public access zones); the public Internet could be a zone; cloud computing subnets such as a private or VPN-only subnet or VLAN in the cloud; and even a honeynet. There's two different types of PEPs: network PEPs, policy enforcement points could be edge routers or routers, edge firewall appliances, software-defined SDP access gateways, network layer 2 and multilayer switches, or authentication proxy servers running on Windows server, for example,or Red Hat Enterprise; application policy enforcement points would be things like API gateways that accept web socket APIs and RESTful APIs, resource groups, network VLANs, code repositories,and various trusted cloud services. Before we leave this lesson, let’s take one more look at the ZT architecture with the separate control plane with the PDP and the data plane with the PEP. And remember, the policy enforcement points could be network-based or application-based.

#### Deception Technologies

In this lesson, we’re going to talk about deception technologies,starting with the honeypot. A honeypot is a single system for example, a web server, maybe a file transfer protocol, FTP server, or perhaps Microsoft SharePoint,or it could be a resource such as a file on a server. And the design is to be attractive to potential attackers and intruders,threat actors performing advanced persistent threats, similar to a honey is eye-catching to a bear. Modern systems are often running as a virtual machine and a type 1 hypervisor such as a VMware solution, typically placed behind the customer premises equipment like your edge router or firewall appliance. They are strategically placed in parallel to public accessor demilitarized zones where public-facing servers would be typically placed. A honeynet can be simply considered a “network of honeypots.” It’s also set up with intentional vulnerabilities hosted on decoy servers and services, with the goal of attracting or primarily redirecting and slowing down attackers. Another primary purpose is to test network security by inviting attack patterns and the operation of "kill chains" by threat agents. This helps security teams analyze an actual attacker's activities and methodologies in order to improve their network security. Here, we can see an edge router connected to the internet,possibly to an internet service provider. One of its interfaces is connected to a zone on production networks. As mentioned before, those could be web servers, file servers,and often DNS servers. Then on the routers ethernet0, there's a Honeywell gateway that could be a firewall appliance. And the behind it is the honeypot network that looks like a real network,smells like a real network, but it’s not, it’s there for active defense. Other deception technologies would be honey files, also known as honey tokens. The compromised privileged insider is actually the number one internal structured threat for most organizations. Therefore, honey files and honey tokens would be strategically placed artifacts and files meant to allure the suspect into exposing themselves as part of an internal investigation. Realize we don’t use honey files and honey tokens to set the innocent employee up for failure. These are used to find suspects in an investigation they’ve already conducted data breach, data loss, data theft,or other unauthorized activities. These can also be valuable in the discovery of attackers who are already deep into the kill chain phases,for example enumeration, elevation, and lateral movement. Common examples would be access keys and credentials to valuable cloud-based assets and services,or database entry points.

#### Preventative Physical Security Controls

Preventative physical security controls. Before security professionals can focus on technical and operational countermeasures, they must be certain that these are deployed in a physically secured property, facility, and environment. Although detective and deterrent controls are important, prevention is critical for protecting all types of assets. Let’s start with fence barriers, very popular. Most organizations will have protective fence barriers around the perimeter to deter or prevent individuals from unauthorized entry and exit. Fences may also be used in certain zones or areas to protect junction boxes,generators, dumpsters, and paper shredding and pulping service pickup points, among others. Fences can be of varying heights and barbed depending upon the locale. Electrified fences and signage are also common for high security properties and facilities (e.g., airports, prisons,and military installations). Fences are often combined with entry/exit gates of varying strengthand guarding. Barricade gates and tire shredders are common, for example,in high security areas or airports where you only want traffic to go one way, think of the airport car rental areas. Types of gates in the United States would include: Class I: Residential gate operations; Class II, which are commercial, such as a parking lot or garage; Class III are industrial/limited access, you see these in warehouses, factories, and at docks, for example in various ports of call; In Class IV are restricted access operating requiring supervisory control, for example, airports, prisons, and military installations. Bollards are strategically placed pylons meant to redirect pedestrian trafficor prohibit vehicles from entering certain areas,such as the foyer of an office building. They could be permanent or temporary pillars. They are typically made of concrete or strong metal, but you could consider traffic cones as temporary bollards, as seen in the picture. High-tech bollards can be mechanical and even include cameras and other types of sensors. Access control vestibules are typically areas that are fortified with forced entry-resistant and bullet-resistant security glazing, perhaps special types of high-quality mesh windows. These fortified entryways serve as formidable barriers to unauthorized access that go beyond traditional security measures. Access control vestibules are also referred to as security or mantrap vestibules.They’re highly effective methods for hardening commercial security, typically through a series of at least two interlocking doors. With a mantrap, there is an entry and exit door, but only one door can be opened at a time. Also, with a mantrap, only one person is allowed at a time - no piggybacking or tailgating on access. There, the person can be identified and authenticated,or conduct further proofing or instant background checks. They can provide credentials such as a license or a passport. It could include biometric readers. Often, the organization will include closed-circuit television, webcams, and or intercom systems. They may have security guards, armed or unarmed, behind a bullet-proof glass in a window. Eventually, the persons allowed in through a strong door, usually with electronic locks. Access badges and cards represent another "something you have" authentication factor as seen in the picture. In fact, in this picture, it's a combination of an access badge and an access control vestibule. Many organizations will have all of their guests register at a reception area security desk,often just inside the entry to the foyer or a special area. There, they can collect and input identification information into a visitor log, distribute temporary access cards or badges, maybe have a camera station for pictures for the temporary badge. There should be a "no tailgating or piggybacking" policy at all access points for all users, regardless of their status: contractor, temporary worker, guest,or permanent employee. In the US military, they’ll often use a CAC, Common Access Card. This card will have an expiration date, a federal identifier, their affiliation, for example, the Coast Guard, their service/agency: CIA, FBI, NSA,a color indicator, their pay grade,or their sensitivity or classification level, military rank and an integrated circuit chip. Security guards are typically 24 hours a day, seven days a week,but they could just be present during business or non-business hours, often this is based on budgetary issues. A security guard is a control of multiple types: they're detective, deterrent, and preventative. They can provide rapid security response if an intrusion or incident occurs. They may be the first responder in your incident response team. Today, robot sentries are rapidly replacing humans in certain scenarios. Some considerations for security guards would be are they hired, contracted or are they freelance individuals? Do they need to be certified and or licensed? Will they be armed or unarmed? What does that impact on your insurance policy? Armed guards could invoke higher premiums. Is the organization directly involved with the screening and the background checks of the guards? Where are they stationed on the campus or the facility? And who provides the ongoing training?

#### Detective Physical Security Controls

In this lesson, we'll continue our survey of physical security controls,starting with video surveillance. Cameras and video surveillance provide a way to monitor and record the property perimeter for intruders and potential attackers. They are also considered detective physical controls, but the mere presence may also be a deterrent. Video surveillance offers a way to record intruders in action with recordings. Alerts should be triggered when a camera is disabled. The systems will typically be indoor and outdoor webcams or closed-circuit television systems, CCTV. They may be closed-circuit to a security operations center (SOC) or linked to a third-party vendor. It's imperative to transfer the media to a safe and secure location. In addition, industrial camouflage involves cameras and surveillance devices hidden in landscaping elements, statues, tall trees, and even fake trees. Video cameras should be combined with various lighting solutions, even though, keep in mind cameras can have nighttime detection capabilities like lighting, they can both have "dead spots." Lighting can enhance other security controls, such as cameras,security guards, and sensors. They should start at the perimeter and be used in every defense-in-depth mechanism. Some modes of lighting can be mercury vapor, (for example, the ones you see in stadiums) sodium vapor, quartz, and popular LEDs. Continuous lighting is the most familiar form of outdoor security lighting and can provide greater protection and control. The glare of continuous or barrier lighting originated in prisons and correctional institutes and is still in use today in high-security environments. Stand-by lighting systems are designed for reserve or stand-by use or to supplement continuous systems. These systems are engaged either automatically or manually when the continuous system is inoperative or when there is a need for extra light. Movable lighting hardware is manually operated and typically made up of moveable search or flood lights located in chosen places, which require temporary lighting. The movable system is also used to supplement continuous or stand-by lighting and is often used at construction sites. Emergency lights are used in times of power failure or other emergencies when other systems are inoperative. These are often gas-powered generators or battery-driven emergency lights. Although cameras are a form of sensor, photoelectric sensors act upon a break in a light beam. Passive infrared will detect infrared light, either diffused or line of sight. Vibration sensors detect a change in the level of vibration, for example, somebody walking on the floor of a museum. Acoustical uses noise detection of change in sound waves. Microwave looks for a change in high-frequency radio waves. Very common in home and small business, alarm systems would be electro-mechanical, where the window or door could have a break in the electrical circuit. Those are often combined with photoelectric or a break in a light beam. Electrostatic will sense changes in electrostatic fields. You also have sensors for moisture and temperature detection - for server rooms and data center environmental controls. Sensors trigger alarms. The alarm could be a static or flashing light on the display panel in the security room or security operations center or at a security guard's desk. It could be bells ringing or horns blaring at loud decibels. Sending a text notification to an interested party. Sending an email message to an administrator. A silent alarm to a security firm or local law enforcement. A sensor could trigger a telephone or cellular call to a software program or live attendant.

#### Change Management Business Processes

Let's step away from physical security and change gears and talk about another very important security principle and that is change management. Change management is the methodical approach to handling the transition or modification of an organization's goals, processes, configurations, or technologies. The purpose of CM is to implement strategies for carrying out change, controlling transformations, and various migrations, as well as assisting individuals in adapting to change. Change management is also referred to as the change control practice. Typically, configuration management occurs first to establish a baseline before standard, normal, or emergency changes occur. Here, we see the change management lifecycle. Phase 1 is submitting, and phase 2 is approving, these are highly iterative processes, in other words, there can be quite a bit of back and forth between phase 1 and phase 2 happening, and then the documentation and testing occur before implementing, and of course, as always, an after-action report. Let's look at some change control business processes. The approval process should be flexible and highly iterative phases of the lifecycle. Ownership of the physical or logical asset must be considered, as well as a custodian or controller, and it's driven by your access control model. All stakeholders should be involved based on the RACI or RACI model: responsible, accountable, consulted, and informed. There’s always at least one responsible, and maybe more, there’s always only one accountable. Consult, that involves a two-way communication, where the consulted party sends feedback and informed is simply a notification. A change impact analysis compares two states (the current and future state) of a change to identify what is changing, who is impacted by the change, and what needs to be communicated to the impacted. This is also referred to as a gap analysis. The change impact analysis involves identifying and categorizing who and what will be affected, assessing the degree of change occurring within these areas, and a complete and concise description of the change. This last activity will fold into stakeholder analysis, communication analysis, and following through on strategies. A change backout, rollback, or fallback plan is a recovery point, and it must be in place before both the testing plan and implementation phase. Remember, make small individual changes instead of several impactful changes. A maintenance window is typically used to show the times during which changes should be scheduled, often by a project manager or a program manager. An SOP is a standard operating procedure, and this offers precise directions and detailed instructions necessary to perform a specific task or operation consistently and proficiently. Remember, the more automated and orchestrated an SOP, the more successful it will be in the long run.

#### Change Management Technical Implications

In this brief lesson, we want to look at some of the technical implications of change management from the standpoint of a security practitioner. Often, allow/deny lists are used with change control in line with the access control model, such as discretionary, to dictate which subjects who are allowed to make changes or not. An allow list is a permissive control, whereas the deny list is a restrictive control. By implementing least privilege and separation of duties, certain change management activities and areas will be restricted. Downtime relates to high availability, which is an aspect of resiliency. Availability consists of planned and unplanned downtime (for example, an outage, a blackout, or a brownout) and must be considered with technical change management when making modifications or performing migrations. Other technical considerations of change management are the service restart process or application restarts, remembering the entire state machine must be secure, working with legacy applications and all of their dependencies and interoperabilities. In the next lesson, we'll discuss the importance of documentation and version control.

#### Documentation and Version Control

Documentation and version control. Organizations must document using a well-established tagging and labeling schema that maps to a configuration management database, a CMDB, or a software as a service solution like ServiceNow. A configuration management system, a CMS, is a set of data, tools, utilities, and processes that are used to support configuration management. Relational databases have been used historically. However, NoSQL and document databases are emerging as a common solution. You could leverage a cloud security provider service such as Amazon Redshift, data warehousing. Remember, even though you’re using a data warehousing solution, typically, a configuration management database is not your ordinary data warehouse, it’s a special use case, often something provided as a software as a service solution. It plays a critical role in several IT management initiatives, such as IT service management and IT asset management. It helps various IT services to better align with business needs by offering current and accurate data for change and patch management, incident and problem management, availability management, and release and deployment management. Version control and change management procedures are important to both the operations team and the security team. Version control applies to operating system builds, application updates,device drivers, licensing updates, various upgrades and patches, container packages and microservices, and basically code in general, firmware updates, trusted platform modules, and component updates.

#### Course Summary

In this course, you learned about gap analysis and zero trust initiatives, deception technologies like honeypots and honeynets, as well as preventative and detective physical controls, and change management business and technical processes, documentation, and the importance of version control. In the next course, you’ll learn about practical cryptography.

### Practical Cryptography

#### Course Overview

Practical cryptography. In this course, we will compare symmetric and asymmetric cryptography. Learn about encryption levels as in full disk,partition, file, volume, database, and record. Examine hashing, salting, HMACs, and key exchange. Explore digital signatures, certificates, and public key infrastructure (PKI). We will observe various cryptographic tools and understand blockchain technology.

#### Symmetric Cryptography

Let's begin by looking at cryptographic services or security services that use cryptographic mechanisms. We can use cryptography for confidentiality. For example, using encryption or other mechanisms to hide the data at rest,the data in transit, or even the data in use from unauthorized principles. This typically involves a system or algorithm that converts plaintext data into ciphertext. It can also use hashing mechanisms, which we’ll talk about later on. We can also get integrity services. This ensures that data hasn’t been altered while at rest or in transit. And non-repudiation. Ensuring the original sender deny sending data or engaging in a digital transaction such as a contract,a financial transaction, or some other agreement. Since the beginning of recorded history, people have used symmetric key cryptosystems. This historic form uses the same key to encrypt and decrypt. Symmetric key algorithms are efficient, they’re fast,and we often use them to handle high data rates of throughput. They’re computationally, relatively inexpensive, and they deploy shorter key lengths from 40 to 512 bits. However, anything below 128 bits should probably be avoided. And they’re primarily used to protect data at rest or for temporary session keys. With the symmetric key cryptosystem, historically, the key management is more complex unless we’re using modern hardware security modules (HSMs)or a cloud-based key management service, such as AWS KMS. Realize there is no built-in origin authentication,and symmetric systems do not scale well unless a cloud key management service is used. The most popular algorithms are Advanced Encryption Standard (AES)in CBC mode or AES-Galois/Counter Mode (GCM)-128 or 256 bits. Symmetric key algorithms function in a couple of different ways,either as a block cipher or a stream cipher. With the block cipher, the cryptographic algorithm operates on fixed blocks of data or bits based on a key size,for example, 128, 256, 384, or 512. Messages bigger than the key size are broken into blocks the size of the key and the final block must include padding to get it up to the 128 or 256-bit key space. Common block ciphers are DES, 3DES-EDE, AES-CBC, AES-GCM, and Blowfish. In most modern implementations, DES and 3DES have been deprecated. With a stream cipher, you operate on a continuous stream of plaintext data by encrypting one bit or byte at a time. Plaintext bits are typically XORed with keystream bits. A keystream would be random bits, bytes, numbers, and/or characters. Stream ciphers are faster and less complex than block ciphers,and modern ciphers can work both in block or stream mode or both.Block being the most common. Examples would be FISH, CryptMT, Scream, and cryptographic hashes. Here's a simple example of a stream cipher. Alice wants to use a stream cipher to encrypt the letter “A”.In ASCII, the letter “A” has the value of 65 or 1000001. The first cipher stream bits are 0101100. We perform an XOR function, also known as Modulo 2 addition. Here’s the original bits representing the letter A. The XOR function on the stream bits and the result is 1101101.It just so happens that’s an ASCII value of 109,which is actually text “m” on the ASCII table. So, using our stream cipher, the letter “A” becomes “m”. Now that you know about symmetric key cryptosystems and either block or stream mode,in the next lesson, we’ll look at asymmetric key cryptosystems.

#### Assymetric Cryptography

Asymmetric key cryptosystems use a mathematically related pair of a public key and a private key. You could think of them as fraternal twins. If one is used to encrypt,the other is used to decrypt, and vice versa. Public key infrastructure (PKI) enables efficient key management and scalability of asymmetric keys. These algorithms are often used for digital signatures and key exchange, and they employ longer key lengths and symmetric, up to 4096 as of 2024. However, they are slower and more computationally expensive. Let's look at Alice and Bob. Let’s say Alice wants to use an asymmetric key cryptosystem to send a message to Bob,and him have a high degree of confidence to employ confidentiality. If she wants to send this confidential message to Bob or encrypted message, she will get Bob’s public key. She can do that in a handshake protocol,or out-of-band, or in a digital certificate. She will encrypt the message with Bob's public key, and then he’ll use his mathematically related key pair private key to decrypt the message. If Alice wants to send a message to Bob and show that the message has an authentic origin being Alice,she will encrypt the message with her private key. Then, she’ll get Bob her related public key, either in a certificate or a handshake protocol, or some type of out-of-band method. When Bob gets the encrypted message that was encrypted with Alice’s private key, he can use her public key to decrypt it. Now remember, all Bob really knows here is that Alice’s private key was used to encrypt the message. So this is a fundamental basic form of origin authentication. Bob is not 100% certain that it was Alice that used her own private key. We have to use more robust identity mechanisms and often multiple factors to give Bob a higher degree of assurance that it was actually Alice that used the private key. Often, Alice and Bob will rely on a trusted third party,for example, a public key infrastructure certificate authority. Some popular asymmetric or public key algorithms would be RSA (Rivest-Shamir-Adleman).This is the most widely used algorithm for securing communication and data encryption. Diffie-Hellman key exchange. This is a protocol for securely exchanging cryptographic keys over an untrusted network. Now, realize RSA can also exchange cryptographic keys,but often will use RSA for the security aspect and Diffie-Hellman for the key exchange. In more modern implementations, because of mobile devices,especially devices and IoT (Internet of Things)will use elliptic curve cryptography or ECC. This is an algorithm based on the algebraic structure of elliptic curves over finite fields. ECC is very popular. Or DSA (digital signature algorithm). Often used by government agencies, it’s a standard based on the mathematical concept of modular exponentiation and discrete logarithm problems. Also, realize that you can do DSA in an elliptic curve mode that would be Elliptic Curve DSA. 

#### Encryption Levels

Let's explore some different types of encryption,starting with full disk encryption. FDE is the process of encoding all user data on a device using an encrypted key. This is also called whole disk encryption. The master boot record (MBR) (or comparable) that includes code that loads the operating system is not encrypted. Once the device, a hard disk drive, a solid-state drive,or even a removable USB or FireWire drive is encrypted,all of the user-created data is automatically encrypted before committing it to disk. There’s also partition encryption. Encrypted partitions are more granular. These are smaller disk partitions that are protected with encryption keys to prevent unauthorized access to the data in the partition on the hard drive. One advantage of encrypting only a partition instead of the whole drive is that you can encrypt and decrypt the partition while using the system for other tasks. If one only encrypts a data partition, however,sensitive data can remain in temporary files or swap files in a non-encrypted partition. We often call this data remanence. And of course, we can just simply do file encryption. File-level encryption enables the protection of individual files by encrypting them. This technique is often utilized when there are specific files that need an extra degree of security or contain very sensitive information. Encrypting individual files offers more control over access and assures that even if one file is cracked, the others will still be safe. Volume or block encryption targets a section of the physical drive, which is defined as a separate partition or volume. It provides a choice to encrypt different volumes,whereas with disk encryption, you can only encrypt everything. Volume encryption can help save time and provide greater flexibility. If a single volume occupies the entire hard drive, then volume encryption will actually function the same way as full disk encryption. Database and database record encryption. Database encryption is the process of using an algorithm to transform data stored in a database into unreadable cipher text. This can often involve more than one key. For example, on a data warehouse cluster, you can often use 3 or 4 keys. The purpose is to protect the data stored in various platforms from being accessed by external attackers or even compromised privileged insiders. When using a cloud database service, key management services are often used. Record encryption will encrypt and decrypt the individual records in a database system. Something that’s also done to data stored in databases is hashing information. In the next lesson, we’ll explore cryptographic hashing.

#### Hashing, Salting, and HMACs

Cryptographic hashing. Cryptographic hashing is a one-way mathematical function,often referred to as a trapdoor, that produces a digest,typically of 128 bits to 512 bits. As you can see in the diagram, hashing converts data of virtually any input size to a fixed-length string called a hash value or message digest or fingerprint. Cryptographic hashing is actually an advanced version of a simple checksum. The type of thing done by your operating system when you copy or move a file or frames on an Ethernet. Cryptographic hashing is subject to the birthday paradox and the avalanche effect. Because of the birthday paradox, a cryptographic hash is only effectively as strong as half of its bit size. So, for example, a 512-bit cryptographic hash is actually only 256 bits because of this paradox.The avalanche effect means that if even one bit is flipped in the origin data,the fixed-length hash will be completely different, like an avalanche. Cryptographic hashing is used in authentication, data integrity, non-repudiation, fingerprinting, password storage, and database indexing. In addition, cryptographic hashes must be collision resistant, which means two different inputs must not create the same fixed-length hash or digest. That’s why we no longer use MD5 in any new security situations. Some common hash functions would be RIPEMD, which has several different bit sizes, SHA-1, which produces a 160-bit digest. The very popular SHA-2 being SHA256 or SHA512. The rapidly emerging SHA-3, which has bit sizes from 224 to 512,and Whirlpool, which is a modification of the AES algorithm. Salting is the technique of adding pseudorandom data to a cryptographic hash function. The goal is to make it less deterministic for cracking tools. When an attacker can access a database of password hashes,they can use either hash tables or what are referred to as rainbow tables,to look up the matching hashes, which they can use to discover the passwords or other hashed data. Two weaknesses are that salts are too short, or if they aren’t unique for each password. In the diagram, we see that the original data is “Hello”, the salt is “3ab9”.It then goes through the hash function which stores the hash and the salt. Let’s talk about Hash-based Message Authentication Codes or HMACs, which are used for integrity and origin authentication.An HMAC is simply a hash function with a shared secret key or shared alphanumeric string. This has many applications. But in this example, we’re going to look at two different routers who are sending route updates to each other,their peers, or neighbors in a dynamic routing protocol. And administrators want to make sure that only proper updates are sent between the routers. In other words, authenticating the origin of the routing update and maintaining integrity. In this example, the routing update information,which is varying length, is added to a key, and the data and the key go through the hash function.The result is appended to the routing update. Then the receiving router, using the same cryptographic hash and the same key,can trust that integrity was maintained and that the origin router was authentic.

#### Key Exchange

Historically, one of the biggest challenges with cryptosystems is managing keys, especially key exchange. And this is highly applicable for symmetric keys and symmetric key exchange. And there's several ways for parties to exchange keys, both symmetric and asymmetric, either using a phone call or a text message, sending secured email, a courier, diplomatic bags, or some other out-of-band method. Alternatively, a more effective method is using an asymmetric key exchange algorithm. RSA key exchange, Diffie-Hellman key exchange, Elliptic Curve Diffie-Hellman,and the newest Elliptic Curve Diffie-Hellman Ephemeral. Diffie-Hellman key exchange or DHKE and RSA key transport are original protocols created for establishing secret keys between two parties over an unsecured channel, such as the public Internet. Diffie-Hellman is a widely used asymmetric cryptosystem found in Secure Shell 2, Transport Layer Security (TLS), and IP security (IPsec). Diffie-Hellman represents an impressive application of the discrete logarithm problem. The RSA algorithm can sign public-key certificates, whereas the Diffie-Hellman key exchange cannot. The basic concept of DHKE is at the bottom. Generating a shared color between two parties over an untrusted network that a third party or a meet-in-the-middle or on-path attacker could not determine. In this exchange, here using colors, the attacker only sees the common colors and cannot arrive at the shared secret color. There are several modes of Diffie-Hellman.The original is simply called Diffie-Hellmanor Diffie-Hellman key exchange (DHKE).Here, the same shared secret is used all the time between the parties.This is rarely used today.Then we have Ephemeral Diffie-Hellman. With this, a different shared key is used each time between parties that can also use a principle known as perfect forward secrecy. It’s very common today with mobile devices, Internet of Things,and other smart devices to use Elliptic Curve Diffie-Hellman. This uses an elliptic curve public/private key pair and the same shared secret is used all the time between parties. The preferred Elliptic Curve Ephemeral Diffie-Hellman or Elliptic Curve Diffie-Hellman Ephemeral uses an EC public/private key pair, but a different shared secret is used each time or for each session. Elliptic Curve Diffie-Hellman Ephemeral is based on rich math functions of values plotted on an elliptic curve. You can see that on the left.Historically, one of the biggest challenges with cryptosystemsis managing keys, especially key exchange.And this is highly applicable for symmetric keys and symmetric key exchange.And there's several ways for parties to exchange keys,both symmetric and asymmetric,either using a phone call or a text message,sending secured email, a courier, diplomatic bags,or some other out-of-band method.Alternatively, a more effective method is usingan asymmetric key exchange algorithm.RSA key exchange, Diffie-Hellman key exchange, Elliptic Curve Diffie-Hellman,and the newest Elliptic Curve Diffie-Hellman Ephemeral.Diffie-Hellman key exchange or DHKE and RSA key transportare original protocols created for establishing secret keysbetween two parties over an unsecured channel, such as the public Internet.Diffie-Hellman is a widely used asymmetric cryptosystem found inSecure Shell 2, Transport Layer Security (TLS), and IP security (IPsec).Diffie-Hellman represents an impressive applicationof the discrete logarithm problem.The RSA algorithm can sign public-key certificates,whereas the Diffie-Hellman key exchange cannot.The basic concept of DHKE is at the bottom.Generating a shared color between two parties over anuntrusted network that a third party or a meet-in-the-middleor on-path attacker could not determine.In this exchange, here using colors, the attacker only sees the common colorsand cannot arrive at the shared secret color.There are several modes of Diffie-Hellman.The original is simply called Diffie-Hellmanor Diffie-Hellman key exchange (DHKE).Here, the same shared secret is used all the time between the parties.This is rarely used today.Then we have Ephemeral Diffie-Hellman.With this, a different shared key is used each time between partiesthat can also use a principle known as perfect forward secrecy.It’s very common today with mobile devices, Internet of Things,and other smart devices to use Elliptic Curve Diffie-Hellman.This uses an elliptic curve public/private key pairand the same shared secret is used all the time between parties.The preferred Elliptic Curve Ephemeral Diffie-Hellmanor Elliptic Curve Diffie-Hellman Ephemeral uses an EC public/private key pair,but a different shared secret is used each time or for each session.Elliptic Curve Diffie-Hellman Ephemeral is based onrich math functions of values plotted on an elliptic curve. You can see that on the left. For example, a 256-bit elliptic key = 3072-bit traditional standard key. This solution is excellent for mobile devices, smart devices,and IoT that have limited memory and limited processing power. Common use cases would be for key exchange. It’s used in IPsec, IP version 2,and Transport Layer Security, and digital signatures. In this lesson, I’ve mentioned digital signatures a couple of times. In the next lesson, I’ll actually explain,and we’ll explore the very popular digital signatures. For example, a 256-bit elliptic key = 3072-bit traditional standard key. This solution is excellent for mobile devices, smart devices,and IoT that have limited memory and limited processing power. Common use cases would be for key exchange. It’s used in IPsec, IP version 2,and Transport Layer Security, and digital signatures. In this lesson, I’ve mentioned digital signatures a couple of times. In the next lesson, I’ll actually explain,and we’ll explore the very popular digital signatures.

#### Digital Signatures and Certificates

Digital signatures are a scalable mechanism for offering authenticity, integrity, and non-repudiation using random public/private key pairs. Keep in mind that a digital signature does not offer confidentiality or privacy. Digital signatures are legally equivalent to a handwritten signature in many countries, including the United States. For digital signature hashing, SHA1, SHA2,or the SHA3 family are commonly used. For the signing or encryption aspect, RSA, DSA, and Elliptic Curve DSA are commonly used. Let’s take a look at digitally signing an Application Programming Interface call or request, which is very common. Whether you’re in a software-defined network or a software-defined data center or operating with a cloud provider,it's common to use an API. In this example, the API is going to go through a SHA-2 hash that’s going to result in a fingerprint or a digest, regardless of how big the API is. Then the sender, who’s generated a private and public key pair will use their private key, which in this case is RSA, to encrypt the digital hash. The encrypted digital hash is then appended to the API and sent over the untrusted network. Again, we’re getting integrity. We’re getting origin authentication because of the private key and non-repudiation. The sender cannot come back at a later time and repudiate that they didn’t send the API because their private key was used to sign the hash or encrypt the hash,and it’s their responsibility to protect that private key. If they can't, they must rely on a trusted third party.For example, a public certificate authority like Entrust, GeoTrust, VeriSign, or GoDaddy. The recipient who has the corresponding public key of the sender,also knows the hash function that was used,and of course, the protocol that generated the public and private key RSA. They can then unpackage and view the API. A digital certificate is a form of file used to bind cryptographic key pairs, for example, the aforementioned RSA, to entities such as individuals, websites, devices, and/or even organizations. If validity affirmation and/or public trust is needed,then a trusted certificate authority (CA) will assume the role of a third party to validate, identify, and associate them with cryptographic pairs using the digital certificates. As mentioned, the key pair consists of a public key and a private key. The public key is included in the certificate, while the private key is stored in a secure fashion. The owner of the private key can then use it to sign documents, and the public key can be used to verify the validity of those signatures. A common format for digital certificates is based on the X.509 standard. Specifically, X.509 version 3, which is just simply a certificate of fields of metadata about the entity that has a public key. Let's look at some of the important fields of the certificate. First, there’s a version number which we’ve established as version 3. There's also a serial number which isn't actually a serial number because it's not produced in series. It’s actually a really large pseudorandom number. As you can see in the screenshot, the serial number is so large it has to be expressed in hexadecimal. There’s also a signature algorithm ID, in other words, what was used to digitally sign this certificate and that was usually done by a certificate authority. Either an enterprise CA such as Microsoft or Red Hat Enterprise, or a public CA like VeriSign or Thawte. Who issued this? In this example, it was GeoTrust. What’s the validity period? It is valid from a certain date and time and valid to a certain date and time. Often, it’s one year, three years, or five years. It’s not valid before, and it’s not valid after. Also, the subject name. Now, in this particular example,the subject name is CompTIA.org, but actually that field is rarely used anymore.The subject alternative name is actually the field that is used and the reason why is that field supports IP version 6 addresses and that’s very important now and into the future. There's information about the subject's public key, including the public key algorithm and the actual subject's public key. There’ll be a unique identifier for the issuer, in this case, GeoTrust. The subject’s unique identifier, which may be a fully qualified domain name or an email address or an IPv6 address. Extensions are very important because those are used to add functionality,specifically, better security to digital certificates. There is the algorithm used to sign the certificate, and finally, the certificate signature.

#### Public Key Infrastructure

One thing that makes an asymmetric key cryptosystem actually work and be scalable is having a trusted third party. For example, Alice applying for a driver's license from a government agency. Then, when she receives the license, she can use it to prove her identity in a wide variety of use cases. For example, when she tries to get a mortgage, her identity is accepted in the mortgage application after her driver’s license is checked by the bank against the trusted third party, which in this case, is the government entity that issued her driver’s license. The public key infrastructure is really taking this concept ofa trusted third party to a potentially global scale. A PKI is a scalable binding of a public key with the identity of an entity, which could be a person, or a system, or a device, or even an organization. In this example, we've got two users who want to communicate, they want to perform transactions, for example,and they want integrity, origin authentication, and non-repudiation. Let’s say in this example, user A and user C happen to use the same certificate authority or CA. Either the user, user A, or on their behalf, the certificate authority will generate a private/public key pair and the same thing goes for user C. Now, the certificate authority, they also have a public/private key pair as well. Digital certificates are registered and issued by the certificate authority. This can be automated through an enrollment protocol,or it can be done manually. More often it’s automated. And as mentioned, the CA may also generate the key pair. For example, RSA 2048, on behalf of the requesting party and include that in the certificate. The certificate authority is the central trusted introducer. But they’re also responsible for letting participants of the infrastructure know if for some reason, that public/private key pair and/or certificate can no longer be trusted. The CA can store, issue, and digitally sign certificates of customers all over the world. Everyone has the public key of the CA or a trusted CA in some type of store. It could be part of their browser or part of their operating system on their device or it could be both. A certificate signing request (CSR) is used by the enrolling party and then the certificate is granted by the CA. The enrollment request is usually automated based on the network operating system or directory service. There are several different trust models of certificate authorities. And remember these could be a local enterprise CA or it could be something public on the Internet. You could have a single CA for your small to medium-sized business. If it’s an enterprise CA, it’s responsible for directly providing certificates to everyone in the enterprise,and it often uses something like EAP-TLS on all the devices, the clients, and the servers. This model, the CA, must always be online, and it's usually Windows Server or Red Hat Enterprise. A hierarchical CA, like you would see with a public PKI CA, would have a combination of root CAs and intermediate CAs, kind of like the way the Domain Name System works. Root servers and intermediate servers. Intermediate servers are the ones that do all the work. The root will send certificates to the intermediates. The intermediate CAs provide certificates and a chain to the users and other intermediate CAs.In a hierarchical model, the root can be online,or it’s generally taken offline almost all of the time and only brought online for a short period of time to push information to the intermediates. If it’s online, it’s connected to the network,and it issues certificates over the network. If it’s offline all the time, it’s not connected,and it issues certificates on removable media. But as I mentioned, often administrators will leave them offline and then bring them online for a short period of time. So in a nutshell, a certificate authority that’s using PKI has two main purposes. One is to globally distribute in a secure fashion public keys in certificates. The other is, if necessary, they have to suspend or revoke a certificate. Certificates are stamped with non-deterministic serial numbers, in other words, they’re pseudorandom, and the validity dates. For security reasons, all keys must have a finite life due to brute-force attacks. A certificate can be revoked. That’s permanent. It’ll never be used again, or it can be suspended or held. This is temporary and can be reactivated. If it’s revoked, it’s going to be placed in the certificate revocation list, the CRL, that’s the original method for revoking certificates,and it's still used today. The more popular solution and the one used on the public Internet is Online Certificate Status Protocol or OCSP. This is basically a transactional database version of the CRL. It’s an Internet-enabled transactional database that certificate authorities,cloud service providers, web servers,and web applications utilize for suspension and revocation.

#### Cryptographic Tools

When an endpoint or device is using an X.509v3 certificate, it’s often stored in a Trusted Platform Module or TPM. A TPM is used to improve the security of various systems, such as servers and PCs, as well as mobile devices. Microsoft uses services like BitLocker Drive Encryption, Windows Hello, and others to securely create and store cryptographic keys. A TPM is often a separate chip on the motherboard or system board, TPM 2.0, that allows manufacturers to build the capability into their chipsets rather than requiring a separate chip or component or even software. For example, Google employees store X.509v3 certificates in TPMs in their devices as part of a zero trust initiative. Hardware security modules or HSMs have historically been hardened, tamper-resistant dedicated appliances or integrated modules into a server or workstation. Today, HSMs can be physical, or they can be virtualized in a hypervisor. We even have SmartCard-HSM. This is a lightweight hardware security module in a smart card, MicroSD,or USB form factor, providing a remotely manageable secure RSA and Elliptic Curve Cryptography key solution. Responsibilities of HSMs include managing,processing, generating, and storing keys, both symmetric and asymmetric. Verifying digital certificates. Accelerating SSL/TLS connections. In other words, offloading the processing of SSL/TLS. They can encrypt sensitive data and store it and verify the integrity of stored data. A cloud-based key management service, such as AWS KMS, is a managed service that enables the creation and control of customer-managed symmetric and asymmetric cryptographic keys to protect various types of data at rest or, in some cases, data in transit. These key services integrate with many other cloud services,such as block storage, object or blob storage, applications,and databases to facilitate the encryption of critical data. Another term for the exam is key stretching. Tools such as PBKDF2 apply a pseudorandom function, such as an HMAC,to the input password or passphrase along with a salt value.This key stretching tool then repeats the process many times, for example, 1000 iterations, to produce a derived key,which can then be used as a cryptographic key in further operations. The stretching process makes password cracking much more difficult, if not virtually impossible. Today, programs will use hundreds of thousands of iterations due to the fast processors involved. Secure enclaves. A secure enclave delivers CPU hardware-level isolation and memory encryption on a server, workstation,or a popular solution is on a mobile device, such as the iPhone,and it does this by isolating application code and data from anyone with privileges and encrypting its memory. With additional software, secure enclaves enable the encryption of both storage and network data for simple full-stack security. Secure enclave hardware support is built into all new CPUs from Intel and AMD, including proprietary CPUs. The secure enclave is a hardware feature of most versions of iPhone,iPad, Mac, Apple TV, and the popular Apple Watch. Steganography is the process of hiding a secret message inside of or even on top of something that is not secret. Tools like Steghide often involve embeddinga secret piece of text inside of a picture or hiding secret messages or scripts inside of multiple Word, Excel, or PDF documents. It is a form of covert communication but not a form of cryptography because it doesn't involve scrambling data or using a key. Steganography is a practice that enables secrecy and even deceit. For example, things can be hidden in YouTube videos, and they are all the time all over the world. Data masking often involves using characters like an “X”or an asterisk (*) to hide some or all of the data. For example, only displaying the last four digits of Social Security numbers, credit card numbers, national ID numbers, bank account numbers,or usernames or email addresses. Whatever method you use to obfuscate data should prevent inference. Therefore, masking would be suboptimal when compared to other methods such as tokenization. With tokenization, you’re sending sensitive data through an API call or batch file to a provider database that replaces the data with non-sensitive placeholders called tokens. These are pseudo-random placeholders,and they have no mathematical relationship to the original data, and it’s definitely not encryption. The practice involves two distinct databases. The original database that has the sensitive personal information,personal health information, or intellectual property, and the database with the tokens matched to each chunk of data. Unlike encrypted data, tokenized data is irreversible and unintelligible. Tokenization is considered far superior to simply masking. You might want to take some time on your own to take a look at this diagram. Briefly, we have sensitive information on the left. Child welfare information, adoption agency, child protective services,and the identity of that child must be protected when information about them is being used by other agencies, other departments, universities, and government entities. So, when the data is sent to the hospital, corrections department, government agency, or third party,the sensitive information is tokenized or think of it as a digital redaction.

#### Blockchain Technology

Blockchain technology. A blockchain is a distributed database that leverages a constantly growing list of ordered records called blocks. These blocks are linked using cryptographic mechanisms. Each block stores a cryptographic hash of the previous block, a timestamp, and the transaction data. Blockchain may be deployed as a public ledger or private smart contract consisting of a digital “chain of blocks” storing information. Data can be read or written to the chain but not modified. We call that immutability. Changes must be made to a subsequent block in the chain, representing a pointer back to the original data. Transaction data such as date, time, and amount is verified with a consensus mechanism such as proof of work (PoW) or proof of stake (PoS). The transaction participant’s identities are based on digital signatures. Unique cryptographic hashes are used to distinguish the blocks from each other. Some use cases for blockchain include popular cryptocurrencies and tokens, money and asset transfer ledgers. Smart contracts, for example, new businesses and agreements and corporations all digitized on the blockchain. Non-fungible tokens (NFTs) such as music, artwork, and memes. Government services, insurance claims for fraud prevention,securities, in other words, stocks and bonds that are on chain, and healthcare.

#### Course Summary

In this course, you learned about symmetric and asymmetric cryptography, encryption levels, Hashing, salting, HMACs, key exchange, digital signatures, certificates, and PKI, and cryptographic tools and blockchain technology. Coming up in the next course, you'll explore threat actors and vectors.

## Threats, Vulnerabilities, and Mitigations

In this track, you’ll dive deep into the world of cybersecurity. You’ll  explore the different types of threat actors and their motivations,  understand common threat vectors and attack surfaces, and learn about  various vulnerabilities. You’ll also get hands-on experience analyzing  indicators of malicious activity in real-world scenarios and discover  the essential mitigation techniques used to secure enterprises.

### Threat Actors & Vectors

#### Course Overview

Threat actors and vectors. In this course, we will compare threat actor types, attributes and motivations, explore social engineering and common attack surfaces,look at supply chain vulnerabilities, examine application,operating system-based, and web-based vulnerabilities,and learn about hardware, virtualization, cloud, and mobile device vulnerabilities.

#### Threat Actor Types and Attributes

Let's establish the fact that without a catalyst, without a threat actor or agent, there is actually no threat.The agents or actors are the persons, methods, operations, techniques, systems,or entities that act (or have the potential to act) with intent to initiate,transport, carry out, or in any way support a particular threat or exploit. So, threats are not realized without an agent or a catalyst and a threat actor can be comprised of an individual or various types of groups. The attacks can also be totally automated, that would be a bot or a robot. Some attacks are structured others are unstructured. Structured attacks are planned, organized, persistent, often multi-phased or multi-staged. They can be internal or external and may involve exploit kits, zero-day code, precompiled modules, for example in Metasploit or ransomware. Unstructured attacks are accidental, they're usually non-malicious. They happen by just drive-by web surfing,or there's no acceptable use policy (AUP) or the employee accidentally violated the policy. They could happen in email and web mail or something stored on a USB fob or personal electronics. Keep in mind that unstructured attacks can also be internal or external, but usually they're internal. Unskilled attackers, otherwise known as script kiddies,originate from the combination of inexperienced crackers using script viruses and prepackaged malicious code, for example,exploit kits or a Malware as a Service campaign on the dark web. The most common script viruses are spread via email attachments using preformed scripts and modules from various exploit kits like Kali or Parrot. Newer techniques are also often learned on YouTube and other social media sites in the dark web,using the Onion Router as a browser. These represent the lowest level of attacker sophistication and capability levels, and should be considered in your qualitative or quantitative risk analysis. Hacktivists unofficially began in the late 1980s,when viruses and worms spread messages of protest, for example,"Worms Against Nuclear Killers".The term "hacktivism" was coined by the Cult of the Dead Cow, which also gave birth to "Hacktivismo" a group of international crackers protesting human rights abuses. Hacktivist responsible for denial of service,distributed denial of service and botnets,ransomware, hijacking and defacing websites, and other cyber attacks in order to raise awareness for their cause. A higher level would be organized crime syndicates.Organized cybercrime is a well-funded,multi-billion-dollar-a-year industry that affects all sectors of government, the private sector and the economy. These are the main contributors to advanced persistent threats (APTs).They perform cost-benefit analysis and other analytic research before carefully choosing their targets. Analyzing risk versus reward,these organized crime syndicate campaigns may last months or even years. An example would be the ALPHV/BlackCat ransomware operation. Many security analysts and experts contend that the world has already entered a third world war in the form of a cyber war that will be referred to as WWC or World War Cyber. The nation-state actor has a "license to hack"since they work for a government or military to disrupt or compromise target governments, organizations,or even individuals to gain access to valuable data or intelligence. They might be part of a semi-hidden "cyber army" or"password crackers for hire" for companies that are aligned with the aims of a government or a dictatorship that can create incidents and false flag operations that can have international significance. The nation-state actor or state-based attack has developed (along with criminals) many zero-day malware exploits that are waiting to be activated. Malware waiting to be activated is also referred to as a logic bomb. Finally, in this lesson we have the compromised privileged insider. These existing and recently released employees or contractors should be considered "public enemy number one". They can often have unfettered and elevated access,and are the most likely to leave backdoors and other covert channels upon exiting the organization. The term "compromised" is more accurate than "disgruntled" since there are several factors that can put an employee in a compromised position without necessarily being dissatisfied with the organization or their other coworkers.

#### Threat Actor Motivations

In this short lesson, we're going to look at some threat actor motivations. Early on, let's say in the 1980s and 1990s, hackers and various attackers conducted their threats,often just to get notoriety. This involved malware that attacked basic instructions and viruses that attach themselves to other executables. Based on the popularity of the attacks, attackers then started to look for fame with more elaborate viruses and malware and more critical targets,and that rapidly evolved into motivations being highly financial. The beginnings of World War C with political, organized crime and rogue states. As the payloads got more valuable, the threat severity went up and the number of occurrences also went up. The motivations could be data exfiltration for financial gain,state-based or corporate espionage, service disruptions,blackmail and extortion, political activism or ethical issues,and revenge or acts of war, that being World War C.

#### Human Vectors and Social Engineering

The dark web is the veiled collective of Internet sites that are not indexed and are only accessible by a specialized web browser such as ToR, Freenet, or Subgraph OS. It is considered to be a part of the deep web. The dark web is a vast repository of Malware as a Service campaigns and resources. It is used for keeping Internet activity anonymous and private, which can be helpful in both legal and illegal applications. While some use it to evade government censorship, it has also been known to be utilized for highly illegal activity, such as purchases of contraband and child pornography. Email phishing attacks or hoaxes are one of the most common exploit vectors available to crackers. Phishing is a cyberattack that uses disguised email and webmail as a vector. The goal of phishing is to trick the recipient into believing that the message is legitimate, so they'll click a link or download an attachment. Common indicators of phishing are vague salutations, suspicious domains, wrong paths, or wrong hypertext, awkward grammar, urgency in the message, a lack of contact information, and spoofed headers/logos on the email or the page. There are different variants of phishing attacks. Spear phishing is a select, targeted attempt to steal sensitive information, such as an account credential, or financial information from a specific victim,often for malicious reasons. Attackers will often launch spear phishing attacks against new employees or low-level employees that may not have the experience or knowledge in security awareness. Whaling is a special spear phishing attack against a high-level and/or highly privileged employee. Smishing is using various text messaging formats such as SMS as a vector, these have exploded over the last few years. Vishing is using voice over IP or telephony as the hoax vector. Business email compromise, BEC is a form of attack that targets companies that outsource, conduct wire transfers and process invoices, often abroad to other countries. It is often an elaborate, advanced persistent hoax that targets corporate email accounts of high-level employees. They're either spoofed or compromised through keyloggers or phishing attacks, but they could even be hoax phone calls to induce fraudulent wire and cryptocurrency transfers. Some attackers have successfully spoofed large vendors and customers, lawyers, CPAs, and even government officials, for example, representing the IRS. Social engineering is a macro term that can involve eliciting information and reconnaissance through various vectors, like shoulder surfing. Dumpster diving can lead to credential harvesting or finding other information in dumpsters or shredded paper repositories. Hoaxes and impersonation,identity fraud and invoice scams such as the aforementioned BEC. Pretexting is using a fabricated story, or pretext, to gain a victim's trust;or do brand impersonation. Disinformation and influence campaigns often done by competitors or state-based and watering hole attacks. This is a web-based attack that often goes against the low hanging fruit of an ad hoc website. For example,a website for a high school or college class reunion where the participants provide a bunch of information and the attacker can gain that information by attacking the weakly secured website. Some of the reasons for effectiveness of social engineering and phishing, BEC, hoaxes and others is just a lack of proper security and awareness training. It could be inadequate, acceptable use policies or no buy-in from management and the employees for their role in prevention measures. Perhaps there's no enforcement of existing policies,you should combine the carrot and the stick negative consequences,but also rewards for good behavior. You may have outdated antivirus, data loss prevention solutions and mobile device and application management tools, or poor perimeter security controls for email, messaging,telephony and other web-based activities.

#### Common Attack Surfaces

In this brief video, we're going to look at some common attack surfaces. First, removable devices, USB, firewire, or memory cards. Vulnerable software, software downloaded from peer-to-peer file sharing sites or drive-by websites or click through sites through Bing or Google Images. Unpatched client-based, and agentless services,default credentials such as default passwords or default access point workgroup names, and unsupported systems and applications. Perhaps running off of a USB drive or in a Type II hypervisor environment such as VM Player or Oracle VirtualBox, you might call this ghost or shadow IT. Messaging and chat, one of the most rapidly emerging vectors for phishing attacks or smishing, various social media including disinformation in the comment section, insecure network perimeters having over-privileged users and not using more modern attribute-based access controls and zero trust solutions,or simply having open service ports that make your systems and servers vulnerable to various attacks.

#### Supply Chain Vulnerabilities

On this newer Security+ (SY0-701) exam,the issue of supply chain vulnerabilities requires greater visibility,especially due to the events that have occurred over the last 5 to 6 years. Software supply chain security continues to be a growing risk for many types of organizations. In fact, once you obtain the Security+ certification, you may decide to specialize with other certifications that focus strictly on supply chain security. Experts predict these challenges will only continue to rise,and damages could exceed 15% growth year-over-year for the foreseeable future. Many organizations allow various third-party enterprises to actually have access to their networks and systems and data. When an attacker exploits a vendor or partner, that trust relationship,both physical and logical,can be leveraged to gain access to an organization's infrastructure. Zero trust initiatives area powerful countermeasure to supply chain vulnerabilities today. Let's see some examples of the participants in a supply chain. It could be managed service providers, vendors for example original equipment manufacturers, suppliers, Internet service providers, cloud service providers,and Software as a Service providers, hardware providers, and software providers.

#### Application Vulnerabilities

In this lesson we’re going to explore application vulnerabilities and some of this technology may not be your area of expertise. Keep in mind that CompTIA does recommend that you have an A+ certification or A+ knowledge before you embark on the Security+ exam. It is not a prerequisite, however, it's highly recommended. Our first application vulnerability, which is extremely popular, is what we call memory injection vulnerabilities. First off, let's define a few things. Shellcodes are a small stub of code used as a payload, often for malware or memory attacks. A DLL, a dynamic link library is a shared library of functions that multiple programs can access. A process is an instance of a program or code being executed. A thread is a small sequence of instructions or a component of a process. Windows API protocols allow interaction with the Windows operating system,some examples would be VirtualAllocEx, which reserves or changes a region of memory. WriteProcessMemory, which writes data to an area of memory in a specified process, and CreateRemoteThread which generates a thread in the address space of another process. These are common targets for memory injection attacks. For example, shellcode injects malicious code into a running application of PowerShell, which is regularly used in attempts to execute in-memory attacks. Process hollowing starts a legitimate process whose sole purpose is to be a container for malicious code, it then delivers the process in a "suspended" state, then rewrites the content with the required code in memory, and continues to execution. Reflective DLL injection is where the contents of a rogue DLL, often by downloading an unsigned DLL, are loaded into memory. Here we can see the process of reflected DLL injection. And let me just state this can be avoided by always installing digitally signed code or updates. First, the process obtains a handle into another process, then it allocates memory by leveraging Page_Execute_Readwrite. Then it writes the DLL contents to Process B, which happens to be the Evil or Rogue.dll. In step four, it will pass execution to the DLL's embedded bootstrapper code and the bootstrap shellcode calls exported reflective loader functions. In a buffer overflow attack, the attacker sends a larger-than-expected input. For instance, when a front-end web server accepts itand then writes it to memory areas,it could be a large amount of data in an HTTP header request. Associated buffers are then filled, and the adjacent memory is overwritten as a result. This "overwrite" can contain malicious instructions or functions or code that crash the server,or perhaps run a persistent remote access trojan communicating back to a command and control server or some other member of a botnet.

#### Operating System Based and Web-Based Vulnerabilities

Operating system based and web-based vulnerabilities. One of the most prevalent misconfiguration habits is leaving debugging features enabled in production environments of operating systems and web-based applications. It's critical to make sure that debugging functionality is disabled or properly secured in those environments. Another common misconfiguration comes from the use of default or weak credentials for various system components such as operating systems,databases, network devices, or application interfaces such as RESTful APIs, and all systems should use tested patch management with rollback and fallback plans. SQL injection, or SQLi, is a common attack that's been run against front-end services like web servers and Microsoft SharePoint that use SQL as a database repository. It involves inserting a Structured Query Language query through input data from client-to-server applications such as, to read sensitive database data with (SELECT FROM),change database data with (INSERT, UPDATE, or DELETE),execute administrative functions such as shutting down the database management system (DBMS), getting the contents of files on the database management system, and running commands on an operating system. Cross-site scripting, or XSS,is basically a flaw in web pages rendered by web servers, actually not the web server code itself, such as Microsoft IIS or Apache, where malicious scripts or code are injected into trusted or innocent website pages. These malicious scripts can steal cookies, capture session tokens or other sensitive data stored by the browser and used with the particular site. The attacker typically sends browser-side scripts to the end user. And this can occur any time a web program takes user input within the output it generates, without validation or encoding. Here we see the attacker injecting the malicious script onto pages rendered by the vulnerable website or web application. If it's persistent, the malicious script is actually saved to the database. Then when the user requests data from the server, the data containing the malicious script is loaded and can often be executed and called back to the attacker or sent directly to the victim,who then makes a call back to the attacker. There are several variants of cross-site scripting. DOM-based or document object model is also called local XSS or type 0.This doesn't involve vulnerable web servers,but rather insecurely written HTML pages on the end user's system or local gadgets or widgets. That's why it's called local Widgets with Apple, Nokia and Yahoo; Gadgets with Microsoft and Google. Reflected XSS is nonpersistent or Type 1,this is a classic input trust vulnerability where the application is expecting some input, for example a query string and the attacker sends something the developer did not expect. Stored, which is persistent, or Type 2 is a variant where rather than reflecting the input,the web server persists the input. This is what we saw in the previous diagram where the malicious code was saved in the database. The difference is an intermediate phase where the untrusted input is stored in a file or a database before unloading on the victim. This is often found in blogs and review/feedback web applications. Cross-site request forgery is expressed as CSRF/XSRF, this attack forces an end user to perform undesirable actions in a web application in which they are authenticated. An effective cross-site request forgery attack can force users to perform state-changing requests such as transferring funds, changing their email address,changing their password. If the victim is an administrative account, the CSRF attack can compromise the entire web application. Here we see the perpetrator forging a request for a fund transfer to a website. In phase 2, they embed the request into a hyperlink and send it to visitors who may be logged into the site. When the visitor clicks on the link, it inadvertently sends the request to the website. The web validates the request and transfers funds from the visitor's account to the perpetrator. And again, remember,the higher the privileges of the website visitor, the more damage can be done to the actual website through the cross-site request forgery attack.

#### Hardware and Virtualization Vulnerabilities

Some of the dominant factors that contribute to vulnerabilities and flaws in hardware are vendors going out of business when that happens, especially if somebody buys the business often, they will end-of-support an end-of-life. Original equipment manufacturers(OEMs) can cut corners, especially if there's supply chain vulnerabilities or breakdowns in the chain. If the product becomes end-of-support and/or end-of-life, you may have few or no alternatives. Lots of organizations still use outdated and legacy systems. In other words, if it ain't broke, don't fix it, and it's common to have unsecure and unsigned device drivers or DLLs. Firmware is software that's embedded within hardware devices and provides low-level control and functionality. Some firmware can be remotely reprogrammed and may be accessed by attackers through remote code execution (RCE). These may also be referred to as field programmable gate arrays. Common firmware exploits are authentication bypass, buffer overflows,and injection flaws. The rapid emergence of the Internet of Things,(IoT) and smart devices has introduced more security vulnerabilities. Because of cloud computing and virtualization, a critical hardware and software solution is the hypervisor. A hypervisor is a virtual machine manager software system that runs and controls virtual machines. It allocates and shifts resources,as well as manages the interaction between the virtual machines andthe hardware.There's two types of hypervisors,Type I is called bare metal or native. This is when the hypervisor runs directly on the underlying hardware. For example XenServer, KVM, Hyper-V,or ESXi running on a data center Cisco server. Type II is hosted, this hypervisor runs on the operating system that's installed on the hardware. That's the diagram to the right on the bottom.Common examples of Type II are Oracle VirtualBox 6, VMWare player,and VMWare Workstation. Hypervisors have vulnerabilities as well,one of them is VM sprawl. This just simply involves having no centralized control of the hypervisors and the virtual machine images in your organization. In other words, you've lost control or you never had it. VM hopping is when administrators do not enforce the partitioning of guests from each other. VM escape is a flaw in the hypervisor that allows the guest to access the underlying hypervisor, and often even the hardware that it runs on and Hyperjacking. This is a scenario where a privileged insider will install malware, such as a rootkit or remote access Trojan, on the hypervisor to conduct unauthorized activities such as data exfiltration.

#### The Cloud Security Alliance (CSA) Treacherous 12 Cloud Vulnerabilities

On the Security+ exam, they'll want you to be aware of vulnerabilities in using cloud computing. The best source of that is the Cloud Security Alliance, or CSA Treacherous 12. The Treacherous 12 - Cloud Computing Top Threats report plays a vital role in the CSA research ecosystem. The goal of the report is to offer organizations an up-to-date, expert-informed understanding of cloud security issues so that educated risk management decisions can be made concerning cloud adoption strategies. This works for organizations that are doing a migration or ever-popular start-up companies. The report reflects the current consensus among security experts in the CSA community about the most significant security issues in the cloud. On the Security+ exam, they'll want you to be aware of vulnerabilities in using cloud computing. The best source of that is the Cloud Security Alliance, or CSA Treacherous 12. The Treacherous 12 - Cloud Computing Top Threats report plays a vital role in the CSA research ecosystem. The goal of the report is to offer organizations an up-to-date, expert-informed understanding of cloud security issues so that educated risk management decisions can be made concerning cloud adoption strategies. This works for organizations that are doing a migration or ever-popular start-up companies. The report reflects the current consensus among security experts in the CSA community about the most significant security issues in the cloud. Here's the first six, No. 1 data breaches. When you're using a cloud computing solution, it's critical to use data loss prevention. No. 2 is weak identity, credential, and access management. No. 3 is not having a secure application programming interface, WebSocket APIs and RESTful APIs are very common in the cloud. No. 4 vulnerabilities with your systems and applications, here we rely on security by default and security by design. No. 5 is account hijacking, you don't want privileged insiders getting access to those account secret keys, especially malicious insiders, No. 6. No. 7 is the overall presence of advanced persistent threats, 8 is accidental data loss, 9 is insufficient due diligence. In the words of Sun Tzu, who wrote The Art of War, tactics without strategy is the noise before defeat. 10. Abuse and nefarious use of cloud services. 11. Denial of service, including distributed denial of service attacks, often on a botnet and, No. 12 vulnerabilities with shared technology. 

#### Mobile Device Vulnerabilities

In this lesson, we'll investigate some basic mobile device vulnerabilities. There are several classic vulnerabilities with mobile devices, most of which have been addressed with vendor updates. For example, side loading in the context of smartphones, involves installing a compatible app for an Android or iOS device that is not available, approved,or at least monitored and maintained by your device platform's official app store, for example, the Apple Store or the Google App Store. Jailbreaking is the act of exploiting the flaws of a locked-down electronic device, usually an iPhone, to install software other than what the manufacturer has made available for that device. It allows the device owner to often gain full access to the root of the operating system and all of the features. Rooting is the process of unlocking, usually an Android smartphone or tablet. A rooted device gives the user much more freedom to customize the device and achieve more administrative control. The reason being is that the Android device is based on open-source Linux, whereas the iPhone iOS is based on the Apple code. Although jailbreaking sounds worse, rooting is actually much more potentially damaging and much more probable based on the fact of its Linux operating system. Jailbreaking is very rare due to the constant updates to the iOS from Apple. To combat these vulnerabilities and other attacks, many organizations employ enterprise mobility management, or EMM, enterprises should employ the most robust authentication mechanisms feasible, including biometrics, QR codes, and trusted platform module TPM. This is accomplished through enterprise mobility management initiatives,which include mobile device management and mobile application management. This is not the only time we're going to talk about EMM in this Security+ training.

#### Course Summary

In this course, you learned about threat actor types, attributes, motivations, and social engineering and common attack surfaces, as well as supply chain vulnerabilities, application and OS-based and web-based vulnerabilities. And finally, hardware, virtualization, cloud, and mobile device vulnerabilities. In the next course, we'll take a survey of malicious activities.

### Survey of Malicious Activities

#### Course Overview

Survey of Malicious Activities. In this course, we'll examine malware in physical attacks, explore network and application attacks, learn about cryptographic and password attacks, and look at Indicators of compromise, IoCs.

#### Malware Attacks

When it comes to malware attacks, remember all security incidentscan be considered an exploit, but not all exploitsinvolve the usage or delivery of a malicious software or malware payload.A malware attack is a common cyberattack where malware,typically malicious software,executes unauthorized actions on the victim's system.The malicious software, also known as a virus and/or a worm,encompasses many different types of attacks such as ransomware,spyware, command and control botnets, and more.Like other types of cyber attacks, some malware attacksend up with mainstream news coverage due to their severity.An example of a famous malware is the WannaCry ransomware attack.Which brings us to ransomware, a popular form of malwarethat encrypts key files and holds them for ransom.Usually committed for cryptocurrencies such as Bitcoin over 90% or Monero.Ransomware eventually evolvedfrom misleading fix apps, for example pop-ups to fake antivirus toolsand bogus fine websites.The average ransom demand has more than doubled since 2020,and over 30% of the victims are in the United States.The newest trend is Ransomware as a Service, RaaS, on the dark net,which is a subset of Malware as a Service.In a ransomware campaign, the first phase is installation.The crypto-ransomware could install itselffrom an attachment in an email or a Webmail.It could be a drive by a website through casual web surfing,or it could be a remote Trojan on a peer-to-peer file sharing site,or something installed off of a USB or a FireWire driveexternal to the system.It installs itself after the system boots up.Then, the malware contacts a command and control server oran intermediate bot belonging to the attackeror the crime syndicate on the dark web.Next in phase three is the handshakes and the keys.The ransomware client and server perform a handshake,and the server generates two cryptographic keys.One of the keys starts encrypting every file it findswith common file extensions.A common target is the My Documents folder in a Windows operating system.Then, a screen displays giving a time limit to pay up in cryptocurrencytypically before the criminals destroy the key that decrypts the files.Keep in mind the attackers will give the victims the keysabout 50% of the time, and you're going to show up on a listof victims who pay ransoms.Trojans are malicious code and programsthat masquerade as legitimate applications,or are actually embedded in programs that have some or all of the functionality.Trojan horses have no replicating abilities,like a virus that attaches itself to executables or replicating worms.Trojans need to be a re-named benign programor the Trojan code can exist in an operable application as mentioned.Trojans can also be part of a more elaboratedistributed denial-of-service or botnet attack.And as you can see on the left, the Trojan can be a game,a utility, a system tool, device drivers, patches, updates and upgrades,freeware, shareware, or system updates.Remote access trojans or RATs are a variant of trojan malwareengineered to permit an attacker to remotely controlan infected computer to some degree.Once the RAT is running on the compromised system,the attacker can send commands to it and receive data back in response,for example, a key logger or capture a webcam.The server can be a command and control serverthat is part of an automated botnet.The RAT infected PC running the RAT server program can capture webcams,log keystrokes, perform remote shell including PowerShell,update the version of the RAT, and download and upload fileseither directly to the command and control server or to an intermediate bot.Let's compare viruses to worms.A computer virus is a type of malware that spreads between computersand causes damage to data and software.Worms are a special form of self-replicating virusthat generally spreads without user action or intervention.Viruses are distinctive in that they typically attach to executable filesto disrupt systems, cause major operational issues,and result in data loss and data leakage.Worms distribute complete copies, possibly modified copies, of themselvesacross networks.The virus code spreads from the document or a software it's attached tovia networks, drives, file sharing programs, or infected email attachments.A worm can consume resources, infiltrate data,or simply cause the CPU on the system to waste cycles.This is a denial-of-service attack that results in the computer,usually a server becoming unresponsive.Spyware is often defined as malware intended to penetrate a device,collect personal data, and then send it to a third party without permission.Spyware can also refer to legitimate softwarethat monitors data for commercial purposes like advertising.Technically speaking, practically all smart devicesand IoT components are potentially spyware.Bloatware is unwanted and potentially harmful softwarepreloaded onto new devices.Bloatware is often preinstalled by vendors, manufacturers,or carriers as a form of marketing to put servicesdirectly in front of new customers.Earlier we mentioned keyloggers.Let's dig deeper.Keystroke logging is typically done by malicious codethat records keystrokes and sends data back to command and control servers.Spyware uses keyloggers to capture passwords, credit card information,and other personally identifiable information.Software can also be used to track employeesor family members to adhere to acceptable use.Keylogger detectors are special mitigation tools.These are installed often in VPN agentslike the Cisco AnyConnect Mobility Client.Examples of keylogger programs would be PAL KeyLogger Pro and KeyGhost.Rootkits are a type of malware that can give a threat actor control of systemswithout user consent or knowledge.Root, admin, superuser, or system adminare all interchangeable terms on different operating system platforms.These are dangerous because they're designed to hide their presence,but get low level access to the system.A threat actor who has placed a rootkit onto a machine,often via phishing email or if it's a privileged insider on a USB fob,can remotely access and control it to deactivate the antivirus software,spy on activities, delete logs, steal sensitive data, or execute other malware.For example, hyperjacking a hypervisor in a data center.Logic bombs are malware triggered by events,such as mouse movement or code execution, the date and time of a major event,file access through a file system,the number of times code is run, or triggering the zero-day attackon a national holiday, or a major sporting event like the Super Bowlor the World Cup when people let their guard down.

#### Physical Attacks

On the exam, CompTIA wants you to be aware of various physical attacks.Safes and other containers are rated based on the amount oftime a tool would take to penetrate with brute force.Safes are often the keepor the container of the most valuable assets of the organization.It could be a standalone safe, a walk-in safe, or some other type of safe room.Doors and windows of all types are also common targets of brute force attacks.Although considered a preventative mechanism, locks are a delay componentsince all could be overcome eventually by brute force.RFID Cloning.RFID, Radio Frequency Identification,and NFC, Near Field Communication, devices are vulnerableto a variety of physical attacks.Crackers can clone credit and debit cards by stealing the name,account number, expiration date, and 3-digit code.Data stored on RFID chips can be stolen,skimmed, and scanned by anyone with easily obtained RFID readers.Skimming uses devices that overlay an ATM machineor point-of-sale scanner, or the payment system at a gas or petrol stationto steal the information from the victim.On the right-hand side, we can see the various applicationsfor RFID at NFC and why they're so vulnerable.They're used for transit, physical access to buildings,cashless payments, tokenization and ticketing,loyalty and membership programs on cards, secure PC log-on,and for time and attendance.There's also environmental attacks.Any environmental system that is not air-gapped,in other words, on a network can be compromised easily.Many systems and sensors are smart or remotely accessible with IP,Internet Protocol, wired or wireless.They can be hacked to shut down systems.They can be overloaded.They can be hijacked to change temperature or humidity.If the environmental system connects to other networks,they can represent potential backdoors.And there should be a zero trust policyand high visibility when considering all of these critical systems.

#### Network Attacks

A denial-of-service, DoS, attack happens when a maliciouscyber threat actor prevents legitimate subjects from accessinginformation systems, infrastructure devices, or other network resources.Affected services include email and webmail, websites,personal cloud storage, online accounts, for example, online brokerage,cryptocurrency exchanges, banking sites, or other servicesthat depend on a server or a network.A denial-of-service condition is accomplished by flooding the target hostor network with traffic, for example ICMP messages, TCP packets,or UDP packets, until the target cannot respondor simply crashes preventing access for legitimate users.A distributed denial-of-service attack, DDoS,floods a server with Internet traffic, or it can be internal traffic as well,to prevent users from accessing connected online services and sites.Some attacks are launched by hacktivists overloading an organization's serversto make a statement or express displeasure.Other DDOSs are financially motivated by competitorsor they involve extortion, in which perpetrators attack a companyand install ransomware on their servers.The most common form of a DDoS attack is a robot network or botnet.Botnets are common DDoS.The robot network consists of zombie computers and a master commandand control server to remotely control victims,and many victims are unaware that they're actually part of a botnet.In fact, millions of systems all over the world right noware part of a DDoS botnet.The communication often occurs over things like IRC, Internet Relay Chat,other encrypted channels, or bot-centric peer-to-peer networks,and even social media.Bots can exfiltrate data.They can log keystrokes. They can capture webcams. They can scan memory.They can force a system to participate in mining cybercurrency,otherwise known as cryptojacking, and more.In this diagram, the attacker has somehow penetratedthe victim's infrastructure.It could even be a captured VPN connection.The zombie devices are the targets, and they communicate backeither directly to a command and control serveror intermediate bots on the DDoS botnet.Domain Name Service, DNS, is historically one of the more unsecureprotocols or services at layer seven in the TCP/IP stack.DNS is also subject to denial-of-service in DDoS attack,where the attacker targets the root or other down-level intermediateDNS servers to overwhelm those systems with large amounts of UDP queries.Cache poisoning is also very common internally and externally,where the attacker attempts to modify the DNS cachein the wrong way so that all DNS requests return an incorrect response.This attack goes all the way back to when we had hosts files,where attackers would corrupt the host fileson the local systems or on FTP servers.DNS is often hijacked.It's similar to poisoning.In fact, the result of the hijack may be poisoning,where the attacker often sets up a cloned site to redirect hijacked usersto steal data or deliver malware.If the DNS hijack sends into a website,this is also called pharming, ph instead of f.DNS spoofing is where the attacker represents a domain nameand IP mapping to trick users or to poison caches.Other attacks are the NXDOMAIN attack.This attempts to make servers disappear from the Internetby flooding the DNS server with request for invalid or nonexistent records.DNS flooding is a variant of the UDP flood attack,since DNS servers rely on the UDP protocol for name resolution.An amplification attack is the form of reflection-based DDoS attackin which the attacker leverages the functionality of open DNS resolvers,often in DMZs or public subnets or its service providers,to overwhelm a target server or networkwith an amplified amount of traffic.And then, DNS tunneling.This exploits the DNS protocol to tunnel malware and other databy registering a domain server that points to the attacker's server,where the tunneling malware program is installed.In this diagram, let's start on the right-hand side,where we have the attacker's infrastructure,usually command and control servers in countries all over the world.They have their own ISP DNS resolver, and they infect machineswith malicious DNS requests and DNS response on port 53,but the information is actually tunneled within the request and response.The aforementioned attacks can be conducted on wireless networks.However, there are some specific wireless attacks.So, remember wireless has four MAC addresses,and it has three types of packets, data, management, and control.Rogue access points and evil twins spoof real wireless LAN devices.The difference between a rogue access point and an evil twinis the evil twin access point is actually masquerading as the same platformand operating system as the access points that are being used.In other words, the attacker has done their reconnaissance,and they're deploying the same exact access point.Attackers will often begin with DHCP starvation, where they use upthe real leases so that the rogue server can then be introduced.Other attacks will target management and control frames,for example disassociation or de-authentication,that are used by roaming devices.On-path attacks, which used to be called man-in-the-middle, MITM,are where rogue devices inject into TCP connections and other communications.Jamming is simply a form of wireless denial-of-serviceconducted against wireless access points and wireless controllers.A credential replay attack, whether its wired or wireless,involves the capture of transmitted authenticationor access control information and its subsequent retransmissionwith the intent of generating an unauthorized effector gaining unauthorized access.Obviously, this is easier for an attackerin a wireless network than a wired network.Attackers will also perform other reconnaissance attacks.They'll do dumpster diving.They'll conduct various social engineering campaigns.Anything to harvest the internal user names of an organization,and then replay the credentials against the AAA servers.

#### Application Attacks

SQL Injection or SQLi attacks involve inserting a SQL query through input datafrom client to server application and can allow for several exploits.For example, you can use the SELECT FROM commandto read sensitive data from the database.You can use INSERT, UPDATE, and DELETE structured query language commandsto change the database data.An attacker can execute administrative functions such asshutting down the DBMS, database management system.Or you can get the contents of files on the DBMS,even run commands on the operating system.The frontend servers that then communicate with the backend serversor without proper firewalls run commands on the SQL Server itself.The buffer overflow attacker manipulates coding errorsto compromise affected applications, particularly on critical servers.For example, email servers, web servers, SharePoint servers, and file servers.It changes the program's execution path and overwrites elements of its memory.It then amends the program's execution pathto damage existing files or even expose data.Buffer overflows usually involve violating programming languagesand overriding the bounds of the buffers.They exist when code is reliant on external data to control its behavioror is dependent on data properties that are enforcedbeyond its immediate scope or so complexthat programmers are not able to predict its behavior accurately,for example an abuse case.Replay attacks.A replay attack happens when the attacker snoops or sniffson a secure network communication, intercepts it,and then deceptively delays or resends itto misdirect the receiver into what the cracker wants.The added challenge of replay attacks is that a script kiddiedoes not need advanced skills to decrypt a messageafter capturing it from a network.A replay attack could be successfulsimply by resending the entire communication.This was possible before the introduction of modern firewalls.With a replay attack, which is a type of on-path attack,the attacker places themselves between the gatewayand the remote server or service.In step 1, the attacker sniffs the packets exchangedbetween the gateway and the remote server.In step 2, the attacker replays sniffed packets at a different time intervalor based on the window size.And this can be launched at the remote server or at the gateway.The attacker modifies and sends sniff packets or forges new packets.Privileged escalation occurs when attackers exploit human misconfiguration,design flaws, or omissions in web applications.This is closely related to lateral movement, in other words,tactics by which an attacker moves deeper into a networklooking for more sensitive assets.The result of this lateral movement is an internal or external usergetting unauthorized system privileges.And depending upon the extent of the attack,bad actors can do minor or major damage.Realize that privilege escalation might be a simple unauthorized email,or it could be a ransomware attack against vast amounts of data.Forgery and directory traversal attacks.Cross-site request forgery, CSRF, is an attack that tricks authenticated usersinto inputting a request to a web application or a website.CSRF attacks exploit the trustthat a web application has in an authenticated user.It exploits a vulnerability in a web applicationif it cannot differentiate between a request generated by an end userand a request generated by a user without their consent.And the higher privilege the user, the more damage it can be doneon the web application or the website.Directory or path traversal or climbing is a type of HTTP exploitwhere the attacker leverages the web server softwareto access data in a directory other than the server's root directory.That's where the traversal or the climbing comes in.The threat agent, usually a browser, can view restricted filesor execute commands on the server.Any server that fails to validate input data from web browsersis vulnerable to a directory traversal attack.

#### Cryptographic Attacks

In this lesson, we'll look at some cryptographic attacks,started with the cryptographic downgrade attack.In a downgrade attack, the attacker attempts to force two hostson the network, typically a browser and a web server,to use an insecure or weakly protected data transmission protocol.The downgrade is often HTTP instead of HTTPS or SSL instead of TLS.If a downgrade attack is successful, the attacker can exploitconnection vulnerabilities to intercept and read transmitted data.A downgrade attack is considered a form of on-path attack.To be considered trustworthy,a cryptographic hashing mechanism must be collision-resistant.This means that two different inputs should never produce the samefingerprint or digest.This collision can then be exploited by any applicationthat compares two hashes together, such as password hashes,file integrity checks, and others such as information in a databasethat is hashed for indexing purposes.MD5 is no longer considered collision-resistant.A brute force attack, also known as an exhaustive search,is a cryptographic hack that depends on guessing all possible combinationsof a targeted password or the entire keyspace until discovered.If the password is weak, it could take mere seconds with hardly any effort.A brute force attack is time and processor-intensive.It may be impossible or absurd from a physics standpoint,for example against AES-GCM 256.It can also relate to trying all possibilities in a cryptosystem keyspace,which is why the larger bit size or larger Diffie-Hellman modulus is preferred.A side channel attack is enabled by leakage of informationfrom a physical cryptosystem such as a smart card or a cryptoprocessor.Attributes that can be exploited in a side-channel attack include timing,power consumption, and electromagnetic and acoustic emissions.Wireless WPA3 had an early side-channel vulnerabilityin its Dragonfly protocol named Dragonblood.It was quickly patched.In the next lesson, I'll do a demoand we'll explore some password cracking tools.

#### Exploring Password Attacks

OK, in this demo, I'm going to do a web safari as we look at password cracking.And we're going to go to the Infosec Institute,and they're going to tell us our top 10 password-cracking toolsand a reminder that there's three different methods.There's actually four.One that will be on the exam, so I'll add that one in here.There's the dictionary attack, where you take a word listand maybe add a few permutations,you know, some of those common substitutions that we use are not so common.Like a $ for an s or a 5 for an s or the @ symbol for an a.There's brute-force guessing, a slow trying all password combinations.We can easily circumvent that by just rotating passwords.And then, a hybrid mixes those two.Now, on the exam, they're also going to mention a spraying attack,password spraying.So, here's an article from Rishu Ranjan at OWASP.And he defines this type of brute force attack, where the logins are based on alist of user names with default passwords on the application.So, they'll use one password, often a common password,against many different accounts.So, often what happens is they do the credential harvestingthat we talked about earlier.So, they'll brute force the usernames,and they'll use the same constant password.And this can be done online or offline.Usually, it's offline.And then, they'll try the different credentials to get access to resources.Going back here, number one is Hashcat, the most popular and widely used,supporting over 300 different types of hashes.John the Ripper is well-known. It's open source.It's for pretty much all the platforms.You might just hear it called John.And it can do cracking more than just operating systems.It can do web applications like WordPress, compressed archives,PDFs, Office files, things like that.Another popular one is Brutus, very fast and flexible.It's free, but it's only for Windows.But it can do a wide variety of different types of authentication.Wfuzz is for web applications, brute-force guessing web apps.It can be used for SQL injection,XSS cross-site scripting injection and LDAP injection.THC Hydra is an online password-cracking tool.And it works for a wide variety of platforms and protocolsand realized you can also get these in exploit kits like Kali or Parrot.Medusa is another online password-cracking tool similar to THC Hydra.It's a speedy parallel, modular, login brute-forcing tool.And as you can see it supports a wide variety different protocols and services.But it is a command line tool only, doesn't have a graphical interface.RainbowCrack has a precomputed table of password/hash pairsand stores them in the rainbow table.Remember on the exam, one of the reasons why we use saltsand the ephemeral pepper is to counter measure things like RainbowCrack.OphCrack and L0phtCrack have been around a really long time.These were used early on for cracking NTLM,which was used in Windows before Kerberos.So, these have been around since the early days.They were used very often for cracking Windows XP, Vista Windows 7,also has free rainbow tables.The thing is a lot of companies and manufacturing and mom and popshops still use old Windows operating systems,so these tools could still be effective against those older systems.And then finally, can't forget our wireless or Wi-Fi.So, Aircrack-ng, ng being next generation, was a Wi-Fi password-cracking tool.Aircrack was used early on to crack WEP,but they can also be used against WPA and WPA2 pre-shared key passwords.And this is available for Linux and Windows systems.All right, there you go.There's a kind of a survey of our top 10 password-cracking tools.And also for the exam, don't forget password spraying.

#### Indicators of Compromise

In this brief lesson, we'll define Indicators of Compromise, IoCs.An IoC is a network or a host-based cyber observable,also defined as a forensic artifact of an incursion or disturbance.For example, some change to a registry as part of the kill chainof an advanced persistent threat.It's a measurable event or a stateful property in the cyber domainaffecting data at rest, data in transit, or even data in use.It can be an artifact in a registry entry,a file on a disk, in-memory, or a swap file, or more.Let's take a look at some other examples of IoCs.When account lockouts occur based on a network operating system reactionor an intrusion prevention system action.Using concurrent sessions.For example, a single user utilizing concurrent sessions of the same action,possibly the same port on multiple addresses.Whenever content is blocked by security infrastructure devices.Impossible travel.For example, a user using a service remotely from two differentgeographic locations, where it's impossible for that user to be there.Or a system being accessed by a user on-site but also remotelythrough their VPN agent on a laptop.That's impossible.An anomaly in resource consumption.For example, at a cloud service provider,a user suddenly instantiating mini machine images.Out-of-cycle logging.For example, logging of activities outside of the user's work hours,for example early in the morning when the user could not be there.Or some malware modifying the date and timestamps on logs,or even deleting logs to hide their tracks.And missing logs would qualify for that activity being an IoC.

#### Course Summary

In this course, you learned about malware, physical,network, and application attacks, cryptographic and password attacks,and indicators of compromise, IoCs.Coming up in the next course, we'll explore mitigation techniques.

### Mitigation Techniques

#### Course Overview

Mitigation techniques.In this course, we will examine segmentation,isolation, and access control models,compare configuration and patch management,explore least privilege and separation of duties.Look at encryption for access controls, monitoring, and visibility.Learn about decommissioning and offboarding, and compare hardening techniques.

#### Segmentation and Isolation

In the first lesson of this course,we want to start out with a couple of very important concepts knownas segmentation, also referred to as compartmentalization and isolation.Segmentation divides a computer networkor really any other system into smaller parts.The purpose is to improve network performance and security inthe context of network segmentation.Other terms that often mean the same thing are network segregation,network partitioning, and network isolation.Segmentation and isolation are logicallyand physically accomplished in network infrastructures using what's calledzoning. Zoning or segmentation,is a logical design approach used to control and restrict access,specifically between zones.Each zone has fundamental characteristics defined by the security.Every zone contains one or more separate, routable networks,or today we would call those VLANs.Every separate, routable network is contained within a single zone,in other words, a VLAN can only be in one zone.Every zone connects to another zone via perimeters that contain ZIPs, zoneinterface points.The only zone that may connect to the public zone,in other words, your ISP, Internet service provider or your ITSP,which is a service provider that also provides access not just to the Internetbut also supports your voice-over IP,for example, in a WAN or MAN environment,the only zone is the public access zone,the PAZ, otherwise known as a demilitarized zone, DMZ.Here we see our public access zone connecting through a perimeter tothe Internet through a ISP.The zone interface point or perimeter would bea customer premises equipment router.Now, when I say router or router,I mean it's highly available either an active active, two devices or it'sa cluster of routers, gateways, or even firewall appliances likethe Cisco ASA 5500.Here we have the presentation tier,which is the public access zone,if it connects to the management VLANit's going to have its own perimeter and zone interface point.Now realize that a zone interface point can be between two zones.For example, a firewall that has an interface inthe restricted management VLAN zone and in the public access zone,or each zone could have its own ZIP,its own firewall that connects at layer two and/or layer three.In this design, the company also has an application tier and a data tier.On the Security+ exam. It's very important that you know where to place certaincomponents. For example, in this three armed firewall solution.So starting from the left we have the Internet provider the ISP or the ITSP,this could also be a cloud service provider.And then your customer premises equipment,which is usually a very high-end router or firewall appliance that's onthe outside zone of the firewall, which is at the center of this diagram.Notice that also it's very common for active defense to deploya Honeynet VLAN out between the perimeter device and your firewall.We can use that for active defense.We can redirect attackers with fake telemetry,we can do attribution of the attackers, we can even counterattack. On the exam,if you deploy an IPS, intrusion prevention system,remember it's always going to be behind the firewall,either physically or logically.The IPS sensor only acts on traffic that's gone through the firewalland has been allowed, based on its access control list or firewall rulesor other type of contextual security.Here we see an inside corporate VLAN zone, an intranet VLAN zone where we have,let's say, our SharePoint servers and other servers,and then a public zone or a DMZ where the DMZ servers are.Here we're calling it the DMZ VLAN,in the previous diagram, it was called the public access zone.In that zone only do we have servers that are actually servingthe outside untrusted Internet.

#### Access Control

Access control list, otherwise known as ACLs or ACLs.Access control lists allow stateless or static traffic filteringand management of IPv4 and IPv6 traffic to and froma network interface or a virtual local area network (VLAN).They contain ordered rules, usually starting at 100,or access control entries to permit or allow or deny or block basedon Internet Protocol (IP), Transmission Control Protocol (TCP), and(UDP) User Datagram Protocol services and ports,as well as Internet Control Message Protocol (ICMP) messages and codes.They often function as an additional infrastructuredefense-in-depth mechanism.And access control lists have an implicit deny-all as the last entry applied,if nothing matches in the ordered rules or ACEs.Cloud service providers use network ACLs often as static inboundand outbound access list applied to virtual networks or virtual private clouds.They apply to all instances, containers, appliances,basically everything in the virtual network or (VNet).They're typically configured with the same techniques asthe traditional access list.For example, in this screenshot we're applying a network ACL to a Public subnet.Notice by default on the Inbound and Outbound rule,the default on Rule 100 is to basically Allow All traffic.So in this scenario the access list is actually not activatedbecause all traffic using all Protocols, and Port Ranges,and ICMP Types are allowed from any Source, any being 0.0.0.0/0in CIDR or CIDR format.To activate this access control list,you would simply replace the first rule 100,usually with the most common traffic. For example, HTTPSand you would allow that traffic add a new rule. For example,skipping by fives or tens, let's say 110,and allow other traffic like RDP, or Secure Shell, or FTP,and then rule 120, and rule 130, and rule 140, and so on.At the end with the asterisk is the implicit deny all.So that if nothing matches,the traffic will be denied going Inbound or Outbound to the networkor subnet in the virtual network or virtual private cloud.Security groups are commonly stateful"allow-list" firewalls that apply to layer 3 and layer 4 network traffic.They can be applied to a virtual load balancer and an instance ofa virtual interface it would operate at the hypervisor level attached tothe virtual elastic network interface, for example,eth0 on the virtual Windows, macOS, or Linux Virtual machine.These are also called network security groups (NSGs) if they're applied toan entire virtual network.So think of a security group asa stateful "allow-list" firewall that can be applied to an individual instanceon the virtual network interface,or like an access control list to an entire subnet or VLAN that would be calleda network security group.Security groups have no explicit deny rules like NACLs,but rather have an implicit deny if nothing matches in the "allow-list",and they evaluate all of the rules before a decision is made to permitor allow or deny, there is no numbered ordered list.For example, in this Security Group we have Inbound Rules which could apply toa specific interface or to the entire network.And you simply have a list of things you're allowing HTTP on port 80for IPv4, HTTP on port 80 for IPv6, SSL,TLS on port 443 for IPv4 and v6,and also allowing Secure Shell on port 22, and Windows RDP on port 3389.Notice there's no numbers and there's no allow or deny column at the end.Everything in the "allow-list" is allowed with an implicit deny,and since there's no ordered numbered list,everything is evaluated before the decisions are made.Another generic term used in access controls is permissions.Permissions that principals have can be dictated and enforced bythe network operating system, for example, Linux or Windows,or using a directory service as in a read permission.Permission to access the file's contents.Write permission where you can modify or change the contents of a file,or you can execute the contents of a file if it's an executable program.For example, one can change a Linux file and directory permissions withthe chmod command, which stands for "change mode".Remember, on the Security+ exam they would like you to havethe A+ certification, so you should be familiar withthe fundamentals of file systems and permissions in both Windowsand Linux file systems. A deep dive into those things is not inthe parameters of this Security+ training.You may have to fill in the gaps on your own.

#### Configuration and Patch Management

Configuration management.The goal of configuration management (CM) is to ensure that accurateand meaningful information is readily available regardingthe configuration of applications and servicesalong with the configuration items (CIs) that support them.It might seem odd to see configuration management in this courseon mitigation techniques,but if I was doing a penetration test against your organization,whether I began with the wireless or wired network,if I knew that you had a mature configuration management initiative anda configuration management database that was up-to-date,I would know in the pre-engagement meeting that my job was going to be muchmore difficult to find a weakness or a vulnerability.Configuration management is a mitigation technique because it includes allthe relationships and dependencies between the configuration items.The objects include hardware, software, networks, sites, vendors, suppliers,and even the people of your organization.Configuration management is a governance and systems life cycle processfor ensuring consistency among all assets.CIs in your operational environment,classifying and tracking individual CIs,documenting functional capabilities and interdependencies,verifying the effect that a change to one configuration item hason other systems or applications.CM practices offer the required data about assets and their configurations,including their interactions with other assets,which assist administrators and managers with problem resolution,otherwise known as root cause analysis.Incident response supporting your incident response teams.Network component deployment, strategy formulation,budgetary forecasting, and overall decision-making.A configuration management system(CMS) is a set of data, tools, utilities,and processes used to support the configuration management practice.All information should be tagged and labeled with a common unified schema,preferably using key-value pairs.This data will populate the configuration management database (CMDB).Relational databases have been used historically for the CMDB.For example, Microsoft SQL or MySQL.However, NoSQL/document-style databases are emerging as a common solution,whether on-site or in the cloud.A cloud service provider (CSP) service such as AWS DynamoDB could be leveragedfor the CMS, or you could use a data warehousing solution froma cloud provider like AWS Redshift.Another vital aspect of configuration management is patch management.Patch management is the process of applying(hopefully fully tested) updates to software, drivers,and firmware to protect against vulnerabilities.Effective patch management helps ensurethe best operating system performance of systems, boosting productivity.All systems need to be secured with patches, if possible.The risk of disregarding patch management can cause exposure of business toleaks and breaches, loss of productivity, and loss of reputation.The benefits of patch management includethe protection of all endpoints from attackers,keeps all systems running in an optimized fashion,promotes productivity within the organization.Patch management helps lower the cost of device life cycle maintenanceand repair. It also supports laws, regulations, and compliance standards.

#### Least Privilege and Separation of Duties

In this lesson, we're going to look ata couple of security principles that are criticalfor reducing risk. First, least privilegeand then secondly, separation of duties.Let's start with least privilege.This is the principle that users and programs should only havethe necessary privileges to complete their tasks, and nothing more,according to the National Institute of Standards and Technology, or (NIST).You may hear this also as a "need to know" or staying within one's "pay grade"or classification level.Least privilege is an aspect of authentications,authorization, and accounting, and identity and access managementwhere the subject has just the proper level or number of permissionsand rights to perform the job role or responsibility and nothing more.And it should be built into all access control architectures and initiatives.Any deviation (escalation or elevation), if allowed,should go through an established change control IT service ora service desk implementation, for example, supported by ITIL 4.Separation of duties, also known as segregation of duties(SoD) refers to the principle that no user should be given enough privileges tomisuse this system on their own.For example, the person authorizing a paycheck should not also bethe one who can prepare the paychecks.SoD can be enforced either statically (by defining conflicting roles)or dynamically (by enforcing the control at access time).An example of dynamic separation of duty is the two-person rule.The first user to execute a two-person operation can be any authorized user,whereas the second user can be any authorized user different from the first.Here's a simple example of separation of duties.One user or principal will draft a purchase order.Then it goes through an approval process with a different principal or user,and then possibly a third separate principal will approve the vendor invoice.Then one or more principals will sign the check.For example, if it's overa certain amount that may demand at least two signatures, physical or digital.Separation of duties may also involve things like dual operator principles,where two or more subjects are needed to modify or approve.As we saw in the previous example, two signatures,digital or physical, or cryptographic keys are required for certain actions.Rotation of duties is also a related principle.For example, mandatory time off or forced vacations wherethe principals must leave the site for twoor more weeks during their forced vacation.In that time, you can perform audits or other visibility reporting.

#### Encryption in Access Control

In this brief lesson, we're going to remind ourselves aboutthe importance of encryption when it comes to access control.Encryption helps protect private information or sensitive dataand can enhance the security of communication between client apps and servers.In essence, when data is encrypted properly,even if an unauthorized person or entity gains access to it,they will not be able to read it.Origin authentication uses symmetric and asymmetric encryption keys ina variety of systems, including digital signatures.For example, encryption technologies are involved in shielding privateand secret information from unauthorized users,thus safeguarding confidentiality.This is done by enciphering information in sucha way that only authorized users,in other words, users with the right key are able to access the decrypted data.

#### Monitoring and Visibility of Access Controls

Monitoring and visibility of access controls.Access controls will determine which subjects have readaccess or visibility into critical access, such as sensitive data.This visibility is vital for data in transit over remote channelsas well as data stored at third-party locationssuch as code repositories like Git,personal cloud storage, and cloud-based block, object,and file storage systems such as AWS Elastic Block Store, AWS S3, and FSx.Access control mechanisms also must be completely visible and always monitored.It's becoming more common to automate monitoring visibilityand sending feeds to locations such as security operations centers(SOCs) or cloud security information and event management(SIEM) and security orchestration,automation, and response (SOAR) systems like Azure Sentinel or Splunk runningon Windows Server 2022 or Red Hat Enterprise.Audits should be performed regularly to discover gapsor "privilege creep" that can occur with poorly maintained access controland inventory systems.Other tools that can be used are compliance scanners,designated PowerShell scanners, JSON scanning in the cloud,and vulnerability scanning, and penetration testing.Notice how we get visibility into this retail system that has vending,ePurse, and other applications in the upper right-hand corner usinga smartcard reader. You've got smartcard and biometric middlewareand optional biometric readers.You have card enrollment, which connects to web cams, card printers,and smartcard programmers, often in the foyer of a multinational corporation,for example, guest users.For employees,at the top left, you'll havea PACS server, physical access control system with contactless smart readerslocated throughout the building or in certain high-security areasoptical biometric readers.In the middle circle, we have a certificate authority.For example, at Google this would be EAP-TLS runningon IEEE 802.1 X PNAC, personal Identity verification (PIV).They may be something like ID RAMP using QR codes on mobile devices.Then the access control system isan integrated combination of systems from different departmentsand business units the Human Resources database,Active Directory, inventory systems, and other user directories.This diagram is indicative of many ofthe modern identity management environments that we see today that need strictmonitoring and visibility.

#### Decommissioning and Offboarding

In this short lesson, we're going to touch on the topic of decommissioningand offboarding, which are really actually the same thing.Outgoing employees pose a major security risk to all organizations,regardless of the circumstance.Security practitioners should make offboardingor decommissioning strategies much more resilient in these modern,uncertain times.Without secure offboarding processes,enterprise expose themselves to a variety of risks,from the harmlessly accidental to the maliciously purposeful.These risks include data theft,disgruntled leavers turning into potential incidents or actually catastrophes,shadow or ghost IT, unauthorized Software as a Service utilization, ITand HR siloed and out-of-sync, and access not removed properly.There are some best practices, you should generate solid onboardingand offboarding policies and processes.Encourage proactive, interdepartmental collaboration with stakeholdersdepartments such as HR and legal,as well as server administrators and endpoint operators and administrators.Make sure you secure corporate assets, devices,and their associated credentials if you're not using single sign-on.Make sure there's complete visibility of employees' SaaS,cloud, and third-party access, usage, and permissions.And monitor for uncommon or even risky behavior of all outgoing staff members.And although all outgoing staff members or leavers will go intothe risk database as potential threat actors,we should handle all leavers respectfully and transparently.

#### Hardening Techniques

Without a doubt, one of the most long standing traditional methodsfor mitigating risk is hardening systems.This can go way back to the days of Mainframe computers andthe early Unix boxes attached to the Internet.Classic methods of hardening systems involve shutting down TCP and UDP portsand services that are unnecessary or unused.That also includes blocking certain ICMP messages and codes.Only the necessary secured protocols should be used,for example, we would use Secure Shell Version 2 instead of Secure ShellVersion 1, and at least Secure Shell Version 1 instead of telnet,which is clear text.Also, continual patch management initiatives,as we discussed earlier in this course,must be implemented for all systems and applications.Strict least privilege access controls, as we've discussed,would be used for all administrative users on these hardened systems,and ongoing monitoring and visibility must be instigated.Data, systems, and applications can be further hardened by using symmetricand asymmetric cryptosystems to protect data at rest, data in transit,and in some situations even data in use.Endpoints are secured and hardened in Zero Trust environmentsusing trusted platform modules and the implementation of endpoint detectionand response tools.Modern solutions suchas Palo Alto Cortex XDR are considered next-generation endpoint detectionand host-based intrusion detection and protection.Other hardening best practices involve, disabling all auto-configure features.For example, a switch port automatically going into trunk mode if a system likea workstation or a laptop requests it.Replacing all default passwords and default workgroup names with strongerand longer credentials.Implementing strict password policiesor if you're in a Microsoft environment with Microsoft Azure using one ofseveral easy-to-deploy passwordless solutions.Removing all unnecessary and unauthorized software for example,personal cloud storage, type 2 hypervisors,exploit kits, and other agents and software that might runon dynamic Ephemeral ports or connect to peer-to-peer file sharing services.

#### Course Summary

In this course, you learned about segmentation,isolation, and access control models, configuration, and patch management.Least privilege, separation of duties, encryption, monitoring and visibility,and decommissioning, offboarding, and hardening.Coming up next, we'll explore architecture and infrastructure concepts.

## Security Architecture

### Architecture and Infrastructure Concepts

#### Course Overview

Architecture and Infrastructure Concepts.In this course, we'll learn about architectural considerations.Explore cloud computing. Define Infrastructureas Code- IaC, serverless technologies, containers, and microservices.Examine network infrastructures including centralized vs. decentralizeddesign, discover virtualization, learn the basics of ICS and SCADA,and define the Internet of Things, IoT.

#### Architectural Considerations

Our first architectural consideration is that of resilience.Resilience is the ability of an application,a service or a system to continue to operate under adverse conditionsor stress, even if in a degraded or debilitated state,continue to maintain essential operational capabilities.Recover to an effective operational posture ina time frame consistent with mission needs.In other words, business impact analysis.Resilience is the ability of a workload to recover from infrastructureor service disruptions. Administratorsand engineers should be able to dynamically obtain computing resources to meetdemand and mitigate disruptions.Disruptions can be misconfigurations or ongoing transient network issues.An aspect of resilience is high availability.Availability is expressed as a percentage of plannedand unplanned downtime over typically an annual period.Percentages such as 99.5, 99.9, 99.95, and 99.99also referred to as 'four nines'.High availability entails a system, component,or application operating at high capacity, continuously, without intervention,for a defined period of time.A highly-available infrastructure is designed to deliver quality,performance and handle various loads and failures with minimumor zero downtime.Reliability is a metric of percentage uptime, consideringthe downtime due only to faults or outages,whereas Availability is a measure of the percentage uptime, consideringthe downtime due to faults and other causes such as planned maintenance.In other words, Availability considers planned and unplanned outages,whereas Reliability does not.And remember, for two different systems,it's possible for one system to be more reliable,but less available than the other.Let's compare Availability to Durability.Availability has historically been achieved through hardware redundancy.Active-active failover, clustering, so that if any component fails,access to data or the system itself will remain.Durability, on the other hand, refers to the long-term data protection.In other words, the stored data does not suffer from bit rot, degradation,or other corruption. In other words, it won't just simply vanish.Durability is concerned with data redundancy so that data is never lostor compromised. An example: AWS Simple Storage Service, S3 andGoogle Cloud Storage are designed for 99.99In other words, 11 nines of durability per objectand four nines of availability per year.Other architectural considerations include cost,which can be the initial cost and the ongoing cost of maintenance,as well as depreciation. Responsiveness,for example, how much latency is there in milliseconds between the clientand the server, we’re shooting for low latency and optimal performance.Scalability, scaling out would be adding physical components on an ad-hoc basisas part of auto scaling. Scaling up means going toa higher performance platform or adding compute processor and memory capacity.The ease of deployment- can you deploy with Infrastructure as Codeor templatized stacks to terraform the environment,as opposed to manual configuration?Can you automate the patching and the ongoing updating ofthe application, system or service?Risk transference-can you transfer risk to a cloud provider or an insurance company,or a shared disaster recovery site for high availabilityand disaster recovery planning?And power considerations- for instance,uninterruptible power supplies, generators,redundant power sources, and protection against blackouts, brownouts, surges,and sags.

#### Cloud Computing

In this lesson, we're going to look atthe different service types of cloud computing,beginning with IaaS. Infrastructure as a Service,IaaS, is where the "capability provided tothe consumer is to provision processing,serverless and server-based storage, networks, databases,and other fundamental computing resources where the consumer can deployand run arbitrary software, including operating systems and applications.The consumer does not manage or control the underlying cloud infrastructure.In other words, data centers in zones in regions all over the world.But they have control over operating systems, storage, deployed applications,for example containers, and possibly limited control of select networkingcomponents, for example, firewalls running on applications."Here we see a diagram of IaaS at Amazon Web Services.Everything below the dashed line is the responsibility ofthe provider- AWS that includes four Foundation Services- Compute, Storage,Databases, Networking, and Content distribution networking.They also provide their own IAM or identity managementand their heavily secured data centers.They have Edge locations to support CDN, content distribution networking,in cities all over the world.Every region of AWS has at least three Availability zones,and they also provide virtual interfaces,called Endpoints to their hundreds of services.Everything above that dashed line is the responsibility of the consumer orthe customer. That includes how the data is encrypted,protecting data in transit,choosing the IAM solution, either a cloud based IAMor federated single sign-on from their own identity provider.And then of course, they choose the operating system.They configure the network, they configure the firewalls,they manage the platform and applications,and of course, the data. With IaaS as it relates tothe shared responsibility model,the customer or the consumer has the most responsibility and the provider hasthe least responsibility.Platform as a Service is when the “capability provided tothe consumer is to deploy onto the cloud infrastructure consumer-createdor acquired, in other words,build or buy applications using programming languages and tools supported bythe provider. With PaaS, the consumer does not manage or controlthe underlying cloud infrastructure, the network, the servers,the operating systems, or storage,but they do have control over deployed applicationsand possibly application hosting environment configurations.”With Platform as a Service,the shared responsibility model is heavily reliant upon the service portfolio.In other words, how managed is the service?Is it a managed service or is it a fully managed service?This is the service type that has the most variance.Platform as a Service could be Developmentand software development kit platforms, for example for Java,PHP, and Python and more. Container services for Dockerand orchestration with Kubernetes, Managed and fully managed relationaland document databases, server based and serverless. PaaS can be Managedsecurity and threat modeling services.Other bleeding-edge technologies like single sign-on, machine learning,artificial intelligence, IoT, blockchain, and other various media services.These all fall under the category of Platform as a Service.Software as a Service is when the “capability provided to the consumer is to usethe provider's applications running on a cloud infrastructure.The applications are accessible from various client devices,maybe a thin client interface or a virtual desktop interface,a custom web application or a web browser,or a web browser with a plugin or an add on.The consumer does not manage or controlthe underlying cloud infrastructure at all.Even individual application capabilities,with the possible exception of limited user-specific profilesor application configuration settings.”Common software as a Service offerings would be customer relationshipmanagement, CRM, enterprise resource management, ERM, human resources,and workplace tools. For example, Workday, finance,sales, and cloud-based marketing services, payroll services, email, webmail,collaboration tools, and personal cloud storage.Help desk and virtual service desk, virtual call centers,and cloud-based business analytics.Let's return to the concept of the shared responsibility model.This isn't something to memorize, but realize regardless of the cloud provider,whether it would be AWS or Google Cloud or Azure, Oracle, IBM or Alibaba,there's going to be a shared responsibility.In this diagram, we can see that some responsibilities are always retained bythe customer. Now if it's on-premises, the on-premises private cloud,everything's in the responsibility of the customer.As we move to the left, notice that with IaaS,the customer has more responsibility, for example, than Software as a Service.As you move from on-premises to Software as a Service,the responsibility of the consumer or the customer gets less and less.Sometimes there is shared responsibility, for example,Identity or directory services, some Applications or Network controls,especially with Platform as a Service,that's the one that has the most flexibility and the most difficult to securebecause of the many different options.In addition to service types, there are also deployment models.In the public cloud, the organization runs an initiative, for example,DevOps or database, or a cloud-based call center or service desk entirely atthe cloud service provider,or it has public customers for its deployed resources.Their public website or e-commerce services.A private cloud is a cloud scenario that supports a single organization,and its internal customers are either in the CSP or they are on-premises.Remember, a private cloud is private for a single organization,but it can be on-premises or it can be at a cloud provider.At a cloud provider, it's in a sandbox or private subnet environment.A community cloud is a consortium that uses a cloud environment fora particular use case, for example, an online gaming community, the metaverse,financial services, a health care PPO or HMO or some other community,for example, a government community cloud.And then the hybrid cloud is a combination of the other three options publicand private, public and community, or private and community.Or it could be edge computing,where a company is running the same type of hardware and software thatthe cloud provider is running on-premises, may be part of a database migration,data replication, backup and restore, disaster recovery.Or they're an online retailer that bursts up during their peak seasons tothe cloud.Hybrid cloud can also be a method for connecting infrastructureand applications between cloud-based resourcesand other resources that are not placed in the cloud.The most common type of hybrid deployment is betweenthe provider's public cloud and a standing on-premises enterprise private cloud.It can be used to migrate, expand,or grow an organization's infrastructure intoa cloud solution while linking internal systems to cloud resources,often with high-speed fiber connections that bypass the Internet.10 gig, 50 gig, and even 100 gigabit per second.These are often used by organizations to "burst up" tothe cloud during peak demand times, or for special situations.The on-premises private cloud involves installing resourceson-premises using virtualization and resource management tools.An on-premises deployment does not offer many ofthe benefits of cloud computing,but it's often chosen for its ability to provide dedicated resources.In most scenarios, this deployment model is the sameas legacy IT infrastructure,with the exception of using application managementand virtualization technologies,for example Hyper-V or VMware to attempt to increase resource utilization.Finally, let's talk about third-party cloud vendors.They could be brokers, for example,local municipal partners, supporting edge location,connectivity or reciprocal agreements.Auditors often Cloud Security Alliance or SOC certified internaland external auditors. Managed security service providers,MSSPs such as Fortinet, Fortigate offering cloud-based services,Next-Gen Firewall, Next-Gen IPS, Endpoint detection and response,high visibility and SIEM and SOAR systems.For example, Azure Sentinel. CASB, Cloud access security brokers, assisting withSaaS providers for compliance, data loss prevention, and leakage,and federated single sign-on services or direct connections.Direct ten gig, 50 gig and 100 gig connections using AWS Direct Connect,Google Interconnect and or Azure ExpressRoute.A key enabler for edge computing and hybrid computing.

#### Infrastructure as Code

Infrastructure as Code. IaC is the provisioningand operations of infrastructure using code or templates in the form of JSONor YAML, instead of the manual process of configuration.Reusable configuration files are generated containingthe infrastructure specifications,making it easier to edit and distribute configurations of virtual networks,virtual machines, and applications.It also ensures that administrators provisionthe same environment every time by creating a single source of truth.IaC assists configuration managementand helps to avoid undocumented, ad-hoc configuration changes.Version control is also an important aspect of IaC,as all configuration files should be under source control just like any othersoftware source code files. Deploying with Infrastructure as Codealso means that architectsand engineers can divide their infrastructure into modular,reusable components that can then be combined in different ways usingautomation and orchestration.This is referred to as generating the aforementioned "single source of truth"or "terraforming the environment".Automating with IaC also means that developers do not need to manuallyprovision and manage servers, operating systems,containers, or microservices each time they develop or deploy an application.Codifying the infrastructure offers a template to follow for provisioning.An automation tool such as Red Hat® Ansible® Automation Platformis a common IaC solution.Cloud services such as AWS CloudFormation empower customers to model, deploy,and manage AWS and third-party resources by handling the Infrastructureas Code. The cloud template language comes in either JSON or YAML formats.Customers can automate, test,and deploy infrastructure templates with continuous integration and deliveryor (CI/CD) automation lifecycle.Templates can also be used to set up lab environments for learning the cloud,such as AWS Well-Architected. In this screenshot, we see AWS cloud formation.On the bottom we see our single source of truth or JSON file representingthe topology in the window above it.In this platform, you can use either JSON or YAML, or your template language.

#### Serverless Technologies

In this lesson we're going to look at serverless technologies.Modern serverless solutions leverage modern cloud infrastructures to emulatethe network operating system environment without the need for a Windows,macOS, or Linux or Unix-based server.These are technologies for running code, managing data,and integrating applications, all without managing servers.Serverless technologies feature automatic scaling,built-in high availability, and in the cloud,a pay-for-use billing model to increase agility and optimize costs.Common serverless solutions are Functions as a Service,for example Azure Functions or AWS Lambda,serverless containers such as AWS Fargate, and serverless databases suchas AWS Aurora for MySQL and Postgres databases.Let’s take a look at AWS Fargate. On the left-hand side,Without Fargate, you build your container image, usually in Docker.Then you define and deploy virtual instances inthe Elastic Compute Cloud, EC2 environment.You provision and manage your compute and memory resources.Then you isolate applications in virtual machines.Then you define and deploy the instances and then you pay for what you run.However, With Fargate you simply build the container image,define your memory and compute resources,and then run and manage the applications, and then pay for what you've used.The applications are isolated within the Fargate by design.Here we see Azure Serverless Functions.Let's suppose you connect IoT devices that are producing telemetry data.The data is sent to an IoT hub,perhaps special conditions that are routed to an Azure function.The function processes the message, calls Logic Apps, which invokes Zendesk.Then you can make a request to repair your IoT device, for example,a sensor in a manufacturing firm.It could be a tractor on a corporate farm or some other industrial component.Here we see Serverless Databases with AWS Aurora. In the AWS cloud, in your VPC,you can Run an Amazon EC2 artillery load test that will send output toAWS Lambda, which Performs business logic.For example, the number of put #orders as a metric,simultaneously sending information to Amazon CloudWatch for visibility,where you can have a Dashboard that shows the number of #orders vs.the current database capacity,and then a Lambda function can be triggered to Amazon Aurora Serverless,which stores the Orders and Inventory tables for the e-commerce solution.In the next lesson, we'll talk more about containers and microservices.

#### Containers and Microservices

A container is a discrete environment within an operating system (ora serverless architecture) where oneor more applications can run and that is typically assigned all of the resourcesand dependencies needed to function.It is a modular and portable environment that includes the application binaries,any software dependencies,and hardware requirements wrapped up into an independent, self-contained unit.Containers are commonly usedfor processes and workflows in which there are important requirementsfor security, reliability, and especially, scalability.All cloud providers offer managed container development,automation, and orchestration services.Containers can be server-based or serverless.For example, the aforementioned AWS Fargate.At the bottom we have our host operating system.For example, Windows Server 2022 or Red Hat Enterprise.Or it could be serverless from a cloud service provider.On top of that, we have the container engine. In this case it's Docker.Keep in mind, however, that Docker is not the only way to generate containers.The container has the binaries and the libraries andthe dependencies running one or more applications.Microservices are closely related to containers,they are specific service-oriented application components made up of smallindependent services that communicate over well-defined APIs for notificationand process queuing.Microservices make applications and apps faster to developand easier to scale by small, self-contained teams of developers.When comparing microservices to containers,think of it this way, microservices are about the design of the software,the functionality, whereas containers are about packaging the softwarefor deployment.

#### Network Infrastructure

In this lesson, we're going to explore some various networking topologies,both in the cloud and in your own private data center.Here we see a Cloud Customer Network Infrastructure at Google Cloud Platformor GCP. In this design, all External clients coming overthe Internet are going to go through a DMZ VPC.The VPC is what Google calls the virtual network.External clients go through the VPC DMZand then connect to various autoscaling appliance solutions.For example, paloalto Networks, Cisco, Fortinet, f5, or others.The autoscaling virtual appliances then send traffic to various production VPCsor production virtual networks. Those contain virtual machines.For example, on the bottom left there'sthe Service project pod virtual machines.Next to that, the Service project infrastructure virtual machines.For example, virtual SharePoint.There's a Service project non-pod virtual machineand a Service project sandbox or detonation chamber,also referred to as a private subnet in a VPC.Here we see a Cloud Customer Network Infrastructure at Amazon Web Services.On the far left, all public customers go through an Internet gateway.On the far right, any site-to-site VPN connections go through the VPG.In this VPC or virtual network, we have two subnets.The subnet on the left is a public subnet because its Route table hasan Internet gateway in it. On the right-hand side, that's a private subnet.In its Route table is a NAT gateway.Any instances, for example,middle tier and back-end database servers that want to communicate withthe Internet, for example,Windows Update service or to get a vendor update or a download,they will route their traffic to the NAT gatewayin the left-hand public Subnet 1using an elastic IP address, a public address.Then it will go out the Internet gateway to the public Internet.If a subnet at AWS doesn't have an Internet gateway in its Route table,nor does it have a virtual private gateway in its Route table,it is a private subnet. There's only three types of subnets at AWS public,private, and VPN only. We also see in this diagrama couple of different firewalls. We see the Network ACL for each subnet.That's a stateless static firewall.And then the Security group firewalls which are applied directly tothe interfaces the elastic network interfaces of the Instances.Most modern data centers and cloud data centersnow use what's called Software-defined networking, or SDN.Software-defined networking is a framework intended to makea network more flexible and easier to manage,especially with disparate hardware and multiple graphical overlays.SDN centralizes management by abstracting the control plane fromthe data forwarding function in the different networking devices.An SDN architecture offers a centralized, programmable network consisting ofthe controller, which is the essential element of an SDN architecture.It assists centralized management and control,automation, and policy enforcement across physical and virtual environments.Southbound APIs relay information between the controller andthe individual network devices, for example, physical leaf and spine switches,or physical and virtual multilayer switches.The Northbound APIs transmit information between the controller andthe applications and policy engines, which at SDN,look like a single logical network device.SDN is directly programmable, for example with templates, stacks,or Infrastructure as Code in the form of JSON or YAML documents. It's agile.You can make quick changes,you can scale out and scale up since many of the components are virtual.It's centrally managed. For example,in a Cisco application centric infrastructure,the management is done from a controller,which is basically special software running on a Cisco server.It's programmatically configured with IAC using API calls.It is open standards-based and vendor-neutral.In fact, SDN is based on OpenFlow technology.With SDN, we have total separation of the CONTROL PLANE and the DATA PLANE.The CONTROL PLANE is managed through the aforementioned controllers.Basically, administrators and engineers log on to the controllers,usually in a zero-trust environment, perhaps with a biometric,and then send API calls to the APPLICATION PLANE,digitally signed and API calls to the DATA PLANE,which is where the physical and virtual layer two and layer three serversand routers reside.Some other network infrastructure concepts to rememberfor the exam is the concept of physical isolation.For example, your storage area network should be ona separate network than your production network.Your corporate LAN should be on a separate network than your HVAC system,your management VLAN, or where your SDN controllers reside should be ona separate, physically isolated network. Some systems should be air-gapped.For example, certificate servers may be offline and off the networkand only brought online for brief periods of time to send informationto down level intermediate serversor other systems like hardware security modules will be air-gapped.A totally air-gapped system is off any network, but in some environments,air-gapped is considered not on the public Internet.And then logical segmentation.A common way to segment traditional networks was accomplished with privateVLANs. Private VLANs are still done in virtual hypervisor environments,but we can also logically segment on mobile devicesfor enterprise mobility management and other systems.Here's an example of Logical Segmentation.We have a Router or multilayer Switch that connects to two access Switches.Those Switches are trunked together.An initial VLAN, VLAN 100 has been segmented logically into private VLANs.The Communities, VLAN101, VLAN102,and VLAN103 will only send traffic to other members of the Communityand northbound to the switch on the Promiscuous port to the Routeror multilayer Switch.They will not send frames to any other Community,nor will they send anything to an Isolated device.In VLAN199, the three servers and VLAN199, which is isolated,will not send traffic to each other.They will not send frames to any other Community.They will only basically respond to send traffic back to the Switches and thenon the Promiscuous port to the Router or multilayer Switch.

#### Centralized vs. Decentralized Design

Centralized Design. Centralized systems typically deploya client/server architecture,where one or more client node communicates directly or through a proxy,otherwise known as mediated access,like an access gateway or authentication server with a central server,either physically or logically.This is the most common type of system in many organizations,where a client sends a request on the corporate intranet, for example,Microsoft SharePoint or an email server and receives a response.Common attributes of a centralized environment are the presence ofa global clock, where all the client nodes sync up with the main clock ofthe central node, often using NTP version 3.Also, there's one highly available central node that coordinates allthe other nodes in the system.Central node failure causes the entire system to fail because ifthe server is down, no other entity is there to send or receive responsesor requests. Centralized servers can, in many cases, leveragea hierarchy of intermediate down-level servers,for example, in the Domain Name Systemor with hierarchical certificate authorities.Here we see a Centralized Wide Area Network.The Corporate WAN will have a central head endor headquarters that connects to four different Branch offices overthe wide area or metropolitan area network.Each Branch will have a customer premises equipment,usually a Router or multilayer switch,or, in many cases, it can be a firewall appliance.Decentralized Design.In a decentralized system, every node makes its own decision.The final behavior of the system is the aggregate or the accumulation ofthe decisions of each individual node or host.In a decentralized design,there is no single entity that receives and responds to the request.The requests are broadcasted or multicasted to the decentralized architecture.Common attributes of a decentralized environment is that there is no globalclock, as each node is independent of each otherand therefore may have different clocks that they run and follow.However, if all of the devices in the centralized environment connect toan atomic clock, they can still have synchronized time.Decentralized systems have multiple or shifting nodes and more than one unit,which can listen for connections from other nodes.One central node failure causes a part of the system to fail;but not the whole system.In this diagram, we see a Decentralized Database Network.Starting from the upper right-hand side,we can have a Decentralized database in a full mesh connection toa Decentralized Data warehouse, a database cluster,a Decentralized SQL database,or a data lake at the upper left-hand corner of MySQL, ORACLE, and SQL Server.In the next lesson, we'll talk about one ofthe most important technologies that would support a centralizedand decentralized environment. And that's virtualization.

#### Virtualization

Virtualization. Virtualization is the process of running a virtualor abstracted instance of a computer system in a layer abstracted fromthe underlying hardware service.It most often refers to running multiple operating systems and applicationson a single computer system,or a stack or cluster simultaneously. To the applications running on top ofthe virtualized machine, it can seem as if they'reon their own dedicated operating system with their own libraries,dynamic link libraries, associated programs, and hardware resources.Hypervisor is a virtual machine manager system.It's the operating system and software that runs one or more virtual machines.Hypervisors control the interaction between the virtual machines andthe underlying hardware. Type I hypervisors are referred to as bare metalor native. These run directly on the underlying hardware, such as XenServer,Linux based KVM, Microsoft Hyper-V, and VMware ESXi. Type II is hosted.This runs on an operating system installed on the hardware, for example Linux,macOS or Windows 11. This could be Oracle VirtualBox,VMware Player or VMware Workstation, to name a few.With the Type 1 Hypervisor, the underlying Hardware in the data center,let's say a high-end Cisco server will havethe Hypervisor directly installed onto it.Then your Guest operating systems and Applications, either one, two, or many,run within the Hypervisor,the virtual machine manager running directly on the Hardware.In this diagram we see what's called native or bare metal.In this diagram we see Type 2 where on the Hardware,let's say the Dell or the HP Workstation,you install a Host operating system like Ubuntu Linux, Red Hat or Windows.Then you install the Type 2 Hypervisor, let's say VMware Workstation Pro,into the Host operating system,and it's going to run simultaneously with other applications and serviceson the Host operating system.Then within the Type 2 Hypervisor,you can run one or more guests with applications.Remember, without virtualization, there is no cloud computing.

#### ICS and SCADA

Supervisory Control and Data Acquisition (SCADA) Systems.SCADA systems represent the software used to collectand send data throughout facility and manufacturing systems,often to cloud services or other third parties.Programmable logic controllers, PLCs,and other embedded systems are common hardware components.Anytime a system is not air-gapped, in other words, not connected to a network,including the Internet will introduce various threats and vulnerabilities.An industrial control system, a subcategory of SCADA,is a combined term that represents varied forms of control systemsand related instrumentation, which include the devices, systems, networks,and controls used to operate and/or automate industrialand mechanical processes.Each ICS typically functions differentlyand is built to electronically manage tasks efficiently.Many of these tasks include monitoring and sensors.Modern devices and protocols used inan ICS are used in nearly every industrial sector as wellas critical infrastructure.SCADA and ICS systems are found in facility and manufacturing controland management systems, water management systems, electric power grid,nuclear power grid, solar, and wind farms,and traffic signals and mass transit systems,and environmental and manufacturing control systems.In the next lesson, we'll define Internet of Things, IoT.

#### Internet of Things (IoT)

In this short lesson, let's officially look at something I've mentioned severaltimes already in this training that is IoT, Internet of things.The term IoT refers to the collective network of connected devices andthe technology that facilitates communication between the devices and usuallythe cloud, as well as between the devices themselves.With the advent of inexpensive computer chips and high bandwidth networking,there are now billions of devices connected tothe Internet using IPv4 and moving forward, IPv6.Everyday devices like toothbrushes, vacuums,automobiles, any type of machine can use sensors to collect dataand respond intelligently to users.Examples of IoT would be mobile devices and corporate fleets of trucks, cameras,farm and ranch equipment,sending and receiving telemetry information to and fromthe cloud, sensors of many types,smart appliances, facility automation systems such as programmablelogic controllers, medical devices, and medical systems.The aforementioned vehicles and aircraft, including drones,smart meters, and embedded devices, and real-time operating systems.RTOS, including system on a Chip, Raspberry Pi and Arduino systems.

#### Course Summary

In this course, you learned about architectural considerationsand cloud computing, Infrastructure as Code, serverless technologies,containers, microservices, and centralized vs. decentralized design.And virtualization, ICS, SCADA, and IoT.Coming up at the next course,we'll explore enterprise infrastructure security principles.

### Enterprise Infrastructure Security Principles

#### Course Overview

Enterprise infrastructure security principles.In this course, we'll discover infrastructure security considerations.Explore port security and firewalls.Define IPsec and TLS virtual private networks (VPNs)and examine SD-WAN and SASE.

#### Infrastructure Considerations

Security zones.Zoning is a logical design approach used to mitigatethe risk of an open network by segmenting infrastructure services.Each zone has fundamental characteristics, defined by the security policy:Every zone contains one or more separate, routable networks or VLANs.Every separate, routable network or VLAN is contained within a single zone,it cannot transcend more than one zone.Every zone connects to another zone using a perimeter that containsa zone interface point (ZIP), usually a multilayer switch or a firewall.The only zone that may connect to the public zoneis the public access zone (PAZ) or DMZ.In the diagram on the right, under our administration arethe controlled zones, restricted zones, public access zones.The uncontrolled zones are on the Internet: Internet service providers, ISPs,ITSPs who also provide voice over IP with Internet and cloud service providers,among others.Here we see restricted zones which have no access to the Internet,for example a management private VLAN or the data centeror middle-tier application services.A zone interface point is between each zone.It can be a firewall with an interface in each zone,or each zone can have its own firewall or multi-layer security switch.Again, in the public access zone,this is the only zone that connects tothe Internet tier hosting public-facing services and servers.This logical segmentation is critical as a defense in depth mechanism.In this diagram, we have several zones.We have the outside zone ofthe firewall which has our customer premises equipment or our router or router.We have a Honeynet VLAN zone.We have a corporate VLAN zone, an Intranet VLAN zone,and the DMZ or public access zone.These zones could be partitioned logically and physically, or they may not be.For example, the Intranet VLAN and the Honeynet VLAN could actually be inthe same physical location but in different logical zones.The attack surface consists of all possible attack vectors thata threat actor can use to access a system and extract data.It represents the targets of a cyber kill chain.The smaller the attack surface,the easier it is to counter or mitigate with various controls.The attack surface is split into two categories: The digitalor logical attack surface and the physical attack surface.Enterprises must continuously monitor their attack surface to recognize, expose,and block potential threats as quickly as possible.They must also endeavor to minimize the attack surface area,to reduce the risk of successful attacks, or attacks that can affecta lot of endpoints, for example,distributed denial-of-service attacks or worms.Attack surface reduction becomes more difficultas organizations expand their digital footprint and leverage new technologies.On the Security+ exam, you also want to be aware of failure modes.Certain security infrastructure devices such as firewallsand IPS sensors can be deployed in fail-open or fail-closed modes.Fail-open means that even if there's a system or component failure onthe device, IP traffic should continue to flow to zones onthe outbound interfaces. In fail-closed mode,the device will stop processing packets.For example, if one of the failover interfaces to a standby device shuts downor fails.

#### Network Appliances

In this lesson, we're going to explore various network security infrastructuredevices and components, beginning with the IDS and IPS solution.An intrusion prevention system (IPS) is a network security hardware andor software solution that continuously monitors a zone or multiple zonesfor malicious activity.It then proactively takes action to prevent it inthe line of traffic. We call this an in-line solution.IPS is more advanced than an intrusion detection system (IDS),which reactively detects malicious activity.IPS systems are often integrated into security appliances or part ofa next-generation firewall (NGFW) or unified threat management solution (UTM).In this diagram, we'll seethe traditional difference between intrusion detection systemsand intrusion prevention systems.On the left-hand side, we have an IDS sensor.Now, this can be integrated into a firewall, or a multilayer switch, or evena router. In this case it's connected to a port on a Layer 3 switch.The Layer 3 switch is using a protocol knownas SPAN, to send copies of frames to the IDS sensor.The IDS sensor is not in-line.It's basically comparing its signatures, its anomaly knowledge base,heuristic rules, and other techniques to the copies of the frames.Therefore, if there is an attack against servers or systems onthe corporate network, they will deliver the payload.The IDS can only react to things that's detected,as it sends information or alerts to the management station.On the right-hand side, we have an intrusion prevention system.This could be integrated into a router or a firewall,or it could be a standalone solution.Regardless, the IPS sensor's in-line behind the firewall.In other words, it only acts on the traffic that is permitted fromthe firewall, but it's in-line so we can drop trafficor block traffic, that's malicious before it gets to the corporate network.Most systems today are IPS systems, which are initially deployed in an IDS mode,in other words, a passive or monitor mode.So you can reduce the false positives and properly tune it,then put it into in-line mode.Actions that IPS can take would be alerts and alarms, for example,to a SIEM system. Verbose dumps,which actually dumps the contents of the packet.TCP resets, drop packets or addresses in-line.Block or shun on firewalls and routers,send SNMP traps or informs, log to Syslog and security informationand event management (SIEM) systems, and or, send flows to NetFlow collectors.Remember, IPS can do everything on this list.However, an IDS system cannot drop packets or block addresses in-line.They can still take aggressive actions like sending a TCP reset to the senderand receiver, or sending a block or a shun on an upstreamor downstream firewall, or router.Regardless, you have to tune your IDS and IPS systems.We want true positives, true being accurate and positive, being an action taken,and true negatives where we accurately did not take an action.The tuning involves reducing false positives,which is an error state when a positive action was taken, for example,at a certain time of the month where employees have to change their very longand complex passwords, you may see more false positives of failed logins.Those must be reduced, and false negatives an error when an action wasn't taken.In other words, the payload was delivered and the IDS or IPS did not detect itor prevent it.Historically, when administrators use Secure Shell 1 or Secure Shell 2or RDP to Microsoft systems, instead of directly connecting to those devices,they'll connect to a jump box or what's called a Bastion server.So here we see a cloud environment, for example, Amazon Web Services.The remote administrators are not going to connect directly tothe MySQL instance or another EC2 instance.They'll deploy a virtual machine,for example, Ubuntu Linux or Windows Server in a public subnet.They authenticate and authorize to that jump host or that jump boxand from there they will administer and manage virtual machines and services inthe backend subnet.Jump boxes and jump hosts are just one example of proxy serversor mediated access. We can have proxies for authentication.They can be interactive or transparent.If they're interactive, they're going to present the user with a login screen,for example, a console or a shell or a captive portal.If it's transparent, they'll authenticate and authorize transparentlyon behalf of the client and server.They can do translation services, for network address translation (NAT),dynamic NAT, port address translation, and other mechanisms.They can be managed services from cloud service providers (CSP)to provide Bastionor jump services. They can be web proxies, for content storageand website security. They can do URL filtering,preventing internal users from going to certain sites on the Internet.Mediated access can be offered by managed security service providers (MSSPs),for content security and data loss prevention.It can be a service from acloud access security broker (CASB) to assist with singlesign-on and federated access, data loss prevention, and other compliance issues.Another common device, is a load balancer. In your Intranet,the load balancer is often used, to improve the experience forthe application clients. It can be a single-site or a multi-site load balancer,where the traffic to and from the services is being load balanced acrossactive-active or clusters of application servers.This provides reliability,it provides fault tolerance and, it improves the delivery of video, audio,voice over IP, conferencing, and other services to the application clients.Load balancers are extremely popular in the cloud.When you use your web browser, it's one of the common places you go to.When you click on a hyperlink or put in the domain name of a website.Cloud load balancers can support IPv4 and IPv6,typically using Transport Layer Security, but also things like WebSocket APIs.The load balancer, will often run a certificate server ora TLS offloading service,so that it can decrypt the trafficfor the layer 7 web application firewall (WAF).The cloud load balancer, produces flow logs for active defense.It can also do health checks against backend targets.For example, auto scaling groups of instances, or appliances, functions asa service, containers as a service,or even IP addresses, using a wide variety of load balancing algorithms.

#### Port Security

In this lesson, we're going to survey some common port numbers, TCP and UDP,that you need to know for the Security+ exam.On the left-hand side, realize that FTP uses two different ports, port 20for the actual data and port 21 for the command and control.If you're using FTP over SSL/TLS on the right,then the ports would be 989 for data and 990 for control.SMTP, which sends email to mail exchangers,uses TCP port 25 or for SMTP over SSL/TLS port 465.HTTP or for the World Wide Web, is going to use port 80for Cleartext TCP and for HTTPS or SSL/TLS, it'll use port 443.POP3, a simple way to download email is going to be port 110,but commonly, you're going to use POP3 with TLS on port 995.Realize that all of these services here use TCP.NNTP or Network News Protocol,is an older protocol, that uses port 119 or with TLS 563.A very common client for email is IMAP, which uses TCP port 143 in Cleartext,but in a TLS environment which is most common, it'll use port 993.Telnet is not used anymore, but that was on port 23.But today you should be using SSH or some substitute like SCPor secure FTP, all of those use port 22.DNS can use TCP or UDP port 53, and, NTP uses port 123and again, all of these services other than DNS, when performinga query, are going to use TCP.802.1X PNAC.IEEE 802.1X authentication is also referred toas port-based network access control, or PNAC.It involves making sure something interfacing witha system is what it claims to be.When someone wants to gain access to an Ethernet or a 802.11 wireless network,.1X will verify the entity connecting is who they say they are,in very flexible ways. Different devices, different locations,different scenarios.The capabilities include, pre-admission control to blockunauthenticated messages.Identifying users and devices with predefined credentialsor machine identifiers. Conducting both authentication and authorization,and .1X uses RADIUS for its authentication, service.Onboarding and provisioning devices in a Zero Trust environment,for example, at Google, and supporting ABAC: Attribute-based Access Control.In the .1X architecture,the devices that want to get on the network are called supplicants.The devices are called supplicants, but also, they're often running an agent,either a native agent to the operating system or something likethe Cisco AnyConnect Mobility Client.Something else .1X will also do, is identify certain ports, for example,Ethernet ports to be reserved for non-supplicant devices, like printersand bizhubs. The .1X supplicants will send EAP messages to authenticators.These could be wired access switches,access gateways or serversor, wireless access points managed by wireless LAN controllers.They will communicate the credentials back to the authentication servers,which are going to be RADIUS or the newer version, Diameter.The identity source actually could be on the RADIUS server,but more often than not, they're going to communicate back withan identity provider, such as,Microsoft Active Directory or some PKI certificate authority.Then, if allowed, they will send the authenticationand authorization information back to the Authenticators,and they can make a decision, to either put the supplicants on a guest VLANor a restricted VLAN, or let them get access to the networkor deny them altogether, and because it supports attribute-based access control,it can have authorization policies, for a wide variety of attributesand characteristics of the supplicants.Extensible Authentication Protocol (EAP),it's an authentication framework as opposed toa specific authentication mechanism.It has evolved over the years, from the original Point-to-Point Protocol (PPP).It's often used in 802.1X wireless networksand point-to-point connections.It offers some basic functionsand negotiation of authentication methods called EAP methods.In the diagram we saw, there was an original EAP over LAN (EAPOL)exchange before the higher methods are implemented.Here's a table of the higher methods. Realize that we don't use MD5 anymore.The most secure environment would be EAP-TLSwhich uses X.509v3 certificates and, performs client-sideand server-side certificates.This has the most difficult deployment, because of having to introducea public key infrastructure or enterprise certificate authorities.With tunneled TLS, you don't have to have a client-side certificate,just a server-side certificate.Protected EAP is very common in Microsoft environments.They actually invented it and then EAP-FAST is a Cisco solution.It's a very secure optionfor Cisco environments that don't want to use EAP-TLS with X.509v3 certificates.Instead, it'll use a PAC file on the client and the server.

#### Firewall Types

The most basic type of firewall is the static or stateless access control list,and this goes way back to IP filtering daemons,on Unix and Linux boxes. An access control list is simplya list of access control entries that permit or allow and deny traffic, basedon the headers of IP and TCP, or IP and UDP, or IP and ICMP, access listsor ordered lists.So they start with the first entry and process allthe way down and as you can see,they're matching on, different protocols and services,different ports and port numbers, which you should already be familiar with.And notice the final entry is an explicit deny, so that if nothing matches,it will send a log, for example, to an SNMP server or a SIEM system.This entry is not necessary unless you want to log,because ACLs have an implicit deny at the end, so that if nothing matches,it will deny the traffic.Notice that we are equaling different services, and port numbers.WWW can actually represent, as a placeholder or a tokenfor a number of different ports.So, for example, you can have HTTP traffic on port 80 or 8000 or 8888,whatever you want that placeholder to represent.We're also equaling FTP or FTP dataand remember we could use the port numbers for that, 20 and 21.On the exam, it's extremely important, that you understand how to constructand interpret access control lists.Here's a network access control list at Amazon Web Services.You can see that our rule numbers start with 100.We're matching on different services and ports, and protocols. For TCP or UDPor ICMP, and then we are allowing or denying based on the source address.Here we see the source as a CIDR or CIDR representation, C I D Rmeaning any source, 0.0.0.0/0.If you're using a network ACL in the cloud,it also has an implicit deny at the end.In other words, if there is no match, it will deny the traffic,and remember, access control lists are static or stateless.In other words, they have no idea if the traffic, is part ofa TCP 3-Way handshake, or an existing TCP connection,or if it's a UDP response, for example, from DNS,or if it's part of ICMP messages,it just simply looks at each datagram or packet one at a time,compares its list of entries, to the headers,and makes an allow or permit or deny decision.Here we see a stateful cloud-based firewall.These types of firewalls, are what we call allow lists.So, this has inbound rules and outbound rules.But they are placed on a specific instance.So, technically they're applied tothe Eth0 elastic network interface of the Windows or Linuxor macOS virtual machine instance.Here, in this allow list we are allowing HTTP, HTTPS,SSH, and RDP traffic to this frontend web serveron both, IPv4 and IPv6.When it compares these rules to the packet or the datagram,if there is not a match, then it's not allowed.Everything on the list is processed before the decision is made.There's no numbers and there's no allow or permit or deny capability.This is an allow list, you cannot have explicit deny entries.If you want to have explicit deny entries, use an access control list.Most firewalls today are next-generation firewalls.Whether you deploy these on-premises, or you decide to deploy a virtual Ciscoor Juniper or Palo Alto firewall in the cloud, they have common characteristics.Remember, a firewall is just a metaphor representing software andor hardware controls, that limit the damage spreading from one subnet, VLAN,zone, or domain to another.That's the main goal of a firewall - preventing the fire from spreading.The main purpose of a firewall is not to prevent a fire from starting.We use antivirus, anti-malware,and endpoint detection and response, along with acceptable use policies, toprevent fires from starting. A firewall is typically deployed as a barrier,a zone interface point, between an internal trusted networkand some external untrusted network.They are integrated systems of threat defense functioning at layer 2,the data link layer, all the way up to the application layer,doing deep packet inspectionand can be categorized as a network firewall, or in the case ofa web application firewall, an application firewallor an application layer gateway.Next-gen firewalls, regardless of their deployment,provide policies for layer 5-7.This is called advanced visibility and control or deep packet inspection.This goes beyond just simply doing a stateful or stateless firewall,making decisions on the layer 3 and layer 4 headers.This can look deeper into the packet and permit and deny based onthe behavior of the application, for example, HTTP, SMTP, DNS, FTP, and more.They can perform authentication proxy, that can be interactive where theypresent some login screen or captive portal, or they can do it transparently.They can integrate with identity servicesfor attribute-based access control (ABAC)and advanced identity management, either on-premises or in the cloud.They often have integrated IDS/IPS,also cloud-based solutions that can provide content security with URL filteringand data loss prevention engines.Next-gen firewalls are well known for their cloud correlation and integrationfor advanced malware protection,including the usage of machine learning (ML) and AI engines at vendors or inthe cloud provider.They provide botnet filtering, for advanced distributed DoS protection,and, they provide unified threat management fora variety of traffic besides just data traffic.Let's talk more about unified threat management, or UTM.Most modern networks transmit more than just basic data,transit and email traffic.UTM, typically offers multiple security features and services on a single,highly-available network device, for example,a Cisco Adaptive Security Appliance.It can further protect email, webmail, fax, voice over IP or IP telephony,conferencing, streaming, peer-to-peer file transfer services, and more.UTM could be considered the first huge step, to evolve into modernnext-generation firewall solutions.Web application firewalls (WAF).A WAF is also called a web security gateway or WSG,it is usually an appliance, physical or virtual, a server plugin,or a virtual firewall running in a hypervisor or in a public cloud deployment.It protects HTTP, Cleartext on port 80 and HTTPSor TLS traffic, at layers 5 through 7 of the OSI reference model.Typically, these web access control list rules cover common web attacks, suchas cross-site scripting (XSS), request forgeries,and various injection attacks, like SQL.They are typically deployed as dynamically configured WebACLsand Anti-DDoS engines, along with other threat management services,either in a hypervisor, in your own data center, or at a cloud service provider.The Amazon Web Services (AWS) WAF is commonly deployedon elastic application load balancers or public-facing load balancers,content distribution networking nodes, or API gateways.

#### Examining Virtual Private Networks (VPNs)

For this demonstration, we're going to look at setting up a rapidly deployed,easy to configure, site-to-site VPN, using IP security (IPsec) atAmazon Web Services.An AWS site-to-site VPN connection is made up of twocomponents. The first component, is the Customer gateway.We already have a Customer gateway here, for example,a remote branch office (RBO) in Austin, let's see what we do to create it.It's very simple, we just name it,so for example RBO-Austin,for our dynamic routing protocol,we're going to use border gateway protocol (BGP),it's our only choice at a cloud provider.If you don't have your own autonomous system number, you can just usefor example, 65000. Then, how do we identify the device on the other side ofthe VPN tunnel? For example,the headquarters or regional branch office or regional office?Let's say that device is an ASA 5500, from Cisco. There's two ways to do it.We could put in here the public IP address, of the customer premises equipment,the ASA 5500, that would be fine. That would be a static configuration.But realize, that may change,especially if it's a broadband provider giving us a dynamic IP address,or, if we decide to change service providers,we'll have to come back here to AWS in this interface and change the IP address.A more scalable way to do it,and an easier way, would be to go to the Certificate Manager service ofthe cloud provider and generate an X.509v3 certificatefor that particular ASA 5500.Then, you could load it up into memory on this device, and simply come back hereand select it from this dropdown.You also want to add new tags and make sure that you are doing thisfor configuration management and ongoing inventory visibility.That's all you do.That's the customer-side.Let's go to the provider-side and look at the Virtual private gateway.We already have one called MY-VPG1000. But let's see how we create that.Basically you just name it.We're going to go ahead and use the Amazon default ASN,the BGP autonomous system numberand we simply add some tags, and then createthe VPG.Realize that for other providers it may be called something differentbesides a virtual private gateway.It may be called a VPN gateway or something like that.Now that we have a customer gateway configured anda virtual private gateway configured,we'll just set up our Site-to-Site VPN connection.We don't have one, so let's create a VPN connection.We will want to name this.We'll call it VPN to the Austin RBO.It's going to be a Virtual private gateway,and we can select the VPG from the dropdown, there is one.Then use an existing Customer gateway, we have one of thoseand then we're going to use Dynamic routing using BGP.When you're doing this in a managed site-to-site VPN,it's really very easy and very friendly in the sense that it offersa lot of defaults. Also, you have options for the actual IPv4 ofthe network. So, this is the CIDR range on the on-premises side,the customer-side. You could just go with anyor you could actually put in whatever the prefix is on the other side,the headquarters or the regional branch office.And then what are you going to have on the cloud-side?Again, we can just go with anyor whatever you're going to use in your VPN subnet at the cloud provider.You could put that CIDR range in there as well.Next, we have Tunnel options.When you're using IPsec you actually have two tunnels that are logicaland the inside tunnels also have to have IPv4 addressing.The good news is, we can just use the cloud provider and they'll provide it.So for example, they're going to give us an inside address from the A P I P Aor the APIPA range, 169.254so we don't even have to come up with that,if we don't want to, the cloud provider will generate that.Then we can see things like logging, turning on activity logs, things like that.Now, the next thing we would do here, if we wanted to is to editthe Tunnel 1 options and that is going to be the IPsec informationand we're going to look at that in the next lesson.

#### Examining IP Security (IPsec)

In this demonstration, we're going to continue where we left off inthe previous lesson in looking at the IPsec information.With IPsec, you have two options,you can use ikev1, which is the older, really deprecated version.It has two phases, phase 1 is a peering session between the two endpoints,for example, between this virtual private gatewayand that Austin RBO ASA.Generally, you're going to use ikev2because ikev2 is going to be more robust.It's going to support a newer security suites and security algorithms.So with IPsec, both sides have to come toan agreement of what they're going to use for encryption,what they're going to use for an HMAC, for integrity and origin authentication,what Diffie-Hellman group are they going to use to createthe shared-secret keys over this untrusted network?So, the good news is, on the provider-side, at the cloud provider,they're going to pretty much offer everything, from older,less-secure versions of SHA, to kind of deprecated Diffie-Hellman numbers,but also up to the more robust solutions.So, the good news is, on the customer-side,I'm going to be offering up a combination of these.It's called a policy or a suite, maybe even a profile.I'm going to make an offer, to this virtual private gateway,generally with a more secure options.So, regardless of what I choose,I'm going to find a match, and a general concept between the client andthe server, or in this example,between that customer gateway, in this virtual private gateway in the cloud, isI want to use the most robust options that both sides would support.So, if I have a new Juniper or Palo Alto or Cisco appliance at let's say,the regional branch office, then I'm going to go ahead and use AES256-GCM-16I'm going to use SHA-512, becausethe new device will support these more secure optionsand these higher Diffie-Hellman numbers,which are going to use elliptic curve cryptographyand that's definitely the one I want to use today over untrusted networks.So, I'm probably going to use something in the 20s when it comes tothe Diffie-Hellman, and then like I said,we're probably not going to use ikev1, we want to use ikev2and the lifetime, by the way,doesn't have to match. On the virtual private gateway side,it's going to do 12 hours, which is 28,800 seconds.It may be, the offer being made on the other side is for 24 hours, which is,you know, 49,000-something seconds. Doesn't matter, they don't have to match.But, when the security association is set up between the two devices,the tunnel is set up. It'll use the lowest lifetime between the two,because that's more secure.With IPsec if we're using the Encapsulating Security Payload Protocol,which we always use over the Internet, not the authentication header,we're going to have encryption.So, you have to match on the encryption algorithmsthe integrity H-max that you're going to use, and the Diffie-Hellman numbers.

#### Transport Layer Security (TLS)

In this short lesson, let's do a deeper dive into Transport Layer Security.Transport Layer Security, or TLS,is the latest iteration of Secure Socket Layer (SSL).TLS is the most ubiquitous certificate-based peer authentication in useon the Internet.In other words, it's HTTPS.TLS 1.3 is the most recent published version and should always be used unlessthe client only supports version 1.2.TLS includes a core Record protocol andan additional highly extensible Handshake protocol.It's the Handshake protocol that continues to add extra security measures.TLS is also used with SMTP, Lightweight Directory Access Protocol (LDAP),and Post Office Protocol 3 (POP3), just to name a few.The only mandatory cipher suite, includes RSA for authentication,AES for confidentiality, and SHA for integrity and digital signatures.Although TLS is TCP-based, most servers today don't use TCP.Instead, they perform a single-packet authentication with mutual TLSor mTLS instead.Traditionally, TLS used TCP.Therefore, in step 1, when single-packet authenticationand mutual TLS are not used, the user will make a TCP connection to port 443.The server response, contains the server's public key.Then, the user software verifies the signature on the identity certificate,validating the authenticity of the public key.The user software, then generates a shared-secret key or session key.The shared-secret session key, encrypted with the public key of the server,is then sent back to the web server.Finally, bulk encryption occurs using the shared-secret key witha symmetric encryption algorithm, usually AES-128 or AES-256.

#### SD-WAN and SASE

Software-defined wide area networking (SD-WAN)is a software-defined networking (SDN)approach, that raises network traffic management away from the hardwareand premises to next-generation software in the cloud for superior agility,control, and visibility.SD-WAN incorporatesa centralized control function with user-defined applicationand routing policies to deliver highly secure, robust,application-aware network traffic management.SD-WAN is also called SD-MAN for a metropolitan area network fiber deployment.Here we see an SD-WAN solution with Microsoft Azure.With this Azure solution,you're going to have a virtual WAN component running inthe Azure cloud. Behind that virtual WAN controller,you'll have multiple virtual networks (VNet),and enterprise control to automate and orchestratethe various remote offices from your headquarters and SD-WAN, branch offices,regional offices, remote branch offices,and small office-home office locations.All of the sites are being managed through API calls to their customer premisesequipment, automated and orchestrated from enterprise SD-WAN controllersrunning in the Azure cloud.Then, you can make connections over the wide-area networkor metropolitan area network with IPsec IKEv2or Transport Layer Security.You can also have federated access or single sign-on access to Software asa Service solutions such as Office 365, Workday, Salesforce, and many others.Secure access service edge (SASE) isan architecture that delivers converged networking and Security asa Service (SaaS) capabilities to your SD-WAN or SD-MAN,as well as cloud native security functionality, with your secure web gateways,cloud access security brokers, firewall as-a-service, for example,Fortinet FortiGate, and zero-trust network access initiatives (ZTNA).Consider SASE, as the secure overlay to your SD-WAN or your SD-MAN.With functions delivered from the cloud and provided asa service by SASE vendors such as Cisco Systems or Fortinet, among others.Here, all users, regardless of where they are at home, on the road,in a hotel or remote branch office,get secure access through controllers to a wide variety of SaaS solutions tothe public cloud of AWS, Google Cloud, Microsoft Azure, IBM Cloud, Oracle Cloud,and others, as well as limited access, to services and data, running in theon-premises or on-site data center overthe public Internet, in metropolitan areas all over the world.

#### Course Summary

In this course, you learned about infrastructure security considerations,port security, and firewalls.IPsec, TLS, and virtual private networks, and SD-WAN, and SASE.Coming up in the next course,you'll explore data protection concepts and strategies.

### Data Protection Concepts & Strategies

#### Course Overview

Data Protection Concepts and Strategies -In this course, we'll discover data states, data classification, data types,and the data life cycles. We'll examine secure data considerations such as: geographic and cultural restrictions;encryption and hashing;masking, obfuscation, and tokenization; and segmentation and compartmentalization.

#### Data States

In this first lesson, we're going to look at thethree different types of states of data, starting with 'Data At Rest.'Data at rest is data that has arrived at a destination in a file system,database, or object storage, for example, disks, hard disk drives,solid state drive (arrays or tape) and is not being accessed or used,for example, over a network or loaded into RAM memory.It typically refers to stored data and excludes data that is moving acrossa network or is temporarily in computer memory, or let's say,a Redis cache waiting to be read or updated.Data at rest is data that is not dynamically moving from device to deviceor network to network.Next, we have 'Data In Transit.'Data in transit is typically being packet forwarded or switched overa wireless or wired network in a unicast, broadcast, multicast,or with IPv6 Anycast fashion.Examples of data in transit include:Wired Ethernet,cable or DOCSIS,fiber optic,802.11 wireless,cellular,satellite data,and personal area networking using RFID, NFC, Bluetooth, Infrared,Zigbee, and more.Our third data state is 'Data In Use.'This is active data undergoing processing, translation, analysis,change, or other manipulation.Examples of data in use include:data in system RAM memory,data in CPU registers,in caches and buffers,data in Memcached or Redis clusters,database transactions,and cloud-based file or code being modified in real-time by one or more users.

#### Data Classifications

In the early phases of the data life cycle,it's important to classify data if it's pertinent to your environment or your access control models. In this lesson, we're going to talk about data classifications,starting with ones that are used commonly in government and military environments. Sensitivity is based upon a calculation of the damage to privacy and security that an exposure of the information or data would cause. The US has officially three levels of classification: Confidential, Secret, and Top Secret. If one holds a Top Secret security clearance, they are allowed to handle information up to the level of Top Secret,including Secret and Confidential information. If one holds a Secret clearance,they may only handle Secret and Confidential classified information –but not Top Secret.Public and commercial entities also use data classifications.There are five common categories usedfor data classification in various business and commercial sectors:public data,private data,internal data.confidential data,and restricted data.Public data may be important, but it's accessible to the public.Since this data is openly shared,it is the lowest level in a public or commercial classification.Private data requires a greater level of security than public data.It should not be available for public accessand is often protected through common security measures such as passwords.Internal data is usually limited to employees onlyand often has different security requirements that affect who can access itand how it can be used.Confidential data is information that should only beaccessed by a limited audience that has obtained proper authorization,usually in a attribute-based ora risk-based access control environment using strict identity management.And then the restricted classification is reserved foran organization's most sensitive information.Access to this data is strictly controlled to prevent its unauthorized use.For example, critical intellectual property such as corporate secretsand formulas, patents, Break The Glass credentials, and other critical data.

#### Data Types

Now that we've learned about Data States and Data Classification,let's look at Data Types.Regulated data, its use and protection is dictated by a government agencyor third-party agreements.Trade secrets - This is any practice or process ofa company that is generally not known outside of the company.Intellectual property - This is creations or generations of the mind, such asinventions, literary and artistic works, designs and symbols,names, and images used in commerce.Personal health information (PHI) - This is thedemographic information, including medical histories,test and lab results, mental health conditions, insurance information,and other data.Personally identifiable information (PII) -This is any representation of data that allows the identity ofan individual to whom the information appliesto be reasonably inferred by either direct or indirect means.Legal information - This involves the careful reading about specific clausesor stipulations that does not constitute "advice."Legal advice is a separate category from legal information.Financial data - This is quantitative informationand data used by organizations to make financial decisionsas well as data concerning a company's financial health and performance.Human versus non-human readable data -Some human-readable formats, such as PDF, are not machine-readableas they're not structured or semi-structured.For example, a relational database, a document database, a graph database,or semi-structured (JSON or YAML) data.In the next lesson, we'll look at the data life cycle.

#### Data Life Cycle

In this lesson, we're going to survey the Data Life Cycle.And there's more than one data life cycle out thereif you search the World Wide Web.But this is the data life cycle you need to know according to CompTIA withthe Security+ exam.Phase one is Create,phase two is Store,phase three is Use,phase four is Share,phase five is Archive,and phase six is Destroy.Let's look at these one at a time.In the Create phase, data is either generated from scratch, inputted,acquired, purchased, or modified into another format.The data owner, stewards, and custodians (if applicable)are also identified in this earliest phase.Other key activities of phase one include:data discovery, for example, eDiscovery to assess what data assetsyou have or discovery for forensic purposes;data categorization;data classification as we saw earlier in this course;data mapping, for example, mapping a city to its airport code;and labeling and tagging data based on an established schema.Remember, the Create phase is one of the mandatory phases of the life cycle.Also, the concept of data minimization should be applied.In other words, don't create data, don't generate it, don't purchase itor acquire it unless it has utility,unless you want the data to become informationand information become knowledge.Phase two is the Store phase.Here's where data is put into a volume or block storage,object or blob or file system storage,or into one of several types of database systems.This phase relates to the optional transactional, near-term usage of dataas opposed to long-term cold data storage.Activities of this phase can also occur simultaneously whenthe data is generated or acquired in phase one.Protection of data at rest and data in transit will often occur in this phaseunless default encryption is employed in the Create phase.Realize however, the Store phase is optional.The Use phase is our second mandatory phase.Data is utilized by people, applications,services, and tools as well as being changed from its original state.This is where raw data becomes information, then knowledge,and then potentially wisdom.If data is used remotely then protection mechanisms must be in place.For example, a cryptographic virtual private network, IPsec or TLS,secure endpoints or virtual interfaces,or digitally signed API calls and requests.The systems that "use" the data must be secured as well;for example, endpoint detection and response (EDR)or host-based IPS agents like Palo Alto Cortex XDR.Next, we have the Share phase.This is an optional phase where data is visible, analyzed,and apportioned among multiple users, systems, and applications.Data can be shared in a client-server model,peer-to-peer, or distributed manner.Global collaboration and sharing of data introduces obvious risks andthe potential lack of control.Most of the control used in the previous phases will be implementedor leveraged here in phase four,including things like information rights management (IRM)and data loss prevention services.Stringent Identity and Access Management and/orIdentity Management should be used to enforcethe least privilege when data is shared.In the optional archive phase,data is stored for the long-term and removed from active usage.Archiving is based on regulations,governance policies, and/or simply best practices and guidance.Stringent cryptography will be introduced for data at rest when archived –as in AES-GCM-256 AEAD solutions.AEAD stands for authenticated encrypter with associated data.When you're using AES-GCM, you don't have to have a separate HMAC,it has its own integrated GMAC.Archiving is often automated and based on things like Intelligent Tieringor using Storage Gateways with edgeor hybrid cloud that's managed over high-speed connections to cloud providersand other vendors.Cost of the archival phase are based on retrieval options.The quicker you want to retrieve the data, the more costly it is.The final phase, phase six is the Destroy phase.This is also called the Destruction phase or Disposition phasebut it's where data is no longer accessible or usable basedon lifetime, utility, policy, governance, and/or regulations.The organization should have their own established methodsfor disposal of data and media,often using military grade programs or physical destructionsuch as crushers and furnaces.Although data can be disposed of using a variety of methods,on the exam, when you store data at a cloud provider, for example, Google Cloud,crypto-shredding or cryptographic erasure is the only practicaland comprehensive solution as the consumer of a cloud provider.

#### Securing Data: Geographic and Cultural Restrictions

In this brief lesson, we want to become aware of the geographical andcultural restrictions when it comes to securing data.A major value proposition of cloud computingand content distribution networking is the ability to storeand share data to edge locations in metropolitan areas all over the world.When storing or sharing data and content,it's important that all local laws and regulations be consideredand strictly obeyed.Attention must be paid to the right of privacy in different countries,as well as the presence or absence of data protection laws.For instance, there may be import/export laws or mandates such asGeneral Data Protection Regulation (GDPR),for the EU and their protectorates in play.It is a best practice to choose a safe country wherethe government is politically stable when distributing data or content.A data center should not be deployed in a location that has the potentialfor instability.Data analysts and architects should considerthe potential lower costs of raw materials, labor, energy, and taxation.Cultural and religious norms and sensitivities must also be consideredfor the storage of data and the dissemination of various forms of content:video, audio, music, documentaries, and the like.

#### Securing Data: Encryption and Hashing

A very popular way to secure data is with cryptographic hashing.By hashing the data before storing it in the database,one can prevent unauthorized parties from readingor changing it without knowing the original data or even thehashing algorithm.It's common for systems like directory servicesto hash the passwords of usersso that they can be verified without exposing the plain text.Examples of trustworthy hashing algorithms for securing sensitive data ina database include SHA-256, SHA-512, bcrypt, scrypt, and PBKDF2.It's important to choosea hashing algorithm that meets all policy requirementsand that is supported by tools and utilities that are readily available.Another defense in depth mechanism is to generate a saltfor each data input that is hashed with a built-in function or library.Then hash the data input and salt with the chosen algorithm.It is essential to use the same hashing algorithm and salt forthe same data input every time it is hashed.Then employ a secure connection to the data storageor database to offer protection of data-in-transit.Encryption at rest is encryption that's used to help protect data that isstored on a disk (including solid-state drives) or backup media.All data that is stored by an organization,whether on-premises or in the cloud,should be encrypted today at the storage layer usingthe Advanced Encryption Standard (AES) algorithm, AES-256.The separation of duties and least privilegeprinciples should also be applied to all subjects who are authorized toadminister encryption policies and perform key management.It's critical to remember that many drives that store data are removableand portable.Data at rest can also resideon removable memory cards, in infrastructure devices, pads, and phones.A common solution for many organizations is to employ hardware securitymodules (HSMs), or use a platform-as-a-service solution, CloudHSM,and even include micro HSM on memory cards.

#### Securing Data: Masking, Obfuscation, and Tokenization

In this lesson, let's look at some other ways to secure data that do notinvolve some cryptographic method, starting with obfuscation,which is a general term,a generic term that applies to any mechanism that makes data lessdecipherable.The goal of obfuscation is to render data unreadableor to hide certain aspects of personally identifiable, personal health,or corporate or government intellectual property information.In other words, things that should be kept secret or private."Obscuring" is a concept where static or dynamic techniques are used onthe original data or a representational data set."Shuffling" is a term that describes utilizing characters from thesame data set to further present the data."Randomization" is when all or some of thedata is replaced with indiscriminate characters.All three of these can be considered techniques for obfuscation.Data masking often involves using characters like an "X" or an "*"to hide some or all of the data.An example is to only display the last four digits of:the social security number,a credit card number,the last four digits of a national ID number,a bank account number,or a username or email address or web mail address.Masking is considered a suboptimal data obfuscation method since it is subjectto inference.In other words, there's still data visible where an attackeror malicious user could infer the masked data from the data that's displayed.It's the weakest form of obfuscation.Tokenization, which is very popular today,involves sending sensitive data through an API call or a request ora batch file to a system or maybe a cloud provider service that replacesthe data with non-sensitive, pseudorandom placeholders called tokens.Unlike encrypted data, the tokenized data is irreversible and unintelligible.In other words, there's no mathematical relationship betweenthe original data and the tokenized data.It's been replaced by a pseudorandom token.The practice involves two distinct databases,one with the actual sensitive data,for example, credit card information or health recordsand one with the tokens mapped to each chunk of data.Here's an example of the United States of data that should be tokenized:Information about minors or underage children that are inthe child welfare system, the adoption system, child protective services,juvenile law enforcement.And there are many entities that want to use information about those minorsfor a wide variety of use cases.Child welfare agencies, corrections departments, government agencies,universities that are doing studies, statistical information, getting grants;the list goes on and on.However, in order to do those things they want to do with that data,it must be obfuscated in a popular way is to tokenize it.Think of it as a form of digital redaction.Now, when the data goes through the integrated data set and gets tokenized,let's say at a cloud provider like Google Cloud, is everything tokenized?No, just the sensitive information.Much of it will be in the clear,so it can be used by third parties or hospitals or government agenciesor universities, and the like.

#### Securing Data: Segmentation and Compartmentalization

In this lesson, we'll continue our discussion of techniquesfor securing data that do not involve necessarily using encryptionor hashing.For example, segmentation.Data segmentation is a process of dividing and organizing dataand information into defined groups or sets to enable:handling,labeling,sorting,viewing,and of course, securing.Segmented data offers a team or a group with segregated, clear,actionable information.Data segmentation involves grouping data into at least two subsets,although more separations may be necessary on alarge network with sensitive dataor maybe a multinational company doing data dispersionor segmentation across multiple cloud providers.When doing segmentation, the data should be grouped on:use cases, orthe types of information,sensitivity levels or classification,a separation of duties policy,or the level of authority neededfor access to this particular type of information.A term that is closely related to segmentation is compartmentalization.It's regarded as a very powerful way to protect personal information.It involves limiting access to information to only those peopleor organizations who need it to perform a certain task.Originating in the military with classified information,the concept can be further understood with another military term:"managing the blast radius."By compartmentalizing data into different compartmentsor data sets, you can minimize the blast radius when there'san advanced persistent threat against that data.Compartmentalization is equally about:spreading the risk so if there's any impact or breach to the data,the damage is limited,for example, compartmentalizing personal health information basedon demographics or geographic location,or compartmentalizing credit card numbers based on ZIP codes;compartmentalization also lowers the effect or the impact on recovery effortsbecause you only have to recover that compartmentalized or segmented data.

#### Course Summary

In this course, you learned about data states, data classification,and data types.You also learned about the data life cycleand secure data considerations such as geographical, hashing, tokenization,and segmentation.Coming up in the next course, we'll explore resilience and recovery.

### Resilience & Recovery

#### Course Overview

Resilience and Recovery.In this course, we'll examine load balancing,clustering, and backup strategies.We'll explore continuity of operations, multi-cloud,and disaster recovery sites.We'll examine capacity planningand testing techniques and look at power considerations.

#### Load Balancing vs. Clustering

Load balancing, load balancing devices and services are populardue to the usage of data and network intensive applications and services.For example, computer-based training,streaming audio and video and conferencing.They can optimize application availability and performance for the end user.Load balancing systems distribute TCP, UDP, HTTP, and TLS trafficacross multiple servers to efficiently allocate resourcesand also offer failover solutions in case one of the servers goes down.Dedicated load balancing appliances and moduleshave become a standard component in physical and virtual networks.All of the major network equipment vendors, Cisco, and Juniper,and others offered load balancingsolutions to basically put traffic in its place.These systems can optimize application availability and performance,as well as distribute traffic across multiple servers,servers that are in active-standby,active-active, or cluster failover solutions.In other words, the endpoint or the client has a virtual IP addressthat is actually represented by a load balancerwhich is load balancing on behalf of back-end services and servers.For example, load balancing acrossfront-end SharePoint servers in your Internet.Load balancing at cloud providers is a very popular solution.Many organizations have shifted their web presenceor their ecommerce solutions up to a cloud providerlike Microsoft Azure or Google Cloud, and they deploy load balancersout in front of those auto-scaling front-end services. In the cloud,you can do network load balancing or application load balancing.The load balancer often represents virtual networks to the publicbased on an IPv4 or IPv6 address or a public domain name.Load balancers perform health checks on the back-end instancesand containers, notifying the cloud service if one needs to be replacedfor any reason, and that replacement can happen within minutes.They produce flow logs that can be used for threat managementservices and active defense.In other words, see who's coming to your load balancer over the public Internet.It runs a TLS listener or a certificate service to decrypt the trafficso that other services can process the trafficlike a web application firewall which can only do its job on decrypted traffic.And as mentioned they can have layer 3 and 4 firewalls orsecurity group firewallsand web application firewalls which are in fact web access control lists.Here we see a cloud load balancer which is acceptingrequests, for example HTTPS or TLS request from the public Internet.The load balancer has different firewall rules, stateful firewallsand a listener, possibly a certificate servicerepresenting traffic in target groups.And those target groups are usually auto-scaling or scaling out groupsof virtual machines. Like Apache front-end web services or SharePointservers or containers running on network operating systems.In the cloud you could consider that target group as a form of auto-scalingclusters of virtual machines or containers.So, let's dig deeper into clustering.A primary target of modern load balancers,whether it's on-premises or in the cloud, is a cluster.Clustering is intended to improve performance and availabilityof a complex physical or virtual system.Clusters are designed to be redundant set of service functionalitiesbased on active-standby or active-active deployments.Cluster deployments are often measured by reliability,which is the ability to successfully provide responsesfor each incoming request.Availability - the uptime of the server usually measuresa percentage of annual uptime in minutes.Performance - measured by the average of the time spentby the service to provide responses or by the throughput.1.25Gbps, 5Gb, 10Gb, 50 or higher.Scalability- the ability to handle agrowing amount of work in a capable mannerwithout degradation in the quality of service.Let's compare clustering to load balancing.Server clustering combines multiple servers and containersto operate as a single physical and/or virtual entity.Load balancing distributes a workloadacross multiple servers to improve performance.Both load balancing and server clustering technologies are used togetherto coordinate multiple servers to handle largerworkloads or auto-scaling workloads.Server clusters typically requireidentical hardware and versioning to function optimally.Load balancers can be used to distribute workloadsto different types of servers and services and can be more easilyintegrated into an existing architecture.However, clustering and load balancingdo have several common attributes. To external devices,both technologies typically appear to be a single systemor a single IP address that manages all of their requests.Both technologies often integrate reverse-proxy techniquesthat allow for that single address, IPv6 or IPv4,to redirect traffic to different IP or MAC addresses.Both clustering and load balancing were developed for managing a data center'sphysical servers originally, but today they've been extendedto applications, virtual servers, cloud servers, and containers.High availability clusters prioritize resilience over other advantagesand can be implemented in either Active-Passive, which meansthe standby device is on and it's running but it's not processing traffic,or Active-Active where two or more are actively processing traffic.Load balancing clusters highlight balancing the jobs among all of the serversin the cluster and incorporate load balancing softwareand special algorithms in the controller node.High-performance clusters use multiple servers to execute a specific taskvery quickly and support data-intensive projectssuch as live-streaming and real-time data processing.Storage clusters, on the other hand, offer massive storage arrays, usuallysolid-state drives, sometimes in support of high-performance clusters,but always in a support role for other servers or clusters,such as a storage area networking or hypervisor clusterdata store or data pool.

#### Backup Strategies

In this lesson, we're going to look at different types of backup strategies,and I want to remind you that CompTIA highly recommendsthat you have the A+ certification before you take Security+.So realize a lot of the things that we're looking athere would be things you would learn on an A+ path.It's not a prerequisite for the certification,it's not a prerequisite for this training, but it's highly recommended.The full backup process backs up everything,regardless of whether the archive bit on the file is set or not.A full backup willclear the archive bit or set it to zero once the backup completes.This method takes the longest to back upand the time depends on how much data there is to backup.A full backup is the quickest to restoreas only the most recent full backup is required.Realize however, it's not as quick as a snapshot,but traditional backups it's the quickest.A full backup should be scheduled, automated, and testedalthough it's common to perform this manually.An incremental backup backs up any new file or any file that's changedsince the last full backup or since the last incremental backup.Subsequent backups only store changes that were madesince the previous full or incremental backup.An incremental backup clears the archive bitonce the backup completes.It also has that in common with the full backup.The process of restoring lost data from an incremental backup is longerbecause you have to include all of the incremental backupsin the restoration process.But the backup process is quicker.It's not recommended to perform incremental backups manually.The differential backup backs up any file that has the archive bit set.It backs up any new fileor any file that has changed since the last full backup.A differential back up DOES NOT clear the archive bit when the backup completes.That's one thing that separates it from the incrementaland full backup, not clearing the archive bit.It is slow to back up, but quick to restore.The last full backup and the most recentdifferential backup are needed for restoration.Why?Because the differential backup does not clear the archive bit,so you must have the most recent differential.But like the incremental, it's not recommended to performdifferential backups manually.Snapshots are immediate point-in-time virtual copies of the source data.It could be simply a data, but it could be an entire Elastic Block Store volume,or a machine image, or a virtual hard disk.It offers easier and faster backups and restores.The snapshot should be replicated to another medium or cloud storageto actually be considered a backup.The time to back up does not increase with the amount of data.Snapshots deliver Improved Recovery Time Objectives (RTOs)and Recovery Point Objectives (RPO) for business or continuity of operations.The restores are fast. And if there's an outage, less data will be lost,often because snapshots are generated every few minutes or every few hours,and a snapshot can easily be encrypted and decrypted as data at restwith AES-128 or 256.Backup frequency is often based on the business impact analysismetric known as the recovery point objective.The RPO is the maximum amount of data lossthat you can tolerate in case of a disaster.The lower the RPO, the more frequently you'll need to back up your data.The type of database management system (DBMS),data volume or block, data change rate, and performance needsall contribute to determining the best backup strategy.Commonly, full backups are conducted automatically or manuallyat least once a week, or more frequentlybased on the criticality or the latency of your data.Differential backups should be donedaily if the RPO is low or the data changes on a regular basis.Incremental backups should be done hourlyif the RPO is low or the data changes very rapidly.Snapshots are common techniques for virtual dataand should also be automated and scheduled based on various recovery pointsand time objectives, all part of your business continuity planning strategy.Remember, backup and restore is a key component of business continuity.Journaling is also referred to as a journal-based backup.Journaling is the simultaneous (real-time) logging of all data file updates.This journal log offers an audit trail and is used to reconstruct the databaseif the original file is damaged or destroyed.Journals can also be used as part of e-Discovery or forensic investigations,or to track the kill chain of an advanced persistent threat actor.Journal-based backup is an alternative method of backupthat uses a change journal maintained by a hardwareor a software storage manager or server administrator.Encrypting the database and other data backups helps secure the data.All DBMS systems offer the option to encrypt the backup datawhen creating the backup.Encryption can also be used for databases that are encrypted usingtransparent data encryption (TDE),so that the database engine forces the creation of a new transactionlog, which will be encrypted by a database encryption key.Most scenarios include various encryption algorithms up to AES-256and either counter blockchaining, CBC or Galois/Counter mode commonly.Administrators can also integrate encryption keyswith extensible key management system (EKM) providersand cloud-based key management services (KMS).When comparing onsite backups to offsite backupshere's some of the considerations.Accessibility- offsite backup is not as reliable to accessphysically as the data is stored in differentgeographical locations, often many miles apart.Cost- for entities with a lot of data, cloud-based backup solutionscan be quite cost-efficient in the long runusing Infrastructure and Platform as a Service.Security - onsite may be as secure as offsiteif a large resource commitment is made for administrative,physical, and technical security controls.Scalability - this is one of the huge advantages of offsite data backupwhere the cloud service provider (CSP) is responsiblefor providing the storage.Support & maintenance - with on-premises solution,the organization has the most control with their own supportteam responsible for data backup.Reliability - offsite data backup is more reliablebecause the data is not storedin the same place as the original data.On the Security+ exam,by no means you have to memorize this diagram.Just realize that many organizations who take advantage of hybrid cloudor edge computing will install some type of gatewayat their data center or a remote branch office server farm or data center.The gateway could be a hardware device or it could be in a hypervisor.For example, at AWS it could be VMware, Microsoft Hyper-V, Linux-based KVM.Then you have various types of caches and various types of file systemprotocols that are supported.NFS, SMB, iSCSI with file gateway caches, Volume Gateway cachesand Tape Gateway caches often going over a direct connection,high speed fiber to a partner in a metropolitan area.It could be AWS Direct Connect or you could use Google Interconnectand as part of your replication life cycle, you can interfacewith many different services at the cloud.You can do data archiving,you can have object storage, you can have a virtual tape library.You can use advanced file systems with a hybrid cloud dataand database lifecycle management system.Without a comprehensive, well-tested recoveryand restoration practice, there is no real backup strategy.In other words, backups without recovery are not backups at all.Many organizations have relied on regular automated backupswhen suffering a ransomware attack, only to find out there were configurationerrors or gaps that were not discoveredthrough ongoing recovery and restoration testing.And remember, the team that performs recoveryand restoration is often different than the backup operators,and that's based on a separation or "Segregation of Duties" initiative.

#### Continuity of Operations and Multicloud

On the Security+ exam, CompTIA refers to business continuityplanning is a more macro termcontinuity of operations.A continuity of operations plan, otherwise known specificallyas a business continuity plan, helps to ensure that the entityremains operational at a pre-determined level when a wide variety of disastersor catastrophes strike.These are plans and documents that must be approved by executive management,if not generated by executive management, then approved.It outlines the risk to business from various probable catastrophic events.It populates the risk register or the risk ledger or some type of database.It lays out the requirements to mitigate from severe incidents.It identifies procedures necessary to recover from a disasterand what is an acceptable amount of time and how do we reduce the impactof the disaster, especially if it happens again.There are several components to the business continuityor continuity of operations plan.The three main components seen in this diagram at the lower left,Business Impact Analysis, then Disaster Recovery Planning,and then the Backup and Restore Policy.Some organizations will include IRT or the Incident Response Teambecause an incident can quickly escalate or elevateinto the category of a disaster.So a lot of things that are done for Incident Response Planningwill be leveraged by the Disaster Recovery Plan.According to NIST, business continuity planning firstinvolves developing a continuity planning policy statement.Remember, if you're a multinational corporation and you have several campusesin the metropolitan area, you may have a business continuity planfor each campus or multiple buildings within a campus.Step two, is conducting the business impact analysis.We'll talk more about that later.Step three, identifying the preventative controls.Four, creating contingency strategies.Five, develop an information system contingency plan.Six, ensure that you have plan testing,training, and exercises, including drills.Seven, generate the after-action report (AAR)and from that report, you'll have lessons learned and ongoing plan maintenance,in other words, do care.One of the first initiatives of continuity of operations isthe Business Impact Analysis.There are several critical metrics or key indicators of your BIA,the recovery time objective (RTO).This is the target amount of time within which a process,for example, your voice-over IP phone, your e-mail,your power must be restored after disruption.The maximum tolerable downtime (MTD), also knownas maximum allowable downtime, is the absolute maximum amount of timethat a resource, service, or function can be unavailable.The recovery point objective is the maximum targeted periodin which an asset or data may be lost from an IT service due to a major event.The mean time to repair (MTTR), also known as mean time to replace,is the average time needed to repair or replace a failed system or module.And then the mean time between failure MTBF is the number of failuresper million hours for a product, for example, 10 years on a solid-state drive.Before we leave this list, a couple of things.The recovery time objective must be less than or equal tothe maximum tolerable or allowable downtime (MTD).Also, even though recovery point does have a time element to it,the recovery point is when something was done.For example, a snapshot, a transaction log, a last known good configuration,the state machine update on the CEO's recovery volume.And the closer the RPO is to the actual event, the better it is.The mean time to repair is one of the most updated metricsover the last five years due to supply chain disruptions.And the mean time between failure is information usually retrievedfrom the original equipment manufacturer or the vendor,or maybe a third-party consumer report.Disaster Recovery Planning (DRP)outlines the technical aspects involved for restoration.The order of restoration, for example, beginning with the most critical asset,then finally the least critical.The location, hopefully off-site of all of your backups,snapshots, and restores, possibly in the cloud.All of the contact information that you needon all key stakeholders and third parties, law enforcement,public relations, law firms, the communication plans.Hopefully, your organization isn't completely reliantupon the single point of failure of voice-over IP.Hopefully, you have other modes such as two-way radios,citizen band radio, shortwave radio, and other communication mechanismslike satellite. The chain of authority, your organizational chart,and who steps up if somebody's incapacitated. Step-by-step instructions,preferably in written format as well.The DRP can't be all digitized because one of the catastrophic eventscould be an EMP, electromagnetic pulse or solar flareor something that takes out your electronics, the digital and physicallocation of all documents, softwares, keys, passwords, credentials, and more.And what are your recovery sites?Hot site, warm site, cold, mobile, cloud, or a shared site.Many large corporations and multinational corporations as well aslarge organizations rely on multicloudfor ultra high availability and business continuity.Multicloud is a cloud computing model where an enterprise leveragesa combination of clouds, for example, two or more public,or private, or a combination of public, and private, and hybrid or edge clouds.For example, using Amazon Web Services and Google Cloud.Multicloud enables the distribution of data, applications and servicesfrom a practical standpoint to accelerate application transformationand the delivery of new apps.However, multicloud also supports disaster recovery by leveraging more thanone provider for enhanced high-availability and durability,often spread across regions all over the world.In the next lesson, we'll dig deeper on the different types of disaster recoverysite solutions as well as geographic dispersion.

#### Disaster Recovery Sites

OK, in this lesson, we're going to start out digging a little deeperinto those different recovery site options.Now let me just say this, I don't want to scare you with this table.By no means do you have to memorize this for the Security+ exam.Now if you want to pause at any timeas you're watching this and read through it, that's fantastic.However, we're just going to hit the high point,the things you need to know for the exam.So the best solution to support your ongoing operations,the best recovery timeeasiest to implement is the internal hot site solutionwhere you can recover often within an hour to 12 hours.The commercial hot site is also a great solution to support ongoingoperations, but it's more expensive because it's a commercial solutionand you're paying some other company,let's say a vendor or some other business continuity consultant.The warm site is going to be really anywhere between theinternal hot site and what we call a cold site, and we'll seethat here on the next slide.So a warm site is kind of hard to define because itreally depends on your organization.It's just something in between the cold site and the hot site.Now you would think that the cold site from this table is thelowest cost solution, but that's not really true.There's a lower cost solution which is a reciprocal agreementcold site, which seldom works,it's difficult to test, and it's a formal agreement, but it canbring in some legal complications.Let's say you're in Houston, Texas, and you find some other businesson the other side of the city, and then you also locateabout halfway in between a cold sitewhere you're going to share the lease on that.That is not necessarily the best solution.It's suboptimal to think that a particular event won't affect both companiesmight be a little bit naive, especially if it's a large hurricane.Something like that.Now on this slide, the emerging solution becoming much more popularfor companies to stage in certain regions of the worldis the cloud solution, where your recovery timecan often be less than an hour, so 0 to 24 hours.So probably the fastest solutionswould be the internal hot site and the cloud solution.With a cloud solution though, you are going to save moneybecause you're leveraging their infrastructure.Let's talk more about geographic dispersion,which is a disaster recovery solution.When you have distance between systems or you disperse geographicallyyou gain a lot of benefits,but realize there are physical limitations and practicalfor example, legal and data sovereignty limitations.For a disaster recovery solution, typically, the greater the distance betweenthe systems, the greater protection you havefrom area-wide or regional disasters.However, this distance will come with application impactsor it will impact your application environment.When distance is added to a data replication solution, for example,you're going to introduce latency.Latency is the added time it takes for data to reach the target system.The further systems are apart,the more latency or time is added to the data transmission.Using cloud service providers and managed security service providers, MSSPsfor high availability, multi-zone, and multi-regional data recoveryand replication is a huge value proposition.Many organizations are migrating from internal and commercial warm or hot siterecovery to cloud-based disaster recovery.And this is made more feasible and cost-efficient by the rapid proliferationof edge and hybrid cloud solutions.

#### Capacity Planning (People, Technology, Infrastructure)

Capacity planning is a technique for analyzing how much production capacityorganizations need to meet consumer or customer demand,either external or internal consumers.It's widely used in data center, manufacturing, and cloud services industries.Capacity planning assists organizations to govern whether they have enough rawmaterials, people, technology, and infrastructureto deliver their value proposition, either product and/or service.There are several different types of capacity planning.There's product capacity planning.Do I have enough product, for example, on the shelvesor available at my distributors or warehouses?Workforce capacity planning.Do I have enough and the right mix of employees?Employees with the proper skill sets?Tool capacity planning. Do I have enough equipmentand am I utilizing the tools effectively?Production capacity planning.What's the maximum amount that I can produce at peak efficiency?The capacity planning process takes four steps.One, identify all existing and new projects and tasks.Two, determine a strategy, three, generate a realistic resource schedule,and four, discover any minute details, tasks, and planning gaps.

#### Testing Techniques

Now this lesson is entitled testing disaster recovery plans.But I want you to remember that the first two or threeof these tests can also be performed for your incident response testing.And often the testing you do on your incident response plansor your incident response teams will be leveragedwhen you do your disaster recovery. As oftena disaster is just an escalated or elevated incident.So the first we have is a read-through or plan review.This is where the business continuity plan owner or the continuityof operations team discusses the business continuity plan.It's a brainstorming session.It's a workshop where you look for missing elementsand inconsistencies within the plan or with the organization.So at the very least even a small to medium-sized businessshould have a read-through plan or a checklist review.This type of checklist test is useful maybe to train new members of your team,including new business function owners.This would be the lowest level of a testreading through and reviewing your plan or evaluating a checklist.Tabletop testing goes to a different level.This is where participants get together in a room to execute documentedplan activities in a stress-free environment.However, it goes beyond a checklist or a review.You're going to use blueprints, topological diagrams,and very likely computer modeling and programs to effectively demonstratewhether team members know their duties in an emergencyand if they need further training.Think of a tabletop as like an old black-and-white movie from World War 2in the Pacific, where a bunch of admirals and captains are standing arounda big large table pushing around little boats on the tabletop.You're adding visualization to help you further document errors,look for missing information,find inconsistencies across the business continuity plan.A walkthrough test is the lowest level of a planned rehearsal.For example, a limited fire drillfor a certain department or a certain building in the campus.You're rehearsing a possible incident to evaluate your capabilityto manage that incident or disaster.It provides an opportunity to improve the organization's future responsesand enhance the relevant competences of those involved.A walkthrough is typically done on a limited basis,for example, scheduling each department or building separately.In other words, it should have no impact on productivityor delivering the value proposition.So it may be done after hours or maybe paying overtime on a Saturday.A simulation test is an actual test where you're going to use to some degree,the cold site, warm site, hot site, mobile site, or cloud DRP solution.You have to determine with the simulation if business continuity managementprocedures and resources will actually work in a realistic situationor a realistic scenario such as a ransomware attackor a fire or an active shooter.This may be the most elaborate test that most organizations can ever conduct.You're going to use your established business continuity resources.You're going to use your recovery site.You're going to do restorations of your backups.You're going to use services, for example, from recovery vendors,for example, commercial hot site, and even transportation to other locations.It'll require sending teams to alternate sites to restart technologyas well as business functions.A parallel test involves bringing the recovery siteto a state of operational readiness.However, you're still maintaining operations to some acceptable levelwith maybe a skeleton crew at the primary site.The staff is relocated, backup tapes are transferred,operational readiness is established in accordancewith the disaster recovery planwhile some operations are still continuing. Now, this may be the mostcomprehensive test that a medium-sized to large organization ever conducts,but realize it can be cost-prohibitive and it will have a negative impacton delivering the value proposition.So often what you'll see organizations do is hirea vendor to bring in crisis actors.So the crisis actors go through the parallel test and they are basicallyreplacements or sitting in for your actual employees,and then you can record it, and then your employees can watch the videoas part of security training at a later date.With a full-Interruption test,operations are completely shut down at the primary siteto fully emulate the disaster.The enterprise will transfer to the recovery site in accordance with your DRP.It's a very thorough test, also expensive, cost-prohibitive,and really unlikely for an organization unless they havelots of extra resources like an Amazon or they havea large amount of taxpayer dollars like a government agencyor a military installation.A full-Interruption test will have the capacityto cause a major disruption of operations, especially if the test fails.This is the least common test of all of the ones I've mentioned.

#### Power Considerations

Now a common form of disastrous or catastrophic event,either man made or natural, is a power outage.A blackout is a complete loss of power to an area.This is a severe type of power outage,especially the longer it lasts.It typically effects large numbers of people over potentially large areas.Brownouts typically occur if there is a drop in the electrical voltageor a drop in the overall electrical power supply.While brownouts do not cause a complete loss of power,they can cause poor performance from some equipment and some devices.A permanent fault is a sudden loss of power, typically causedby a power line fault.These are simple and easy to deal with:once the fault is removed or repaired, power is automatically restored.For example, it could even be something like a squirrelor some other animal causing a fault on a transformer.Rolling blackouts are different from the other three.These are planned power outages,often performed in certain regions in time of extreme weather.They're also implemented in areas with unstable gridsor with infrastructure that cannot handle the population it serves.Rolling blackouts could also be caused if there's not enough fuel to runpower at full capacity, whether for the short or long term.An uninterruptible power supply, or UPS, is an electrical componentthat delivers emergency powers, often for 20 to 30 minutes, to a loadwhen the main power source (typically utility power) fails.It conditions incoming power to ensure that it's clean and uninterrupted.It protects devices from power problems and enables seamless systemshutdown or a temporary measure before switching over to a generator.A UPS system is particularly beneficial for networking equipmentand other infrastructure devicesthat can lose data when power is suddenly lost.UPS is a critical investment to thwart damage,data loss, and downtime caused by various power issues.A backup generator is a failover power solution that offers power to businessoperations, organizations, and even small office, home office environments.They're typically stationary, although not always in a small officeor a small business, and require a concrete pad used as a foundation.They're usually situated outside a facility or outside of the site.Standby generators are a robust solution that can offer power for daysduring extended power outages, depending upon the fuel typeand the configuration of the generator.Many sites employ prime or continuous generatorsfor disaster recovery site solutions. According to the Uptime Institute, the UI,which thousands of companies use worldwide,all tiers should have at least 12 hours of fuel, for example diesel,for their backup generators at a minimum.Electricity companies can operate in the same area becausethey can compete to provide electricity to consumers.While the power may come from the same grid or transmission lines,different companies can generate and supply electricity to the grid.These companies then compete based on factors such as pricing,customer service, and offerings of renewable energy,for example, wind farms and solar farms.Think of it as how different phone carriers can operate usingthe same cell towers and infrastructures.A critical aspect of high availability and disaster recoveryis to leverage, whenever possible, multiple power sources.

#### Course Summary

In this course, you learned about load balancing,clustering, and backup strategies. Continuity of operations,multicloud, disaster recovery sites, capacity planning, and testing techniques,and power considerations.Coming up in the next coursewe'll explore computing resources security techniques.

## Security Operations

In this track, you’ll master the art of securing computing resources by  applying common security techniques in various scenarios. You’ll delve  into the security implications of managing hardware, software, and data  assets effectively, and explore the essential activities involved in  vulnerability management. You’ll also gain insights into security  alerting and monitoring tools, and learn how to enhance enterprise  security capabilities. Additionally, you’ll implement and maintain robust identity and access  management systems, understand the importance of automation and  orchestration in secure operations, and develop skills in incident  response. Finally, you’ll use data sources to support investigations,  ensuring a comprehensive approach to cybersecurity.

### Computing Resources Security Techniques

#### Course Overview

Computing Resources Security Techniques.In this course, we'll explore secure baselines and device hardening,examine wireless issues and wireless security,examine mobile issues and mobile security,take a look at application security and understand asset management.

#### Secure Baselines

Secure baselines: a security baseline is defined asthe minimum amount of security controls needed for safeguardingan IT system based on its identified needs for confidentiality, integrity,and/or availability protection. For vendors such as Microsoft or Cisco,the baselines would be a group of recommended configuration settings,for example, in Microsoft using Group Policy that describe their securityimplications. The settings are basedon feedback from security engineering teams, architects,product and program groups, partners, customers, and other third parties.The Center for Internet Security, CIS has benchmarks.These are strict configuration recommendationsfor more than 25 vendor product families.The CIS Benchmarks representa consensus-based initiative by cybersecurity experts globally to helporganizations protect their systems against threats more effectivelyand with more confidence.The CIS Benchmarks are community-developedsecure configuration recommendationsfor hardening organizations' technologies againsta wide variety of cyber attacks, both external and internal.These benchmarks are mapped to the CIS Critical Security Controls,at the time of this recording,the top 18 CIS Controls. Benchmarks elevate the security defensesfor cloud provider platforms and cloud services, containers, databases,desktop software, or UX, user experience, server software, mobile devices,IoT, network devices, and a wide array of operating systems.To implement a baseline initiative, you'll have several processes.First, you have the project mandate or the mission, budgeting,project managers and timelines, then defining the project.That's baseline number 1 where you have a high-level scope,high-level milestones, and high-level cost estimates.Next are the business requirements document which could bethe strategic approach, then the system requirement specification which isthe technical or tactical,and then baseline 2 would be the revised project plan.Here you have a detailed scope,detailed milestones and detailed cost estimates with resources forecast, allbased on the business and system requirements, documents and specifications.Realize this is an iterative process.

#### Hardening Targets

Hardening is a generic term also referred to as server hardening,security hardening, or more specifically, operating system or OS hardening.System hardening is a combination of methods, tools,and best practices that are used to reduce vulnerabilityor lower risk in servers and other types of computing systems,physical, virtual, on-premises and in cloud environments.The goal of hardening is to lessen network and IT security risks.It's accomplished by shutting down portsand communication channels used by unnecessary services and applications,for starters. It also includes removing defaultand automatic configuration settings and activating built-in security features.Some common targets of hardening would be: operating systems, Windows, Linux,Unix, macOS, Sun, Solaris, and others; wired and wireless networks;database systems, relational databases,no SQL or document databases and graph databases, among others;applications and containers; and industrial control systems (ICS);Supervisory Control and Data Acquisition systems (SCADA); embedded systems;real-time operating systems; and a wide variety of IoT devices.Some challenges to hardening embedded or specialty systems, including IoTwould be dependability. Many critical aspects such as utility grids,transportation infrastructure,and communication systems are controlled by difficult to patchand often highly proprietary embedded systems or uneven security updates.Many of the embedded and specialty systems are not regularly upgradedfor security updates, or they may simply not be available.For instance, the vendor has gone out of business,or they've been acquired by another company who has no interest whatsoever inupgrading the system or the component. Attack replication,since embedded devices are mass produced,the same version of components has the same design and buildas other devices in the lot. That's a broad attack surface.Industrial protocols, embedded systems,for example, components written in Raspberry Pi and Arduino often followa set of custom procedures that are not protectedor recognized by commercial enterprise security tools.They may have their own special industrial protocolsand communications channels. Device life cycles,this is a challenge because specialty IoT devices typically havea much longer lifespan than PCs and remote deployment,many embedded devices are deployed in the field, for example,oil and gas facilities, wind farms, solar farms, and the like,and they're outside of the common enterprise security perimeter.They may be directly connected to the Internet or satellite withoutthe security layers offered in a controlled industrial environment.It's critical with these specialty systems,controllers and SCADA utility devices to automate as much as possiblethe system hardening process.First, we base it on hardening guidelinesand configuration guides if they're available.If we must, we can customize those.We can add some new rules,or we can remove rules from the configuration guides that are not applicable.We may have to create identity exceptionsfor certain high-privileged users that will all populate our security databaseor configuration management database, that will drive automated implementations,automated verifications, and continuous monitoring as we configure,scan and report on various hosts.

#### Wireless Device Installation Issues

Wireless device installation issues:compared to Ethernet and fiber wired networks,there are a wide array of wireless protocols and technologies.Wireless networks are often called the low-hanging fruit of network security,and are a common starting point for attack vectors and even doinga penetration test.Wireless signals are more affected by physical obstacles,electromagnetic noise, and other wireless devices,resulting in lower qualityor loss of connection that adds complications to wireless implementations.This makes it all the more important to conduct wireless site surveys,wireless analysis, and produce heat maps for proper implementation,for example, antenna deployment and directions, power outputs,and designing areas and their RF overlap for devices that are roaming,for example in a hospital.A wireless site survey is critical as part of planning and strategy fora wireless network. The first phase ofa wireless site survey is to identify all ofthe wireless deployment requirements.Questions to ask in the initiation phase of a site survey are:what is the desired speedand bandwidth that can be affected by how many devices are in the area?For instance, how many client devices will be accessingthe network simultaneously and what different types of devices are they?For example, how much transmit power will they have?Which generation of the 802.11 Wi-Fi standard will the site be using?Typically something newer like .11n, .11ac, or .11ax,even though you might still see some older environments using .11g.Next, the surveyor should get a diagram of the area the network will cover,preferably with building blueprints.Perform a walk through and document the infrastructure evaluation.The next step is to look outfor places where wireless access points can be mounted,such as ceilings and pillars. After this, determine the areas to be covered.Don't forget utility rooms that may house wireless equipment.Indicate areas on the floor plan.Then determine the tentative access point locations.Make sure to check the coverage range of your access points.They can be omnidirectional, semi-directional, or highly directional.Build in some overlap between neighboring access points to guarantee seamlessroaming between areas or service sets.Consider dynamic load balancing and the resiliency of your network.With wireless analysis, the initial decision should be to acquirean industry-leading wireless analysis kit and spectrum analysis toolkit.A Wi-Fi analyzer is a useful software application that can report many thingsabout the wireless network and other networks around you.Remember, if you're in a multi-floor building,you have to consider other businesses and organizations under you, above you,and next to you. Also, what is your proximity to military installationsor airports that will affect your wireless analysis andthe decisions you make in optimizing your Wi-Fi best performance?You want to ensure that the decisions made in the site survey are as optimizedas possible.A Wi-Fi heatmap tool generatesa color-coded graphical representation of different wireless metrics suchas signal strength, SNR: signal-to-noise ratio levels, power outputs,and indicating interference in different areas.By leveraging the power of data visualization,Wi-Fi heatmaps empower network engineers to makethe right decisions when optimizing a network, enhancing performance,or addressing potential issues suchas interference with neighboring organizations or businesses.In this diagram, courtesy of dnsstuff.com,we see an example of a wirelessheatmap displayed in wireless analysis software.

#### Mobile Device Solutions

Mobile deployment models: let's begin with BYOD, bring your own device.With BYOD, employees are permitted to use their own personal mobile devicesiPhones, Android phones, pads and others to access enterprise dataand systems. Generally, there are four basic options under BYOD initiatives:one, unlimited access for the personal device;two, access only to non-sensitive systems and data in the enterprise onthe BYOD device; or access with ITcontrol over personal devices, apps and stored data,in other words, under the control of mobile device management,or the strictest being access, while preventing any local storage of data.Another option is COPE, corporate-owned, personally-enabled.Here, the company gives the employeesor contractors mobile devices that are provisioned from vendorsand cellular providers without end user input.The users can handle it as if it was their own.This model prevents the need for two smartphones.COPE programs should use containerization toolsand extensive mobile device management and mobile application management.CYOD, with choose your own device,much like bring your own device,employees can work from anywhere using a mobile device.CYOD devices must be approved by the organization that separates itor sets it apart from BYOD. Remember that for the exam.CYOD users will often choose from a list of approved devices,which are usually smartphones, but they can also be specialty pads.These networks offer more stability,security and simplified IT for many businesses.CYOD also demands device management.Organizations must securely configure and implement each layer ofthe mobile technology stack, including hardware, firmware, operating systems,management agents, provider agreements, and the apps used for business.The solutions should reduce risk while enabling employees to accessapplications and necessary data from nearly any location over any network,using a wide variety of mobile devices in some cases.Enterprise mobility managementfor large enterprise organizations is typically made up of two different groupsor teamsmobile device management (MDM) and mobile application management (MAM).There are three basic core competencies that all organizations need froman enterprise mobility management solution.One is visibility: understanding at all times what is running onthe mobile devices. This is key to discovering potential risksand adhering to compliance policies, for example,acceptable use policies or other governance and regulations.Secure access: providing the ability for mobile users to securely authenticateand authorize access to apps and data.And data protection: offering dynamic antimalwareand data loss prevention capabilities to help limit the risks of attacksand data breaches.Another mobile related term on the exam is sandboxing.Sandboxing is also referred to as partitioning or compartmentalization.Sandboxing techniques involve orchestrating the packaging, isolation,and encapsulation of apps and work data ina separate segmented user space within the device, often called an enclave.Storage sandboxing, or segmentation comprises partitioning various types ofdata on devices to protect intellectual property, PII, PHI,and to support data loss prevention initiatives.For example, the iPhone has a separate secure enclave for security and privacy.Some common mobile device management solutions would be:onboarding, offboarding,otherwise known as provisioning and de-provisioning,as well as installing certificates such as Cisco EAP-FAST PAC files,or X.509v3 certificates;implementing touch ID authentication and screen locking;configuring PINs and push notifications for user devices;deploying and managing full device encryption; and finding lost devicesand remote wiping, including geofencing and geotagging.Some modern attributes of enterprise mobility management would include:bring your own device and MAM application management security capabilities,flexible bundles for different use cases and scenarios,for example, a different policy for the CEO as opposed to the sales teamas opposed to the marketing department;unified endpoint management and enterprise integration;end-to-end Zero Trust security and identity management;productivity applications without having to write code,often choosing from a portfolio of different packages;and introducing user behavioral analytics (UBA).Three other mobile solutions to know forthe exam would be:cellular, multiple access technology,where multiple voice or data connections are placed intoa single radio channel using 5G and upcoming 6g;Wi-Fi, various IEEE standards that employ different aspects ofthe radio frequency spectrum and modulation schemes to transmit data wirelessly;and Bluetooth, an IEEE radio-frequency PAN standard inthe 2.4 to 2.485 gigahertz ISM, an agreement protocol.

#### Wireless Security Settings

Wi-Fi Protected Access 2 (WPA2) was the replacement for WPA around 2004.WPA was the replacement for deprecated WEP, (WEP) Wired Equivalent Privacy.WPA2 has been the de facto standard for almost 20 years,still a common solution. All devices require testing and certification fromthe Wi-Fi Alliance by 2006. It uses Counter Mode Cipher Block ChainingMessage Authentication Code Protocol (CCMP) for security.WPA2 supports both pre-shared keyand enterprise authentication using 802.1x. In WPA2, ManagementFrame Protection (MFP) also known as protected management frames,was optional but highly recommended and universally deployed to protectthe management frames of wireless.Remember, there's three types of frames: data, control and management,management being the most vulnerable.In 2018, the Wi-Fi Alliance announced a new security protocol,with WPA3 support becoming mandatory for all routers or routers carryingthe Wi-Fi CERTIFIED label since July 2020.All WPA3 networks use the latest security methods,disallow outdated legacy protocols,and demand the use of (PMF) Protected Management Frames.PMF enhances privacy protections already in place for data frames,with mechanisms to improve the resiliency of mission-critical wireless networks.Let's look at some of the newer cryptographic mechanisms in WPA3:authenticated encryption with GCMP-256, in other words, using Galois fields;key derivation and confirmation with 384-bit HMAC with Secure Hash Algorithm(SHA384); key establishmentand authentication with Elliptic Curve Diffie-Hellman exchangeand Elliptic Curve DSA using 384 bits;robust management frame protectionwith 256-bit Broadcast/Multicast Integrity Protocol Galois MessageAuthentication Code (BIP-GMAC-256).Since WPA organizations have had the option to use pre-shared key mode oran 802.1X enterprise mode,where the supplicant sends initial EAPOL frames to an access point.On the back end, you have a wireless LAN switch,or it could be a module and a router using an authentication server,and that would be radius or diameter on the front end.But you could also use a wide variety of solutions as your identity provideron the back end, for example Active Directory.After the initial EAPOL exchange, then they'll agree upon which EAP type to use.For example, EAP-TLS, protected EAP, EAP-TTLS,tunneled TLS or in Cisco environments, EAP-FAST.The client and server will establish a shared secret key,agree upon authentication ports and accounting ports,perform network authentication,and then finally encrypt the data sent over the RF spectrum.

#### Application Security

Our next topic in computer resources security techniques is applicationsecurity and specifically validation testing.Validation testing is the process of ensuring that testedand developed software applications or mobile apps fulfill the needs ofthe customer, functional and non-functional, functional being what it does,non-functional being how it does it.This includes the business requirement logic or use cases.They must be tested in full detail.For example, a mobile app that generates financial reports.All the critical functionalities ofan application must be tested in validation testing.It's critical to know how to verify the business logic that is provided.A common technique is input validation,which ensures only properly formed data is entering the workflow inan information system.Here's four examples of functionality testing.DQ is design qualification.This defines the functional and operational specification of the instrumentor the component, and details the conscious decision in the selection ofthe supplier. Installation qualification (IQ) establishes that the app,application or instrument is received as designedand specified, that it's properly installed in the right environment,and that this environment is suitable for the operation of the instrument.For example, a new application or container running in a virtual environment.And then operational qualification or OQ,this is the process of demonstrating that the app, for example,will function according to the operational specification inthe chosen environment and performance qualification (PQ),the process of demonstrating that the microservice, for example,performs according to a specification appropriate for its routine use.Another important application security mechanism on the exam is secure cookies.Hypertext Transfer Protocol (HTTP) cookiesare small packets of data stored in a browser client.This data may contain sensitive information like passwordsor user information, PII, and is therefore vulnerable for attacks.To limit vulnerability, developers can enhance cookie security by addingspecific attributes to the set cookies,making it difficult for attackers to manipulate.Some methods for securing cookies would be using Really SimpleSecure Sockets Layer, which uses HttpOnly, secureand use_only_cookies parameters to make cookies more secure.The HttpOnly flag will tellthe browser that this cookie can only be accessed by the server.The secure parameter will make sure that cookies are only sent overa secure SSL/TLS connection,and the use_only_cookies parameter tells your website to use only cookies tostore session data.Another critical application security mechanism is code signing.In the diagram we see our original code. The code goes through a cryptographichash function which can be verified by an X.509v3 certificate. The code ishashed using a hashing algorithm, for example SHA-2 or SHA-384.On the receiving end, the recipient compares hashes to see ifthe signature is valid. It is imperative today that in every situation you usedigitally signed code.Let's compare two application security testing methodologies, static versusdynamic. Static application security testing (SAST) is commonly defined as awhite-box or clear-box test where an analysis of the application source code,byte code, application programing interface, including binaries,is carried out by the application test without executing the code.In other words, it's not in a running or runtime state.This is used to find coding errorsand omissions that are symptomatic of security vulnerabilities,specifically early on in the development life cycle.SAST is often used as a test method when the tool is under development.It can be used to find SQL injection attacks, cross-site scripting errors,buffer overflows, unhandled error conditions, poorly written APIs,and possible backdoors into the application.Dynamic application security testing is considered a black-box or opaque test,where the tool must find distinct execution paths inthe application being analyzed.Unlike SAST, which analyzes code that is not running,dynamic AST is used against applications in their running or runtime state,for example, a website or a web server.It's primarily considered effective when testing exposed HTTPand HTML interfaces of web applications.Static and dynamic application tests today work in concert to improvethe reliability of applications being built and bought by organizations.

#### Asset Management

Another category of security techniques with our computing resources is assetmanagement. Let's start out looking at the first phases of asset management,acquisition and procurement.The acquisition/procurement process involves possible assignment of ownership,custodians or stewards to the assets, more commonly, data assets.The labeling or tagging schema will be applied, for example,a physical UPC code or QR code, or a logical tag such as a key value pair.Classification levels and sensitivity levels are attached.For example, if using a mandatory access control model,either public or private, or government or military,the accounting methodology will be implemented in the acquisitionand procurement phase, which may include using RADIUS,DIAMETER or LDAPS, accounting with automated and integrated inventoryengines and/or integration with directory services suchas Active Directory, Configuration Management Databases (CMDBs),human resources, legal departments, and more.Next, we want to monitor and track our assets, both physical and logical.This initiative will involve the ongoing enumeration and tracking of all assets.The monitoring process may involve the implementation of security informationand event management (SIEM) and/or security orchestration,automation, and response systems (SOAR) often with cloud-based analysisfor resource planning and optimization.Continual improvement is a key aspect of the monitoringand tracking area of asset management.This phase also involves the ongoing search for shadow assets and/orghost IT, unlicensed software running in type two hypervisors,or pirated content violating copyrights,or content downloaded from peer-to-peer file sharing sites.Some organizations have dedicated digital asset managers to control informationand digital rights or information rights management initiatives.The next phases are disposal and decommissioning.We can use sanitation methods such as DoD-compliant wiping softwareor cryptographic shredding of data in the cloud. To be accredited or certified,we have to properly dispose and decommission physical and logical assets.We may have retention periods which are based on governance, policy,best practices or government regulations.For example, keeping customer records, tax information,medical records or other property.And the final phase of asset lifecycles, the destroy or destruction phase.In this diagram we see a basic asset management life cycle. Realize this canapply to all different types of assets,but one of the most common assets that have a life cycle would be data asan asset. So first we identify the asset. We call that assessment.Next we categorize the asset.For example prioritizing in a qualitative way on a scale of 1 to 5or 1 to 10 identifying mission critical assets.This could also involve classification or sensitivity levels.Next we assign ownership or stewardship or custodianship of the asset,physical or logical, if it applies to your organization.Next we monitor the asset life cycle.Then we adopt access control mechanisms that are appropriate to ourarchitecture, for example, attribute-based or risk-based access controlsin a Zero Trust environment. You must continuously monitor our asset management,especially assets that are not on-premises,physical media that's stored in offsite locations,data stored in the cloud block and object storage.And then finally for our assets,implement incident response and recovery strategies.Critical to this is your backup and restore policy.

#### Course Summary

In this course, we learned about security baselines and hardening,wireless security and mobile security,application security and asset management.Coming up in the next course, we'll explore vulnerability management.

### Vulnerability Management

#### Course Overview

Vulnerability management.In this course, we'll explore threat feeds and other resources.Look at application vulnerability assessment and vulnerability scanning.Understand penetration testing.Examine vulnerability response and remediation.And survey remediation validation and reporting.

#### Exploring Threat Feeds and Resources

Okay, we're going to start out this course doing a little demonstration ora web safari and we're going to look at some sourcesfor vulnerability assessment information.This exam is supposed to be vendor-neutral or vendor-agnostic.Even though I can't guarantee that you won't have to know abouta couple of these things specifically.Now, the first thing I want to define while I'm here is open sourceintelligence. So I'm at sentinelone.com which is a pretty well-known site,and they're just going to describe what it is.This is basically information that's available for free, public sources.Anything you can find, not just digitized.It could be books, white papers, reports, articles,statements in a press release, different media types, anything you can gather.So, it's different than other forms because it focuses on publicly availableand legally obtainable information.So think of like, Freedom of Information Act or public recordsor government reports.This is often used by penetration testing companies who,let's say in the early phaseswhen you do that pre-engagement meeting,if you decide to do a what we would call a black-box or a you know,opaque or know-nothing type of test or even a gray-boxthen, they're going to probably use tools like this.And you can see it says right down here, you know,it can be used by an attacker - or friendly penetration tester, right?Now often, what they'll use is a tool,and one of the tools you can use to do open source intelligence is Maltegoand Maltego can actually come in an exploit kit like Kali or Parrot.Usually it's not a full-blown version,but if you can see down here, on the bottom-right, it's kind of hard to see.This tool allows you to express graphically what you're finding out thereon the Internet as you go and you spider indexing on websites.So, it's a way to visualize a wide variety of this informationand kind of compile it into one area.And of course, you can generate all types of reports from Maltegoand it does more than just open source intelligence.I mean, if we continue to go down here,we'll see that not only do we use it as that,but it's an intelligence and analysis tool, data mining for example,you can automate.So, threat intelligence collection and analysisand to reduce your incident response time from hours to minutesand so it's used by SIEM systemsfor example, integrating siloed data sources, SIEMs, logs, ticketing systems,threat intelligence, OSINT, information from vulnerability scanners.So, that's something to be aware of on the exam.Open source intelligence and then tools that actually leverage this.Another place to look at is the National Vulnerability Databaseand this is NIST: National Institute of Standards and Technologyand this is exactly what it says it is.It is a vulnerabilities database, okay?And it's pretty up-to-date.If there's a zero-day that shows up,it generally shows up in the database pretty quickly, once it gets evaluated byexperts and companies like Trend and others.Also, along these lines, with the NVD, if we scroll down we'll see CVE.So, this is the actual unique identifier.So, we see like the last 20 ones that came up.So on the exam,CVE stands for common vulnerability enumeration.So, basically you have the CVE followed by the year.So these are, this is 2023 for this vulnerability identifier, and thena unique number.Okay, so this was just saying Node.js vulnerabilityand when it was published. So, Published: February 8, 2024.So that's the CVE number and then, there's a score that's the CVSS.So, you know 9.8 is CRITICAL, 7.1 is HIGH, 5.5 is MEDIUM,and CVSS stands for the common vulnerability scoring system.So on the exam, be aware of the CVSS severity score, the unique CVE identifier,and the National Vulnerability Database.Some other organizations to be aware of uh, sans.org S A N Sand they have a wide variety of resources, they have webcasts,they have a newsletter, they have a security awareness training.This is also who you would go through if you wanted to get trained.Once you get your Security+ it's not uncommon to go this path.Now it's not cheap, okay? You can seethe different focus areas, you can go to these week-long boot camps.They also have, you know, courses that are kind of streamed.But since most of the people that come to GIAC certifications are sent by theircompany or they're in the military or government agencies,they're extremely expensive. But sans.org has a lot of research areas.They have the Internet Storm Center,they've got different research, white papers.This is a really good place to come to get templates for security policies.A lot of people around the world who want to create a secure,a written security policy will use these templatesas their starting point at sans.orgAnother group called MITRE,they're really well known for their ATT&CK Matrix for the enterprise.So you can see the MITRE ATT&CK Matrix has all these different categoriesand then within each category, they have these different types of areas.So, you have different Reconnaissance, different Initial Accessfor example Drive-by Compromise, Phishing attacks, Execution, Persistence.They divide these up into kind of attack chain categories. Privilege Escalation,evading the defense, Credential Access, Discovery, Lateral Movement.So, within these different categories you've got all these different techniques.Techniques that are used against us,and these are actually things that we don't want to be vulnerable to.Okay, for the final topic in this lesson,I'm going to go back to the slides, and we're talking about the dark web.Now, contrary to popular opinion,the dark web is not some private island somewhere in the Pacific likea James Bond movie. It's an overlay network or a darknet.The dark web is not indexed by search engines.It's basically an "anything goes" peer-to-peer network that extends globallyand you access it through the Tor, Freenet, Riffle browsers and others.Tor, by the way, stands for The Onion Router.What's offered on the dark web?Well, pretty much anything.Botnets, which are the most popular form of distributed denial-of-serviceattack. Marketplaces like the Silk Road,where you could find all types of contraband.Malware as a Service (MaaS) sites.For example, to conduct ransomware attacks for you.Illegal pornography, fraud and hoaxing services.Phishing, ransomware, and scam campaigns, all part of Malware-as-a-Service.Zero-day malware, malicious code that's not yet shown up on the public Internetand been discovered by vendors like Cisco, Amazon Web Services, Fortinet,and other antivirus solution vendors.Niche social media and unfortunately, a meeting place for terrorists.

#### Application Vulnerability Assessment

Application vulnerability assessment.An application vulnerability assessment isa testing methodology used to recognize and assign severity levels toas many security defects or flaws as possible in a chosen timeframe.This process typically involves manualand automated techniques with varying degrees of precision,with an emphasis on comprehensive coverage.Starting with the most critical applications.It's often part of a larger software assurance initiative,such as the OWASP Software Assurance Maturity Model (SAMM).Static application security testing (SAST) tools are also knownas code analyzers, that conduct a direct white-box or clear-box analysis ofthe application source code.The analysis runs on a static view of code,and that the code is not running at the time of the assessment.SAST security tools are mainstream and are widely adopted throughoutthe software industry.They have broad programming language support,and use concepts that are relatively easy to comprehend,even for the beginner programmer.Keep in mind however, that SAST code analyzers, have no visibility ofthe execution flow.They can be slow, somewhat inaccurate and outdated,and often need additional customization and or tuning.Dynamic tools, on the other hand, are most often web scanners like OWASP ZAPand Burp Suite, known as vulnerability scanners.They perform know-nothing,and that they do not have access to the code or the implementation specifics.A DAST tool will only inspect the system's responses toa series of tests, designed to highlight vulnerabilities.They function independently of the underlying application platformand offer solid support for manual penetration testing.Another term for the exam is package monitoring.Processes and tools that troubleshoot application performance issues in Dev, QA(quality assurance) and production environments with code-level insights.Distributed transaction tracing, for example on e-commerce sites,banking sites and online brokerage services, application service maps and more.Package monitors usually support Java, .NET,.NET core, Node.js, Python, PHP, and Ruby applications.Container monitoring empowers DevOps teams to stayon top of outages and pinpoint server issues with root cause analysiscapabilities. They proactively monitor and optimize the performance of Docker,Kubernetes and RedHat OpenShift containers and applications.

#### Vulnerability Scanning

Vulnerability scanning.Vulnerability scanning isthe process of identifying known and unknown weaknesses in systems,applications, services, and policies using various tools.Vulnerability scanning is an easier and often more focused process lookingfor unpatched systems, open ports, misconfigurations, poorly-written code,and more.It is typically automated and done on a routine basis.For example, weekly or quarterly, taking at most a few hours.Vulnerability scanners include Nessus, OpenVAS, Core Impact, Nexpose,GFI LanGuard, QualysGuard, OWASP ZAP, and Burp Suite, to name a few.Network scanners can be used to scan IP addresses, ports,and device locations in wired and wireless networks,then presented, in a customized graphical XML rendered view or dashboard.Most scanners provide network monitoring and management capabilities,with the goal of detecting, diagnosing,and hopefully resolving network issues and outages.Active malware worms are also considered a form of network scanner,as they scan a particular TCP or UDP port ona number of contiguous IP addresses or random IP addresses.The most common vulnerabilityscanners will test web applications,websites and services in order to find cross-site scripting and request forgeryweaknesses.SQL and other command injection,for example, LDAP, broken authentication and session management.Insecure direct object references.Insecure server configuration (XML, PHP, etc.) and other.Or the exposing of sensitive data,for example, passwords or personally identifiable information.A compliance scan is different than performing a scheduled vulnerability scan,although there can be some overlap.In other words, compliance scanning will involve using vulnerability scanningtools. A compliance audit, decides if a system is configured in agreement witha recognized governance policy or regulations or mandate.It could also be a baseline, for example from Department of Defense, NIST,ISO/IEC or COBIT 5.Whereas the vulnerability scan is determining if the system,application or service is exposed to known vulnerabilities.Sometimes compliance involves auditing more sensitive data and systems.Typically, the compliance requirements are minimal baselines, that can be takendifferently depending upon the goals of the organization.Compliance requirements, must be in line with the businessor organizational goals and mission,making sure that risks are correctly recognized and alleviated basedon risk treatment and handling as well as business impact analysis.Whenever performing vulnerability scanning or compliance scanningor other types of vulnerability testing,you must take the same approach as you would with an intrusion detectionor intrusion prevention system.In other words, how accurate is the test?For example, what you want are true positives and true negatives.A false positive, is an error state when a particular action was taken,for example, the way the system reacts,or a false negative, an error state when an action was not taken.You can also apply this to endpoint detection and response, anti-spam filters,anti-malware, and other systems that can take actions, such as droppingor blocking, or sending alarms and alerts, or quarantining a file or a service,or killing a process.

#### Penetration Testing

Another type of test that definitely involves vulnerability scanning,but is something different is a penetration test.Penetration testing is security testing,in which the assessors simulate real-world attacks,to identify methods for evading the security features, or controls ofan application, system, or network.Pen testing often involves launching real attacks on systemsand data that use tools and techniques that would be commonly used by attackers,for example, exploit kits like Kali and Parrot or Metasploit.Penetration testing can be useful for determining how wellthe system tolerates real world-style attack patterns.The likely level of sophistication,for example, is it a script kiddie, or a proficient Python programmer oran expert programmer? What do they need to successfully compromise a system,application, or service?They can determine additional countermeasures thatcould mitigate threats against the systemand quantify the defender's ability or the blue team, to detect attacksand respond appropriately, for example, your incident response team.Here are some terms that relate to penetration testingyou should know for the exam. A pre-engagement meeting.This determines a variety of elements, the scoping and restrictions.For example, are you going to begin with the wireless network anda restriction? Is the CEO's laptop off-limits or certain databases?Pricing and cost structure?Are you charged by the hour or by the penetration test?Or some other cost structure? Is it know-all or know-nothing?Previously referred to as white-box and black-box.Know-all is also called clear, viewed, or visible.Know-nothing is considered opaque, hidden, or invisible.Remember, the less the pen testing team knows, the more opaque, or hidden it is,the more expensive it will be,especially in the early phases of reconnaissance and information gathering.Is it credentialed vs. non-credentialed?If it's a know-all test, you may provide a user account, or evena privileged user account.So they have a credential to work with.Often to discover how effective a compromised,privileged insider could be against different applications,servers and hypervisors. Is there a bug bounty?Are you going to give the pen testers a reward?Typically in thousands of dollars, for finding a major flaw,weakness or vulnerability.Is it intrusive or non-intrusive?This could also be referred to as active or passive.If it's active or intrusive, it will have an effect on end users.It will have an effect on client-server productivity and other activities inthe production networks.If it's non-intrusive, it'll be more passive.Here's the penetration testing life cycle.The first phase, is information gathering or reconnaissanceand as mentioned, the less you know,the more lengthy and costly this first phase will be for the pen testers.Next, we have threat modeling, using data flow diagrams against applications,determining the vectors that are going to be used, exploits, and malware.Phase three will involve vulnerability analysis and scanning tools.As mentioned, vulnerability scans and other scans are part of a pen test.Then, the actual exploitation when they determine the vector and the payload.Next is post exploitation,where they attempt to move laterally or escalate or elevate privileges,or cover their tracks by deleting logging, or deleting log files.Placing malware resident in RAM, or in encrypted or compressed files low inthe file system. Then, when they're done, they'll be reporting.The after action report, should show all the different weaknesses,but also make recommendations for various categoriesand types of controls that you should implement to reduce riskand vulnerability to a wide variety of attacks.

#### Vulnerability Response and Remediation

In this short lesson, we'll remind ourselves of various ways to respondand remediate vulnerabilities, starting with implementing additional controls.For example, administrative controls such as, acceptable use policies.Technical controls such as, endpoint detection and response,and physical controls such as biometric locks.Different types of Controls, Detective, Preventative, Deterrent,Compensating, and Corrective.We can implement tested patch management to respondand remediate to vulnerabilities and risks.The initiative of applying updates to software, drivers,and firmware to protect against these vulnerabilities.Effective patch management also assists in choosing the optimal performanceand productivity of applications, services, and systems.Purchasing insurance. This is a method for risk sharing treatment.Additional segmentation and compartmentalization, both physical and logical,for example, partitioning systems, micro-servers, host applications, containers,and more to introduce countermeasures and reduce risk.Compensating controls.The security and privacy controls employed in lieu of the controls inthe baselines that offer equivalent, or comparable protection for a systemor an entire organization. Being aware of exceptions and exemptions.

#### Remediation Validation and Reporting

In this lesson, we're going to start out talking about validatingand reporting on your remediation processes and procedures.Implementing controls in response to vulnerability scanningand testing must be followed up with a security control assessmentand evaluation.It's one thing to go to the steering committee and getthe resources to implement technical, administrative, and physical controls,but they're going to hold you accountable on a monthly, quarterly,or semiannual basis to justify that they madethe right decision in giving you those resources.The next steps involve tuning to address false positives and false negatives,as we saw in an earlier lesson.This will lead to rescanning as well as any new configuration changesor updates/versioning.This is where Infrastructure as Code (IaC) can really come in handy.Official internal and or external audits should follow to assess compliance,maturity, assurance, certification, and accreditation.The assurance testing process concludes with validationand robust after action reporting.Let's take a look at some examples of robust reporting.In other words, what are the decision-makers, the steering committee,the C-suite or the C-team,as well as your own team members expect in modern reports?Well, they should have as much information as necessary,but not a data overload or a data dump.You may need to express certain terminology, or certain processes ina more simpler or basic terms, especially for non-technical stakeholders.For example, the Chief Financial Officer or Chief Privacy Officer that doesn'treally have knowledge or experience in cryptosystemsand various security infrastructure devices,or consider having different reports for different target audiences.Dashboards are very effective (R programming).For example, if you can find some advanced Python or R programming experts,you'll be able to deliver better visibility and more robust reports.So understand the components of visual communications.We want to avoid three-dimensional representation.Use a palette of sequential colors, avoid simple pie charts.Consider scatterplots, bars, bubble charts, histograms, density plots,and boxplots which can be much more effective at delivering meaningfulmetrics and key indicators.

#### Course Summary

In this course, you learned about threat feeds,application vulnerability assessment, and scanning.Penetration testing, and vulnerability response, remediation,validation, and optimal reporting.Coming up in the next course, we'll explore security monitoring and alerting.

### Security Monitoring & Alerting

#### Course Overview

Security Monitoring and Alerting.In this course, we'll explore computing resource monitoring and activities;examine SCAP, SIEM, and SOAR systems,and learn about data loss prevention systems,Simple Network Management Protocol (SNMP), and NetFlow.

#### Monitoring Computing Resources

Monitoring Computing Resources.Monitoring and visibility is a critical aspect of hardened securityand support for zero trust initiatives.The more automated the monitoring solution,the more accurate the results will be as human errorand configuration mistakes are minimized.All types of systems, applications, and services, including people,must be monitored, corporate LAN endpoint devices, web, email, productivity,and other application servers,for example, Microsoft SharePoint,which is very popular. Voice over IP, messaging, and conferencing services,databases and storage area networks,all infrastructure devices, switches, wireless access points,wireless controllers, multilayer switches,routers and other specialty appliances, and the customer premises edge.For example, your highly available Cisco ASR routers at your regional branchoffice. Network monitoring tools enhance visibility into system health byoffering real-time information and meaningful metrics about various wired,wireless, 5G, and other cloud-based components.This suite of tools utilizes two techniques to capture performance metrics fromassorted infrastructure and security devices - both physical and virtual:Agent-based monitoring, which leverages lightweight software, known asa monitoring agent, for example, an SNMP agent or a VPN client such as CiscoAnyConnect Mobility, on the devices or virtual machines to track the uptimeand performance. Agentless monitoring uses special application programminginterface calls or requests,or integrated code to track the health of the devices.Let's talk more about agent-based monitoring.A prototypical example of agent-based monitoring is usingSimple Network Management Protocol version 2Cand preferably the secure version 3 agentson infrastructure devices to send traps and informs to SNMP management stations.In cloud computing environments,special agents can be embedded or installed into virtual machine instancesor instantiated virtual servers to perform various system management activities.Usually, within the context of a Secure Shell 2 session.Here's a case study of a very popular solution from Palo Alto Networks knownas Traps. Let's say the end user working ina call center accidentally downloads an infected document, a PDF file,and they open it in an unsuspecting way.Palo Alto Traps, or the newer, next generation Cortex XDR,is seamlessly injected into the processes.An exploit technique is attempted,but it's blocked by Traps or XDR before any malicious activity is initiated.Then, the Palo Alto solution reports the event and collects detailed forensicor eDiscovery data. Forensic data is collected, the process is terminated,and the user and/or administrator are notified.Agentless monitoring: Agentless monitoring is a less intrusiveway to achieve visibility.It typically utilizes application-specific APIsand different network protocols (such as SNMPand Windows Management Interface - WMI)to discern the overall performance of on-site and cloud-based assets, suchas servers and applications.This monitoring method does not involve the overhead of installing, tuning,and updating dedicated or third-party monitoring agents on every component.This may be considered easier than the traditional agent-based approach.In this diagram, we can see agentless monitoring solutions.On the right-hand side, we seea wide variety of devices that are sending information.For example, Storage arrays sending SNMP version 3 or SMI-S, which isa storage management initiative specification.Network devices sending SNMP traps or informs.Windows/Linux servers sending WMI for Windowsor Secure Shell 2 from Linux.Virtual machines and hypervisors sending HTTP and HTTPS information.Applications sending SQL informationor JMX, Java Management Extensions, or users running browsersor other applications and appssending JavaScript. All to an Agentless manager or controller,often in the cloud, and then using Transport Layer Security or WebSocket APIsor RESTful APIs to the Remote data collector.

#### Monitoring Activities

Log aggregation is the process of accumulating, categorizing, standardizing,and consolidating log data from across an IT infrastructure to enableand enhance streamlined log analysis.Without log aggregation, administratorsand engineers would have to manually organize, deduplicate, normalize,and search through massive amounts of log data from a wide variety of sources,often on the same event or incident,in order to generate meaningful metrics andmake data become information and knowledge.The goals of log aggregation are to replicate log files toa centralized location, collect Syslog, auditd, and other trap information,support automated pipelines and workflows, parse key-value pairs,perform more complex transformations such as multiline log aggregation,tokenization, scrubbing, or masking or obfuscating sensitive data,tokenization being the most common and effective method.Here we see a simple example of log aggregation pipelines.The sources can come from services running in the cloud, containers eitheron-premise or in the cloud,various IoT and mobile devices, common web browsers, serverless solutions,server-based solutions, and databases, all fed into the pipeline targetinga centralized database to search & analyze, generate alerts,and monitor or send information back up to the cloud toa different service to do business intelligence or analytics,often with machine learning and AI tools.An alerting system, which could occur from eitherthe aforementioned things we saw in the previous slide, delivers metricsand alarms from various tools and systems to administratorsand security operators for informational/event notifications,incident management, and optimization of the wider ecosystem.These alerting platforms help to ensure that event responses are quickand efficient so that the odds of overlooked actions are reduced.In other words, false negatives.As systems grow larger and more complex,alerting systems become more automated and long-term orchestrated,often running behind API gateways on specialty platforms and server-basedand serverless solutions.Some best practices for alerting include choosing quality over quantity.Although you may accept everything early on in the life cycle,you'll soon want to filter, normalize, streamline,and focus. Produce actionable results. Data becomes information,information becomes knowledge.Consider broadcasting for mass notifications toa wide variety of interdisciplinary and cross-functional team members.Have a well-designed service desk and elevation or escalation process witha ticketing system. Prioritize alerts that are sent by peopleand not by automated systems. However, we do want to automate whenever feasible.Remember, automation does not mean entirely removing the human element.Scanning tools also send alertsand other output to information systems. Tools such as wiredand wireless IP scanners, for IPv4 and IPv6,port scanners scanning UDP ports, TCP ports,and even ICMP messages and codes, compliance scanning for Sarbanes-Oxley, GDPR,ISO/IEC, CIS, and other benchmarks, and vulnerability scanners such as Nessusor tools integrated into well-known exploit kits like Kali and Parrot.Let's look at the scanning and alert life cycle.Of course, the first part is planning or information gathering,then implementation, then testing and validation of the scans and alerts,removing false positives and tuning.Next is response, which could be a security orchestrationand automation response system with fully automated,semi-automated, and manual runbooks or playbooks.Remediation involving quarantine, compartmentalization,file disposition in the cloud or in the data center,and hopefully disposition or deletion, and then archiving and reporting.

#### Security Content Automation Protocol (SCAP)

Security Content Automation Protocol (SCAP).According to NIST: "The Security Content Automation Protocol isa synthesis of interoperable specifications derived from community ideas.Community participation is a great strength for SCAP, becausethe security automation community ensuresthe broadest possible range of use cases reflected in SCAP functionality."The importance of SCAP involves improving the cybersecurity posture.This ongoing process is called security hygiene.Secondly, you can streamline your vulnerability evaluation,vulnerability assessment, and vulnerability analysis.3, it simplifies compliance. Compliance to best practices,security policies,or regulation and governance.4, it makes software deployments easy or easier.And 5, it boosts collaboration inthe cybersecurity process between different stakeholders with different skillsets and different interdisciplinary focuses. SCAP has several specifications.It's important to have a general idea of what SCAP accomplishes,but on the exam you won't need to memorize these specific specifications.Asset Identification plays an important role inan organization's ability to quickly correlate different sets of informationabout physical and virtual assets, the most important often being data.The Asset Reporting Format (ARF) is a data model to expressthe transport format of information about assets, andthe relationships between assets and reports. Common PlatformEnumeration (CPE) is a standardized method of describingand identifying classes of applications, operating systems,and hardware devices present among the enterprise computing assets.You could compare this to the configurable items inan SNMP management information base.Open Vulnerability Assessment Language (OVAL),this is an information security community effort to standardize how to reportupon the machine state of computer systems.The Open Checklist Interactive Language (OCIL),this defines a framework for expressing a set of questions to be presented toa user and corresponding procedures to interpret responses to these questions.This can be extremely valuable in a SCAP audit.Trust Model for Security Automation Data(TMSAD) describes a community trust model that can be applied to specificationswithin the security automation domain, such as SCAP.The Extensible Configuration Checklist Description Format(XCCDF) is a specification language for writing security checklists, benchmarks,and related kinds of documents.Software Identification Tagging (SWID) allows forthe proper management of software inventories of managed devices in support ofhigher-level business, information technology (IT),and a wide variety of cybersecurity functions,or IT security management practices such as configuration management,change management, problem management, and other practices.

#### Security Information and Event Management (SIEM)

Security Information and Event Management (SIEM).SIEM is a solution that helps enterprises detect, analyze,and respond to security threats before they affect business operations.SIEM is a combination of security information management(SIM) and security event management (SEM) intoa unified security management system.SIEM technology gathers event log data from a range of sources, infrastructuredevices and applications, and services,and then recognizes activity that diverges from the norm in real-time.The excellent thing about SIEM, it can do many things.If you deploy something like Splunk on Red Hat Enterprise, which by the way,is now owned by Cisco, or a cloud solutionfor your hybrid cloud like Azure Sentinel,you can basically pick and choose what features you want in your SIEM system.However, one of the things that's best knownfor is its ability to collect information from a wide variety of devices,applications, and services, aggregate it, correct it, normalize it,and deduplicate it. Then the SIEM system can do other things like real-timealerting, object access auditing, forensics, file integrity monitoring,and more. Or you can send the output to another system called a SOAR system,or a wide variety of services running in the cloud.The benefits of SIEM systems include a centralized look at potential threats,real-time threat identification and response, advanced threat intelligence,regulatory compliance auditing and reporting,and enhancing transparency into users, applications, and devices.In the next lesson, we'll look at one ofthe most common repositories from events sent from SIEM systems,and that is SOAR.

#### Security, Orchestration, Automation, Response (SOAR)

Security Orchestration and Automation and Response (SOAR).SOAR is an assortment of softwareservices and tools that allow organizations to simplifyand aggregate security operations in three core areas: threatand vulnerability managementand/or incident response and/or security operations automation.Security automation involves performing security related tasks withoutthe need for human intervention, or reducing the need for human intervention,with semi-automated tasks. This can be defensive detection,response, and remediation,or offensive vulnerability assessment and penetration testing.Remember, you should automate the process if it's routine, monotonous,and especially time-intensive.The life cycle of a SOAR system involves detecting the threat,often from an event from a SIEM system, security information event monitoring.Then notification, investigation, response being fully automated,semi-automated, or manual, and then resolution of the problem.In this diagram, we see a combination of SIEM + SOAR systems.Not unlike running Cisco's Splunk on Red Hat Enterprise Linux or Windowsservers, or AWS Inspector or Azure Sentinel.The SIEM system sends out alerts based on a wide variety of different rules,from use cases and use case categories.The alerts are then mapped to incident categories within the SOAR system,and then contextual variables are considered,such as the criticality of the asset.For example, on a scale of 1 to 10,focusing on mission critical assets and mission critical data.The alert criticality, which may be on a scale of 1 to 5, 0 to 9, 1 to 10,or even green, yellow, and red. If it's a vendor or a service provider,they can classify based on service level agreements to give larger customersmore attention. And then classifying the data,for example, confidential, private,public. And then the criticality of the app or the network or the useror based on the analysis of log data.And then the playbooks would be fully automated,semi-automated, and manual to automate for defensive, forensic,offensive, or deception. Remember,a SIEM system can run separate from the SOAR system.A company may be only using a SIEM system or a SOAR system,or it's a combination of both, which is the optimal deployment.

#### Antivirus and Data Loss Prevention (DLP) systems

Antivirus software is intended to protect computers,mobile devices, and other systems from exploits, malware, various crackersconducting advanced persistent threats like ransomware,and an assortment of cybercriminals.These systems examine data on hard drives, data resident in memory,and swap files, as well as incoming packets from the Internet (websites,email messages, attachments, and applications) to recognize,block, and offer ongoing protection against malicious software, infected links,and other threats and suspicious activity, including malvertisementsand spyware. Antivirus software functions by regularly scanning alldevices to discover and block known worms and viruses,as well as new and emerging malware variants basedon their relationship with vendors and cloud providers.If a device gets infected,antivirus software will also quarantine and optionally eradicate it.Many systems employ a heuristic detection method with human derived rules toexamine code for suspicious architecture and behaviorrather than a specific static signature or established pattern likea spam filter.To offer the best possible protection,these systems use several forms of detection:Signature detection from a database,heuristic detection of files or human-made rules, multicriteria analysis(MCA), which uses the data from other detection methods to flag a fileas possibly dangerous. Sandbox and cloud analysis, for example,moving the potentially unwanted process pup to a sandbox environment ora quarantine area. Intrusion prevention via host intrusion prevention systems,anti-spam, and ransomware protection.Some of the features of antivirus systems include:Signature detection to look for specific code from known viruses.Heuristic detection to find suspicious architecture and behavior in code.Cloud and sandbox analysisto run suspicious programs inside a containedand secure system to see what they do. And host-based intrusion prevention,HIPS to bridge firewalls and other security systems for added protection.Another critical security service in the enterprise is Data Loss Prevention(or DLP). Data Loss Prevention is a security initiative that recognizesand mitigates unsafe or unauthorized sharing, transfer,or use of sensitive data such as personally identifiable information (PII)and protected health information (PHI).DLP engines and services can help organizations with monitoringand protection of sensitive information across on-premises systems,cloud-based locations, service providers, and various endpoint devices.DLP also assists with compliance for regulations such as HIPAA and GDPR.All different types of data, such as trade secrets, account numbers,intellectual property, and health records can leak by being stored on networkor shared drives, by being copied on external removable media devices,or transmitted electronically through email, webmail, instant messaging,or other modalities to a wide variety of unauthorized outsiders, competitors,regulators, authorized internal users have been recently let go or laid off,or the press or media, or a site like Reddit or GitHub,resulting in primary and secondary loss, loss of assets, breach of trust,company defamation, the cost of incident response,replacing a compromised employee, legal liabilities,and potentially the close or bankruptcy of the business.Here we can see a simple example ofa Data loss prevention engine doing Real-time scanningor Historical scan of stored logs or stored files.The DLP engine provides File and field level data protection,App and session context, and supports Data and compliance rulesfor email, software as a service solutions, websites, private apps,and a wide variety of endpoints.

#### Simple Network Management Protocol (SNMP) Traps

Simple Network Management Protocol (SNMP).SNMP is a powerful protocol and toolset that facilitatesthe sharing of information among various devices on a network,regardless of their hardware or software.Although there are many emerging replacements for SNMP,it is still widely deployed in enterprises globally.SNMP uses a basic client-server architecture that leverages managers whocollect and process information about devices on the network.Clients, called agents, are any type of device or device component connected toa network. SNMP uses an inverted tree architecture.At the top we have the Root where we can see this is the iSO organization.And to identify a configuration item,you would identify it based on the path that it takes from the Root.So, for example, if it's a cisco device configuration item,the identifier would be 1.3.6.1.4for private, .1 for enterprise, .9 for cisco.Here we just see three vendors.But SNMP is supported by many, many different vendors,all of which have their own configuration items.The newest version of SNMP is v3.Version 3 is still a client-server solution,so you have a Command Generator and a Notification Receiver.Older versions of SNMP version 1 and version 2C useda Community Based Security Model.In other words, a form of shared password called a community string.The community string could bea writeable string where you could actually change a configuration item,or it could be read-only.Version 3 went to a user-based security model,doing away with the community string,and also adding security algorithms and mechanisms for confidentiality, originauthentication, and integrity. Notice a couple of differences here.In version 3, it involved several more RFCs or requests for comments,and the authentication is no longer based on a community stringor community based. It is user and group based.Version 3 has no plain text community strings anymore,and it supports several mechanisms for data encryption, AES being preferred.Also, version 3 has no default or pre-known passwords.

#### NetFlow Records

NetFlow is a network monitoring protocol, developed by Cisco,invented to capture metrics about the volume and types of traffic traversinga network device, for example, a Cisco Adaptive Security Appliance.Technically, a flow is defined by its 5-tuple,a collection of five data points,which would be the source and destination IP address exchange information,the source and destination ports, if any(for example, ICMP doesn't use ports, it uses message types and codes.The protocol, for example, File Transfer Protocol orHypertext Transport Protocol.The most common and most recent version is NetFlow 9.By collecting and analyzing this flow data, engineers can learn details abouthow the network is being used for troubleshooting network issues,identifying bandwidth hogs,and tracking which external IPs or even countries one is exchanging data with.NetFlow has been around a while.It was first implemented in Cisco devices in 1995.It has followed a curious evolution over the years,starting as a static protocol with a fixed set of statistics collectedfor all flows. In version 9, the latest version from 2021,network professionals can choose which statistics to enable,and vendors can implement extensions.In other words, version 9 is extensible to attach proprietary metadata toflow entries. In the architecture,the NetFlow Exporter would be something like a multilayer switch ora customer premises equipment router or router,or a firewall appliance like a Cisco ASA.The software and hardware combination that's receiving the flows,which is a set of records, is called the NetFlow Collector.That could be something from SolarWinds or Scrutinizeror some other NetFlow Collector.The data is stored traditionally in relational databases,but not necessarily any more.And then the data is analyzed often in the cloud,with the results sent often to terminals or dashboards inthe security operations center.It can also be sent to a SOAR system for automation and orchestration.

#### Course Summary

In this course, you learned about computingresource monitoring and activities, SCAP, SIEM,and SOAR systems; and data loss prevention systems (DLP), SNMP and NetFlow.Coming up in the next course, we'll explore enterprise security capabilities.

### Enterprise Security Capabilities

#### Course Overview

Enterprise security capabilities.In this course, we'll explore firewalls, IDS, IPS, and web filters.We'll examine operating system security and secure protocols.We'll learn about DNS filtering, email security, and file integrity monitoring.And we'll look at data loss prevention (DLP) solutions,network access control, and endpoint detection and response.

#### Exploring Firewall Implementations

Let's start out this course looking at different types of firewalls,starting with a static or stateless firewall that we commonly referred toas an Access Control List.So I'm up at Amazon Web Services in my virtual network,which they call the Virtual Private Cloud,and I'm creating a Network Access Control List.So whatever I do, Inbound or Outbound,is applying to everything in the network or the subnet.So whatever I have deployed in this subnet, maybe it's Windows servers,maybe it's various versions of Linux.I could even have appliances that are virtual in this network.This Access Control List is going to be affecting all of the packets going inand the packets going out.So notice that in the rule, and this is a characteristic,an Access Control List is that there's an implicit or it implied Deny all.So typically when you use an Access Control List,you could use it as like an infrastructure Access Control Listand you could go in here and you could say I'm going to add a new rule.And usually, when you do this, you start with something like 100,and then as you add the rules you want to increment by 5s or 10s.So if I were adding things to this Access Control Listor what am I going to allow inbound, I'll go 100, 110, 120, 130.That way if I have to add another entry, and that's what this is called an ACE,an Access Control Entry, or a rule later on, and I want it to be likehigher up on the list or be processed first,I don't have to renumber them all because I've skipped by 5 or 10.So it's going to say next, what really are you going to filter on?You can see we've got ICMP traffic, TCP, UDP traffic,and here's our common services or protocols.And later on in this course, I actually am going to give you a lessonto remind you of these different port numbers.So right now we're not going to worry about memorizing these port numbers,for example, LDAP being 389, but we're going to go through here and say,you know what are we going to allow.That's our goal.Now another way to approach an Access Control Listis having the first entry allowing everything.So I could say you know the first entry is All trafficand allow everything and then only come in and if I need to put in specific denyentries if I'm under attack. From a certain IP address,you know the Source IP address and the whatever portthey're attacking on, but here you can see you want to identify.And lot of these ports are well-known, but you'll also see people use portsthat aren't well-known, like HTTP on port 8080 or HTTPS, TLS on 8443.Others are more proprietary.So for example, AWSs data warehousing solution is called Redshiftand it uses port 5439 or you know MySQL/Aurora (3306).Now you wouldn't need to memorize those on the exam by any means.Those are usually the higher numbers.But we're going to see later on in this coursewe're going to talk about these that you do need to be aware of.OK, so this is how an Access Control List works.I'm going to Cancel out of here and realize it applies to all the trafficgoing into the network or Outbound rules out of the network.Now that's a type of firewall, that's what we call stateless or static.Does it's really agnostic to whether it's a TCP three-way handshake,or it's a DNS response or whateverit doesn't know, it just looks at each packet one at a timeand it compares the rules to the headers, right, that we just looked at?You know the address, the ports, the services,those are in the headers of layer 3 and layer 4.Now, cloud providers and vendors also offer a type of firewallcalled a security group, and this is what we would callan allow list or a stateful firewall. At AWS, the security group,which is a stateful firewall and if we look at this,it has Inbound rules of what we're allowing.But notice that in a security group firewall, there's no numbers.It's just what we would call a white listor a allow list of things we're going to allow.There's also no permit and deny, it's just all what you're going to allow.But a security group at Amazon Web Services is applieddirectly to the instance.So technically it's applied to the virtual network interfacethat has a virtual Mac address of the server,let's say the Red Hat Enterprise Server.If it's called a network security group,that you might see somewhere like Microsoft Azure.If it's a network security group, then it's a stateful type of firewall.It's storing information about the TCP connection,the UDP flow, the ICMP behavior.It's storing that usually in the cloud, in the hypervisor, or in some typeof firewall appliance or whatever security gateway.If it's a security group it can be applied to the instance or if it's a networksecurity group that would be applied to everything in the networkor in the subnet.So those are two kind of main types of firewalls that you would see generally.The static or the stateless network access control listand the stateful kind of dynamic allow list Security groupwhich could be applied either to an individual instanceor it could be applied to an entire networkthat would be called a network security group.Now another term on the exam to be aware ofis what's called a screened subnet.So here's a diagram of what a screened subnet is.A screened subnet is another fancy termfor a three-legged or a three-armed firewall.Where you have this firewall, it could be a firewall router,it could be a firewall appliance like a Cisco ASA,maybe it's a Juniper firewall or a Palo Alto Networks.And it's got three interfaces going to three different zones.So this screened subnet is basically a three-armed firewallwith one interface going to the public Internet.So actually either connecting to an upstreamed routeror your upstream customer premises equipment,or if it's a regional office or a branch office,it could be actually the Ethernet connectionthat you're using to go to your provider, your broadband provider.So that's the Internet or the public zone or leg.And then you would have a DMZ or a public access zone.That's where you would have all of your public-facing servers.So if you have anybody on the Internet that wants to get something from you,you must put those servers in that DMZ network.Web servers, FTP servers, maybe you have DNS servers out therewhatever but only the things that you're making availableto the public go in that DMZ network and then you would have an insideinternal network.So this screened subnet or this three-legged firewallis a very historically common solution for the small to medium-sizedbusiness and it's something to be aware of on the exam.

#### Intrusion Detection and Prevention Systems (IDS and IPS)

Intrusion Detection Systems, IDS, and Intrusion Prevention Systems IPS.IDS and IPS is a combination of hardware and/or softwareto allow visibility and mitigation of existing exploitsand malware on the network or on individual hosts.However, today it's usually IPS and it's usually network-basedas host-based IPS is often reserved for endpoint detection responseand other next-generation endpoint solutions.Snort IDS running on Unix machines was the original intrusion detectiondaemon, so it's been around for quite a while.Original services were highly static signature-based solutions,but they included anomaly detection in later models.These have evolved into advanced next-generationartificial intelligence and machine learning solutions.Let's compare traditional IDS to IPS.On the left-hand side, we have intrusion detection.Traditionally the IDS sensor was a standalone applianceor a daemon running on a Unix or Linux box.The layer two switch would isolate certain ports using a protocol known asSPAN to send copies of the frames to the IDS sensor.Then, if the packet matched a signature,it would send an alert or an alarm to the management station.But the IDS was actually reactive, so it's possiblethat the attacker actually delivered the malwareor the exploit to the target on the corporate network.In other words, the IDS sensor was not inline,so it couldn't take aggressive action such as dropping packets inlineor denying attackers based on their IP address.The IPS is an inline solution.Again, this could be a standalone IPS sensor running behind the firewall,or it could be a service or a module in the firewall.Either way, it's physically or logically behind the firewall,but it's inline, so it can actually aggressively drop packetsor deny attackers before it reaches the corporate network.This would be a proactive solution.Today most solutions are intrusion prevention,and they're initially deployed in an IDS mode or a passive or monitor mode,so they can tune them to reduce false positives, for example,and then put them into inline mode.Some common characteristics of intrusion prevention systemsor that they are inline or in an initial monitor or passive mode.You may deploy the IPS sensor in a passive mode,for example in a honeynet for active defense.They can be in-band or out-of-band.That's another way to think of it as inlinebeing in-band or detective or passive mode being out-of-band.They could be signature-based and anomaly-basedor they can have human-written heuristic rules.But modern IPS solutions that qualify as next generation will leveragethe cloud often and use machine learning engines and artificial intelligence.And they use cloud-based or vendor-based correlation and integration.Some of the actions that can be taken are alerts and alarms and verbose dumps.This can be done by either IDS or IPS sensors. TCP resets, blocks, and shuns.An IDS or IPS sensor can both do a TCP resetor send a block to an upstream router, or a downstream router,or multilayer switch, or a shun to a firewall.This is an aggressive action that can still be taken by IDS.However, it's not inline, it's reactive.Only IPS inline sensors can block attackers and drop packets.Both IDS and IPS can send traps in forms,alarms, or alerts to assist log servers.They can be SNMP agents and they can send NetFlow records,for example NetFlow version 9 to NetFlow collectors.And both IDS and IPS can integrate with SIEM and SOAR systems.When it comes to tuning IDS and IPS,what we want are true positives and true negatives.A true positive is when a sensor accurately takes an action.A true negative is when a sensor accurately does not take an action.In other words, it's benign traffic. Early on in the processyou want to fine-tune to reduce false positives.This is an error state because a positive action was taken.In other words, you blocked benign traffic and a false negativeis when you falsely or in an error statedid not take action when it was necessary.IPS tuning involves reducing false positives and reducing false negatives.

#### Web Filters

In this lesson, we're going to look atweb filtering and some of its capabilities.A web filter is an application layer gateway server or service.It can be physical and/or virtual dedicated to analysis and controlof HTTP and/or HTTPS traffic.HTTPS today being Transport Layer Security. Agent-based web filtersrequire the deployment of lightweight software packageson network devices, whereas agentless filters can be instantly deployedin RAM memory or persistently without any manual configuration.Many filtering solutions are deployed on the customer premises equipmentas a centralized proxy to process all web traffic from layer 3all the way through layer 7of the OSI model or the application layer of the TCP/IP model.Some of the capabilities of web filtering include safe search.End users can do safe searches or go directly to a websitewith URL filtering, they can provide protection for DNS,a well-established unsecured service running at layer 7 on the Internet,So it can provide DNS filtering specifically for DNS queries on UDP port 53.They can provide URL filtering to prevent either local end usersor remote end users going through a VPN gatewayor a controller to go to certain websites, like adult sites,gambling sites, and other unauthorized web surfingbased on the acceptable use policy.They can also perform content categorization in real-time or file filtering.Many end users don't use a dedicated FTP client.Instead, they use their browsers to upload and download files using FTP.Web filters can also filter for file uploads and downloads as well.Another common feature of web filters,whether they're running in a Web Application Firewallor a web security gateway is reputation filtering.The accuracy of a web reputation service is typically determinedby the breadth, depth, and variety of the data being used.The algorithms used to analyze the relationships between Internet objectsand web reputations must be persistently trained by experiencedhuman analysts and possibly using artificial intelligence tools.With an accurate web reputation source fueling associated URL filters,firewall solutions, or other specialty appliances,one can produce a resilient, proactive cybersecurity posture.Finally, when using cloud correlation and cloud reputation,vendors and cloud service providers can distribute this informationto devices and services all over the world.Often notifying their customers within minutesof a new zero-day attack, by a bad actor, its domain, or its IP address.

#### Operating System Security

In this lesson, we're going to look at operating system securityand specifically topics that would be on the exam.This is actually a really broad area of securityand we're going to focus first on Group Policy.Group Policy, or GP, is a Microsoft Windows service, and it's an importantservice because Microsoft Windows is such a popular operating system worldwide.It enables IT administrators to centrally manage and configure the settingson Windows operating systems, both software and hardware.Group Policy can manage operating system settings, applications,browsers, and user settings.From a broad standpoint, Group Policy is used in Active Directory environmentswith domain-joined computers as well as Microsoft Azurehybrid joined device, Azure being the Microsoft cloud service provider.Some Group Policy examples include Password Policies,Screen Lock settings, Power Settings, for example, to supportsustainability, Mapping Network Drives not just to file serversin the local data center or server farm, but also network drivesto cloud resources. Assistance in installing printers, software,desktop shortcuts, and more. Implementing software restrictionsfor example, blocking access to certain programs like BitLocker.Group Policy Objects (GPOs) are collections of policy settingsthat apply to the domain or organizational unit OUto manage users, computers, or the entire domain.Here we see a screenshot, and actually not a very clear screenshot,but it's one of the Group Policy Management Console (GPMC).Now on the exam, you won't have any specific configuration questions,but as you can see in the diagram, the Group Policy Management routebegins at the forest, and a forest can contain multiple domains.The forest is contoso.com and each domain has its own default domain policybroken up into three areas, Groups, Servers, and Users.We can also see areas to manage domain controllers,and there's those GPOs, Group Policy Objects.Let's switch from the Microsoft worldto the Linux world. Security Enhanced LinuxSELinux is an access-controlled system built into the Linux kernel.It's used to enforce the resource policies that define what level of accessusers, programs, and a wide variety of serviceshave on the Linux or Unix-based system.In its default enforcement mode, SELinux will denyand log any unauthorized attempts to access any resource.This enforces the principle of least privilege, in that explicit permissionmust be given to a user or application to be able to accessany file, directory, socket, and a socketas a combination of an IP address and a port number, as well as other services.There are several ways to configure SELinux to protect a system.Realize this is often built into an imageor a pre-established baseline that you download.The most common configurations are targeted policyor multi-level security (MLS).Targeted policy is the default optionand it covers a range of processes, tasks, and services.MLS multi-level security can be very complicatedand is typically only used by government organizations.For example in a mandatory access control modelor a strict zero-trust environment.The /etc/sysconfig/selinux file has a section that shows youwhether SELinux is in permissive mode, enforcing mode, or disabled,and which policy targeted MLS is supposed to be loaded.Here we see a diagram of the SELinux architecture,when the subject or process wants to get accessto a file or a socket or a process, it goes to the Access Vector Cache (AVC).The Security Server determines if it's Ok.If it's no, it gets sent back to the cache and SELinux will deny access.If it's allowed, then the security server applies the policy.Notice that SELinux operates all the way down at the Linux kernel level,so it's extremely low level, and only explicit access is giventhrough the security server policy to the object.

#### Security Protocol Implementation

With the Security+ exam,you do want to have some type of networking background.In fact, CompTIA recommends you have the A+ certificationbefore you take the exam, although it's not a prerequisite.Here's the 7-layer OSI model.Layer 7 being application, layer 6 being presentation, layer 5 being session,layer 4 transport, layer 3 network, or internetwork.Layer 2 is link or data link and layer 1 is physical.You should memorize this 7-layer model from top-downand bottom-up, perhaps using a mnemonic or make up your own.Notice that layer 7, all of those services are unsecure by default or by design.That's one thing to consider when it comes to security.You should be aware of the transport layer as well with TCP and UDP.And the network layer with IP, ICMP, ARPand dynamic routing protocols like BGP and OSPF.You also want to memorize certain ports for the exam,remembering that FTP File Transfer Protocol uses 2 ports,21 for the control and port 20 for the data files.Secure Shell 2 is a secure protocol along with SFTPor SSH file transfer protocol, which both use port number 22.TACACS+ is a Cisco proprietary AAA authentication,authorization, and accounting service on TCP port 49.DNS can use port 53 TCP to transfer the database, for example,but it uses UDP port 53 for its DNS queries.DHCP also uses two different UDP ports, 67, and 68.And of course, HTTP uses port 80 for its clear text traffic.Most Microsoft Active Directory sites are going to use Kerberos on port 88.The older Post Office Protocol uses 110.More often than not, you're using IMAP for your email client on port 143or secure port 993, which is IMAPS. SNMP uses two different ports.LDAP uses port 389 or LDAPS, which is really basically just LDAP with TLS.Transport Layer Security uses port 636,and HTTPS or HTTP with TLS is going to use port 443.You need to memorize these for the exam.FTPS also uses TLS, so instead of port 20 and 21, it's going to use 989 and 990.Secure IMAP will use 993, secure POP will use 995.RADIUS has used different ports historically, but most modernRADIUS services are going to use 1812 and 1813 and that'll be UDP.Diameter is the upgrade to RADIUS on port 3868 and then the secure RTPwhich is really a Voice over IP transport layer protocolinvented by Cisco and Ericsson.The secure version of that would be 5004.And as mentioned, most of those layer 7 or application layer servicesand protocols are unsecure natively or by default.So in the next lesson, we'll look at how we can secureone of those very unsecure protocols, DNS.

#### DNS Filtering

As mentioned in the previous lesson, the Domain Name Serviceor the Domain Naming Service DNS is one of the most notoriouslyunsecure protocols in the TCP/IP stack on the Internet.So you want to try some things like DNS filtering.This is a technique of using DNS to block malicious websitesand filter out damaging or unsuitable content.This filtering ensures that organizational data stays secure and private.You can also call this content security or even data loss prevention.It allows the enterprise to have control over what their employees can accesson company-managed networks and endpoints.DNS filtering is often part of a wider access control strategy.With DNS filtering all DNS queries are delivered to a DNS resolver.Realize this could be a controller running in the cloud as well.Specifically configured DNS resolvers often function as filtersby refusing to resolve queries for certain domains, for example in the dark webthat are tracked in a blocklist or reputation list,therefore blocking users from accessing those domains servers.DNS filtering services can also use an allowlist instead of a blocklist.DNS filtering can also blocklist web attributes based on the domain nameor IPv4 or IPv6 addresses by domain.Here the DNS resolver will not resolve or look up the IP addressesfor certain domains at all. By IP addresswith this filtering mechanism, the DNS resolver attemptsto resolve all domains, but if the IP address is on the blocklist,the resolver will not send it back to the requestor.Another solution to secure DNS is DNSSEC or DNS Security Extensions.DNSSEC adds a layer of trust on top of DNS by providing authentication.This extension does not provide confidentiality though.When a DNS resolver, for example looking for www.skillsoft.com,the .com name servers help the resolver verifythe records returned for skillsoft,and the security extension servicehelps verify the records returned for the site.The root DNS name servers help verify .com,and information published by the root is vetted by athorough security procedure, including the Root Signing Ceremony (RSC).DNSSEC also allows you to digitally sign your company's DNS recordsso that any system that has an authenticating DNS resolverwill automatically verify if the records are validor if they've been compromised by a man-in-the-middleor what we would call today an on path attack.To facilitate signature validation, DNSSEC adds a few new DNS record types.RRSIG contains a cryptographic signature, DNSKEY contains a public signing key,the DS record contains the hash of a DNSKEY record.The NSEC and NSEC3 are for explicit denial-of-existenceof a DNS record, and the CDNSKEY and CDS recordsare for a child zone requesting updates to the aforementioned DS recordor records, the one that contains the hash of the DNS key recordin the parent zone.Another popular solution is OpenDNS.OpenDNS is a company that offers DNS resolution servicesand a suite of consumer solutions with the goal of making the Internet faster,safer, and more reliable.It's also a cloud-delivered enterprise security servicethat protects against threats on the Internet.OpenDNS's consumer products include parental filters,content filtering, web performance, and web security.They offer businesses the Umbrella service, for exampleCisco Umbrella, which is designed to protect against malware,botnets, phishing, and targeted online attacks.It's very common for a company who's deployed the CiscoAnyConnect Mobility Client to also use Cisco Umbrella for all the DNS queries.

#### Email Security

As we've already established, another common vector foradvanced persistent threats and other exploitsis through the email electronic mail.In this lesson, we're going to look at several countermeasuresand ways to enhance email security,starting with SPF Sender Policy Framework.Practically all abusive email messages carry fake sender addresses today.The victims whose addresses are being spoofedoften suffer consequences such as reputational damage,the need to repudiate the abuse which can be costly,and the time it takes to sort out misdirected bounced messages.Sender address forgery is a threat to both usersand companies because it undermines the email medium and erodespeople's confidence in email.This is why, for example, a bank never sends direct informationabout an account by email and keeps making a point of that fact.The Sender Policy Framework is an open standard that introducesa method to prevent sender address forgery.More precisely, the current version of SPF - called SPFv1 or SPF Classic -protects the envelope sender address, which is used for messages delivery.SPFv1 permits domain owners to designate their mail sendingpolicy, for instance, which mail servers they use to send mailfrom their domain, for example Microsoft Exchange.The SPF solution requires two sides to work together.The domain owner publishes this information in an SPF recordin the domain's DNS zone,and when someone else's mail server receives a messageclaiming to come from that domain,the receiving server can check whether the message complieswith the domain stated policy.For example, if the message comes from an unknown server,it will be marked as fake.Another solution is DomainKeys Identified Mail or DKIM.DKIM is an email authentication methodconducted between the outbound and inbound mail serveror Message Transfer Agents.The authentication process happens transparently to the end user.With DKIM, the outbound mail server appends a digital signature to the mailthen the inbound server verifies the signature by looking up the public keyand then comparing it with the signaturefrom the specified outgoing mail server.With DKIM, if the public key does not match the signature,it may be because the email was not sent from the email serverdesignated in the email header,but was instead sent from another spoofed server instead.Or the email was modified in transit the recipient.For instance, an attacker could intercept an emailthat was sent from a valid mail server, change it and then resend it.Domain-based Message Authentication Reporting and Conformance (DMARC).Organizations and end users undergo a highvolume of spam and phishing from the Internet.Many modern solutions work in isolation from each other.Each receiver makes exclusive decisions about how to evaluatethe spam and phishing reporting results.Legitimate domain owners rare to get any meaningful feedback.DMARC attempts to address this by providing coordinated, testedmethods for domain owners and email receivers.DMARC is an email authentication, policy, and reporting protocol.It builds on the widely deployed and previously discussed SPFand DKIM protocols, offering linkage to the sender or"From:" domain name, published policiesfor recipient handling of authentication failures,reporting from receivers to senders,to enhance and monitor protection of the domain from fraudulent email.Email security gateways are special gateway appliancessuch as the Cisco Email Security Appliancethat are dedicated email security servicesthat work in or with MTAs to protect electronic mail.They're also called secure email gateways (SEGs).This is commonly a suite of tools that filter emailsas they enter or leave the email server.Emails are routed through the gateway serviceand typically require the DNS MX-records to be changed,regardless of the email platform.Many email providers today also offer a cloud-native email security solutioncalled an Integrated Cloud Email Security (ICES),either in parallel or as a replacement for the legacy SEG,which would often be a highly available appliancerunning in the server farm or the data center.In this diagram, email security is accomplished by either using a cloudprotection layer like Fortinet, Fortigate, and/or an email security gateway.The email security gateway could be a physical highly available appliancein the data center or it could be something installed in a hypervisor.If we use the cloud protection layer, we'll get our anti-DDoS protection,our antivirus, our anti-spam,and then we can optionally send that to the email security gatewayor directly to the email server.However, we have this setup, in this diagram, we have the initialcloud protection layer and then the email that gets through thatgoes to the email security gateway locallyfor the anti-spam deep inspection and then our own policy engine.Then the traffic is sent outbound through the email security gatewayto do rate controlling, antivirus, anti-spamand then another policy engine for outbound email.Often that policy engine for outbound is a Data LossPrevention engine or we could be using a Cloud Access SecurityBroker that can be sent back up to the cloud protection layer to go outbound.In this diagram, all of the outbound email is going through our own system,where we go through our own e-mail security gateway,potentially the DLP policy engine,and then we send encrypted email outbound using AES 256or perhaps digitally signing it with S/MIME.

#### File Integrity Monitoring (FIM)

In this brief video, we'll define file integrity monitoringfor the Security+ exam.File Integrity Monitoring, or FIM, examines operating system files,configuration files, registries, application software,and Linux system files for changes and for indicators of compromise.Windows FIM provides alerts about suspicious activitysuch as file and registry key creation or deletion.Any modifications to files, for example, changes in the file sizeaccess control list and a hash of the content.Registry modifications, changes in size, access control list,registry types, keys and contentIn the FIM lifecycle, everything begins with the baseline.For example, leveraging your change and configurationmanagement principles or practices,you must have the known and trusted state.Preferably it's digitally signedfor integrity and non-repudiation.You must have visibility solutions, for example SIEM systemsto notify you of unauthorized changes or the new existence of suspect files.In a zero-trust environmentthat would involve robust and semi-automated detection, alert, and reporting.And then of course your incident response to handle the situationand get the servers, the workstation, the endpoints, or the network devicesback to a known and trusted state.This is a highly iterative process and it could be automatedwith robust inventory systems, SIEM and SOAR systems,and highly skilled security practitioners,especially on the incident response team.

#### Data Loss Prevention (DLP)

Data Loss Prevention DLP. DLP involves protecting theloss and leakage of all different types of data.Intellectual property like trade secrets or account numbers,PII such as social security numbers, PHI such as personalhealth records and other data that must be kept private or secret.That data can leak using a wide variety of vectors,for example removable drives, USB, and FireWire or memory cards.It can be stored on shared drives such as file serversor peer-to-peer file sharing or personal cloud services.It can be data moved to a cloud service provider.It can be transmitted electronically through email, Webmail,instant messaging, or other vectors.And it can leak to a wide variety of unauthorized personnel,your competitors, regulators, unauthorized internal users,for example, recently laid off or fired employees.Or it could leak to the press, or to the media,or show up in places like Reddit or GitHub,resulting in primary and secondary loss. Primary loss being immediate loss,secondary loss being subsequent loss.Realizing that secondary loss can actually be more costly in the long runthan a primary loss.Results such as company defamation,the monetary expense for each of their records lost, legal liabilities,criminal and civil, the loss of assets, lenders charging a higher interest rateor insurance premiums going up, breach of your customer trust,and even bankruptcy or the close of the business.There are a variety of hardware and software solutionsthat can mitigate data leakage and data loss.You can implement secure email gateways for exampleon your Microsoft Exchange Mail Transfer Agentsor some other Linux-based solution in Red Hat Enterprise.A cloud-based email security solution, for example from Ciscoor Palo Alto Networks.Cloud access security brokers that can assist with data loss prevention,compliance and single sign-on solutions,using endpoint detection and response or next-generation EDR,implement database activity monitoring which is transparent to the useras they store and send data through a variety of platforms and databases.

#### Network Access Control (NAC)

Network admission control or NAC was an industry initiativesponsored originally by Cisco.It typically enables IEEE 802.1X port-basednetwork access control on Layer 2 and Layer 3 networks.With NAC, the enterprise does not trust anything inside or outsidethe perimeter without stringent authentication and verification,typically attribute based.This helps secure access from users and their devices.Application programming interface calls IoT components,microservices containers using Docker and Kubernetes and more.In this diagram, we see traditional network access control.On the left-hand side,today we have a wide variety of devices.These icons are a little bit outdated, but we know we have laptops and pads,mobile phones, voice-over IP devices, and other IoT components.To get access to resources on the network, they're going to go through an NAD,which is a network access device.Now this can be an IEEE.1X enabled switch, layer 2, or multi-layer switch.Or of course as you see at the top it could be a wireless access point.On the backend, we're going to have a wide variety of services.If it's a wireless access point, they'll definitely be a wireless controlleron the backend, that could be represented as the NAC/EDR server.We have the choice to run a NAC/EDR serverdirectly off of the network access devicesor they could be in a backend server farm or data center.NAC is our network admission control server.For example, Windows Server 2022, Enterprise Red Hat Linux,EDR is our endpoint detection and response solution.That could be something like Palo Alto Traps or a SIEM and SOAR systemusing Splunk which is now part of Cisco.On the backend of the Network Admission Control, we have an identityservices engine and that's going to support our attribute-based access controls.And of course, probably supported by some type of identity provider,in this example an Active Directory environment.The beauty of NAC is that when the device attempts to connect to the networkwith an EAPoL start frame ExtensibleAuthentication Protocol over LAN, the NAD can either request credentialsor the credentials were providedwhen the device attempted to get on the network.It could then pass those credentials back to the backend,and in this environment of .1X, it's usually a RADIUS or diameter server.And then based on the credentials provided,a NAC environment can do a wide variety of things.It can refuse to even allow the device to get onto the networkor even connect to a DHCP server altogether.Or it could put it in a restricted VLANand then go to one of the backend services to be remediated.For example sending them a captive portalwhere they have to go to a remediation serverand update their antivirus, or install some agent,or do an upgrade or an update.Then they get onto the network.Or they can be put into a guest VLAN to simply go out to the Internet,but still controlled by content security,URL Filtering, and other engines like Data Loss Prevention.You can also restrict certain ports and isolate themso that only a printer or a Biz Hub can connect to them.And placing them in restricted VLANs is only one type of activity you can doin an attribute-based access control environment.In certain Cisco environments you can inject datainto the outgoing frames as they go from device to device.You can put special rules on the NAD device to control IP connectivity.This is a very popular solution.In fact this is what Google uses in their zero-trust environment.More often than not todayyour network access control and EDR solutionendpoint detection and response is going to be cloud-based.Often with a vendor like Palo Alto Networks or F5 or Fortinetor Cisco, where the external usersare often being authenticated and authorized against a controllerand a software defined perimeter environmentand then getting access to the internal network.Or people who are attempting to get on to the corporate networkusing wired or wireless or having their information sent upto a cloud-based solution.As I mentioned, a very popular one would be Palo Alto Cortex XDR.

#### Endpoint Detection and Response 

Endpoint detection and response solutions.In the previous lesson when we looked at network admission controlboth on-premise and cloud-based, I referred to EDRas part of the solution several times.In this lesson, we're going to dig a little bit deeperinto modern endpoint detection and response.EDR has evolved from early host-based IDS solutionsand it involves now a more "lighter" software agent, for example,Palo Alto Traps Cortex XDR or a Fortinet agent installed on the host system.This often provides the basis for event monitoring and reporting.EDR tools focus on detecting and investigating suspicious activitiesand are indicators of compromise on hosts or endpoints,or part of an active kill chain, for example,escalation of privileges or lateral movement.EDR monitors endpoint and network eventsto send information off into a SEIM system or a centralized database,or to the cloud for further analysis, investigation, and reporting.Modern solutions are Extended Detection and Response (XDR),which include machine learning, user behavioral analytics.The main components of extended DRare threat protection driven by machine learning,incident management, automated and orchestrated, automated rootcause analysis or problem management, deep forensicsas part of e-Discovery after an incident.Flexible response based on attribute-based access controls and otherrisk-based access models often supported by the network admission controlenvironment or cloud-based solution. And extendedthreat hunting for penetration testers and cyber forensic teams.Here's an example of a modern solution that's becoming quite popular,Palo Alto Cortex XDR.The endpoint will run the lightweight XDR agent that'sresponsible for collecting events and sending alerts.They can be periodic through the Cortex Data Lake, or they can bead hoc on-demand and immediate sent to the Cortex Data Lake.The security administrator in the Security Operations Centeris running the Cortex XDR management Console,which is also integrated with several cloud components at Palo Alto Networks.And includes two main engines, the Analytics Engine andthe Causality Analysis Engine, commonly drivenby machine learning and user behavioral analytics.

#### Course Summary

In this course, you learned about firewalls, IDS, IPS, and web filters.Operating system security, secure protocols, DNS filtering,email security, and file integrity monitoring.And data loss prevention solutions, network access control,and endpoint detection and response.Coming up in the next course, we'll explore identity and access management.

### Identity and Access Management

#### Course Overview

Identity and Access Management.In this course, we'll learn about provisioningand deprovisioning user accounts, understanding password concepts,know federation, single sign-on, and access control models,examine multi-factor and biometric authentication,and define privileged access management (PAM) tools.

#### Provisioning and Deprovisioning User Accounts

Let's begin this course with a demonstration.We're going to talk about user accounts and provisioning and deprovisioningand some other more deeper issues like identity proofing.Now, if you're going to be working for an organization,they're very likely going to have either an onsite directory servicelike Active Directory or maybe they're going to use Azure AD up in the cloud.And if that's the case, when you get hired or you move to adifferent area, you're going to go through a pretty automated processof being populated as a user into a directory.And in that directory, it'll be a whole lot of information about you.And generally speaking, you'll be placed into a group and that groupwill give you the permissions that you havethroughout the organization to resources.We call that provisioning or onboarding.Now you can also provision an onboarding devices.So when you get hired and you get put into a directory,you're also going to be provisioned laptops, mobile devices, pads,other specialty equipment.And that's also part of provisioning or de-provisioning.In this simple example, I'm going to look at provisioninga user up in the cloud.Now, I could do this through federation or single sign-on,but in this case, up in AWS, I've got groups of users,Auditors, Managers, and Production, and I'm going to create a user.And when I Create the user here, you want to give it a name.And this is where we get into what it's called schema.So, if you're going to be using a directory service on-premisesor in your organization, you're going to have an establishedschema like the first initial of your first name and then your last name.Maybe you know your first initial and underscore or your full name,maybe for me, Michael.Shannon.But whatever schema you're using, it has to be unified and consistent.If you're going to also provision users somewhere else,the best case scenario is to use the same naming scheme.So, for example, here I'll use first initial underscore and then a last name.Now, here in the cloud, when you provision a user and this ispretty much across the board, you really want to consider what's calledseparation of duties, and within the separation of duties, least privilege.So, for example, how I would separate duties here is I would say is this usergoing to use a graphical management console?And if that's the case, I would just go ahead and create this user.Or do I want this user to have programmatic access and get access keysand do it that way through a command line interface or throughsome software development kit or something like that?So, the thing here is you shouldn't give users bothprogrammatic access with an access key and make them console users.So, this is part of separation of duties.Now, is it possible to have a user that's both?It is, but it should be very rare.Maybe a high-level programmer or developer, maybe who's a team leador a lead developer would need to have their programmatic credentials,but then also want to manage through a graphical interface.So, that's kind of a separation of duties aspect in the cloud.And then within that, whether they're using the consoleor they're using some programmatic access,you only want them accessing the services that they need access to.So, what we're doing here is we're creating a persistent userwho's either going to have 24 hours a day,7 days a week console access, or programmatic access.Let's go back.So once I create a user, I'm not going to assign permissions to that user.You just saw a user name S_Shannon.I'm not going to assign permissions or what we call policies to that user.And you wouldn't really do that in a directory service either.You would put the user into a group or nested groupsthat give them the rights and permissions that they need.So, for example, here I've got several groupsand I would put the user in the groupthat I want them to have the permissions to.Now in the cloud, this might be different than your own directory service.Like, for example, if you're using Active Directory.In the cloud, what the groups can do are based on the policies that they have.So, policies in the cloud,if we look at, are called Permissions are basically a JSON documents.So let me look at it in JSON here.Now, on the exam, by no means you need to understand JavaScript ObjectNotation language, but if you're at AWS or Google Cloudor some of the third-party platform,it's very common to assign permissions with JSON.And then they're going to be making APIsor application programming interface calls or requests.So when you apply, for example, some type of policy or permissions,you're basically saying what is the allow list?What API calls are they allowed to make?And by default, these kind of policies don't have any conditions in them.OK, you have an Action, you have an Effect which is Allow,and then what Resource can they access.Could you put conditions in here?You could, you could Copy this and make your own customized permissions.For example, you might want to put permissions that say they can onlyfunction in a particular organizational unit.Or you could do a time-based type ACL.So, you could say they're only allowed to run these API callsagainst these resources certain times of the day.So that's something that you want to consider.So, provisioning is adding users generally to groups.It also includes giving them the rights and permissions to access resources.But in a broader scale, provisioning also includesyou're giving them the endpointsand the hardware that they can use, right?The devices they can use.The opposite of that would be called de-provisioning.They saw it coming here and delete users or delete users and groups.Now, another key thing is what we call identity proofing.So, for example, if you're hiring a new user, often you're going to go throughmore stringent background checks than just simply they're going to provide youa single driver license or a passport.Proofing goes deeper into looking into their background and making sure thatthey are who they say they are because they could be somebodywho's got a false identity, they're part of a hoax,they're part of an advanced persistent threat.So doing things like proofing is critical today in the modern enterprise.So, for example, if you were to go open up a Brokerage Accountat somewhere like E-Trade, you would provide additional credentialsto open up the account.However, if you want to do an elevated or escalated process,like you want to make a large withdrawal or you want to start tradingon margin, it's like that you're going to go through step-upauthentication or what's called identity proofing.You know, this happened to me with my IRAwhere I wanted to do a withdrawal from my IRA.So they set up a whole separate secure Zoom meeting with this third party.And then I was on camera for a secure Zoom meetingand I showed him two forms of ID with a picture.And I also had to answer some questions based on my public record.So that's what we call identity proofing and it's a higher levelof authentication authorization and it's a conceptthat they'll very likely test you on the exam.

#### Exploring Password Concepts

In this lesson, let's do another web Safari demonstration,and we're going to come back up here to the IAM area of Amazon Web Services.And I want to go into this area called Account settingsbecause this is where you're going to have what's called Password policy.Now, as I mentioned, you could be using Active Directory, Open LDAP,some other identity provider, and generally speaking,you have different password policies.Let's talk about some of the elements of a policy.If we do Custom right here, we'll be able to kind of modify this.So let's kind of go through, this is a very common Password policy set.So, characters and generally speaking,today you really want to be at least 10 characters.Often you're going to see 12 to 15 characters.I would say 8 might be considered not stringent enough.Then we have what's called the Password strength.So if you wanted to have the maximum strength,we would require at least one uppercase letter from the Latin alphabet,at least one lowercase letter from the Latin alphabet,require at least one number, and require at least one alphanumeric character.So, often a strict password policy would include all four of thoseand then let's say a 12 to 15-character policy.Then some other requirements down here would bepassword expiration or what is the lifetime of the policy.Generally, this should be no less than 30 days,but often you'll see things like 45 days, 60 days for policies, or 90 days.Anything longer than that is probably not a good idea.Anything shorter is more secure, right?So, that's the expire, and we'll say 30 days.And then the Password expiration requires an administrator reset.So basically, if the password expiresand the user hasn't changed their own password,then the admin is going to reset it.They prevent password reuse.This is what we call the password history.So for example, if you put 12 here, you have to use 12 uniquepasswords, let's say every 30 days.So, that would be 12 over the course of a, you know, annual year,every 30 days and you cannot reuse another password till next year.OK, so that's a good thing to do as well.So on the exam, remember just the different elements of password policy.It's true that more often than not you're moving to passwordless solutions.Maybe in your Microsoft environment or Microsoft Azure,or maybe you're starting to use things like tokensand smart cards and biometrics.Maybe QR codes on your mobile device, we're moving awayfrom passwords, but they're still widely usedand on the exam you should be familiarwith the different elements of a password policy.

#### Federation and Single Sign-on (SSO)

Federation and single sign-on or SSO, federated identity management,also known as federated single sign-on,refers to the formation of a trusted relationshipbetween separate entities and third parties,such as cloud or application vendors or other partnersoffering various services, enabling them to share identitiesand authenticate users across unsecured domains and realms.When two domains are federated, a principal,let's say an end user with an endpoint device, can authenticate to one domainand then access resources in the other domainwithout needing to perform an additional login or credential procedure.Single sign-on allows a user to access multiple applicationsusing a single set of credentials.That's the exact definition of single sign-on.Federation is actually a much more broader concept.It relates more to the trusts between the multiple parties,and we'll see that here in a moment.The SSO feature can be applied to employees, contractors and customers,as well as devices to simplify the login experience.To perform their duties,employees often sign on to multiple business applications including messaging,email, productivity apps, like Microsoft SharePoint, various accounts,for example, a normal user accountand then an elevated administrative account or root account,human resource functions, intranet sites like conferencing,financial records, and more.Here we see the three parties of federated access.What the consumer is doing is single sign-on,but the real federated access part is the relationship of trust and tokensbetween the identity provider or some token service running at acloud provider or a vendor and various services.Services like Microsoft 365, Yammer, Workday, Salesforce,Box and many, many more.And realize when it comes to the identity provider,that could be an on-premise identity provider like Active Directoryor OpenLDAP or some other directory service running on Red Hat Enterprise.Or the the identity provideror token service could be running in the cloud, for example, Azure AD.So, the complex aspect here is step one,where the identity provider or the trust service creates a trust relationshipwith one or more services and service providers.The identity provider has a lot of information about the consumer.However, it only exposes certain information that's necessary and neededto authenticate that consumer to the service or the service provider.If the consumer wants to access something like Microsoft 365or Yammer or Workday or maybe a service running at a cloud provider,it will authenticate with the identity provider either on-premisesor in the cloud and request a token.The provider's token or assertion service will returnthe token or the assertion to the consumer who then presents it to the service.At that point, there is a time-based sessionwhere the consumer is only authorized to do what's in the tokenagainst the service or on the service.Lightweight Directory Access ProtocolLDAP was based on the X.500 directory.In fact, X.500 actually predates the World Wide Web by quite a few years.It's a lighter, cross-platform standards-based solution,the standard being IETF, Internet Engineering Task Force.LDAP servers are easy to install, maintain, and optimize,but they're without solid security of the queries, updates,and valuable information in the LDAP directory.And as you know, to solve that problem we can run LDAP over TLS.So, LDAPS running on port 636.Simple Authentication and Security Layer (SASL) BINDalso offers authentication services using mechanisms like Kerberoswhich is very popular in a Microsoft Active Directory environment,or a client x509 v3 certificate sent with transport layer security.In this diagram, the business application supports the LDAPservice and LDAP directory and various users can be authenticated and authorizedagainst the LDAP directory to get session-based access to the application.Security Assertions Markup Language or SAML is an XML-basedopen-source single sign-on standard.It traces its roots back to when people decided to doauthentication and authorization over the World Wide Webusing the Hypertext Transport Protocolas opposed to some of the other mechanismslike RADIUS, LDAP, TACACS+, and others.SAML is used by many cloud single sign-on connections forthousands of large enterprises, government agencies,and service providers that communicate on the Internet.A key advantage of SAML is its open-source interoperability.Some large companies now require SAML for Internet single sign-onwith Software as a Service applications, and other external ISPs.It's a common federated solution with cloud service providerssuch as Amazon Web Services and Google Cloud Platform.Here we can see SAML 2.0, which is now the de facto standard at AWS.In this example, your organization has its own identity provider.It's an LDAP identity store, let's say, running on Red Hat Enterprise.The browser has an interface, often with a custom portalthat is generated by a service provider like Amazon Web Services.So the user browses requesting authentication from the identity provideror the LDAP store and the store returns as SAML assertion.Now realize the LDAP identity store and AWS have already createda trust relationship by going through several stepsand exchanging trust tokens.So, this has already happened under the hood.Once the user with the browser interface gets the SAML assertion,it will post a SAML assertion to a sign-in URLwhich is going to be at Amazon Web Services to an AWS single sign-on endpoint.Notice that AWS on the backend, they're running an STSor a security token service at somewhere like Microsoft Azurethis would be called an SAS or the Security Assertion Service.In step 6, the SSO endpoint validates, sends redirects,sends it back to the browser interface.The browser can now access the service using the AWS Management Console.And when I say service at AWS, it can be hundreds of different managed servicesat AWS or one of its many marketplace partnersthat represent software as a service offerings.Up until the iPhone came out, SAML 2.0 was the de facto standard.However, the developers of mobile apps on the iPhonewanted to go a different direction.So because of that, OAuth and OIDC came to be.OAuth 2.0 is an open authorization frameworkthat allows a third-party application to get limited access.And that's important.Limited access to an HTTP service.Developers post the iPhone introduction,use OAuth to publish and interact with protected datain a safe and secure manner.Service provider developers can use OAuth to store protected dataand give users secure delegated access.OAuth is designed to work with HTTP and basically allows accesstokens to be issued to third-party clients by an authorizationserver with the approval of the resource owner.Realize that SAML 2.0 and OAuth/OIDC are not mutually exclusive.An organization or an enterprise can use both for different use cases.With OAuth, the third-party then uses the access token to accessthe protected resources offered by the resource server.OpenID Connect is a basic identity layer on top of the OAuth 2.0 protocol.OIDC is actually the service that verifies the end-user identityusing an authorization server.It can get basic profile information about the userwith an interoperable REST API methodology.It supports web-based mobile and JavaScript clients.OpenID is extensible, as functionality is often added.Realize the proper implementation of OAuth is to use OIDCor OpenID Connect as the basic identity and authentication layerrunning on top of OAuth.

#### Access Control Models

In this lesson, we're going to explore various access control models,specifically, the ones you need to know for the security+ exam,starting with mandatory access control or MAC.Mandatory access control is a control policy that is uniformlyenforced across all subjects, in other words, end users and objects,for example, servers within the boundary of an information system.A subject that has been granted access to information is constrainedfrom doing any of the following:one, passing the information to unauthorized subjects or objects,granting its privileges to other subjects.Changing one or more security attributes on subjects, objects,the information system, or system components.Choosing the security attributes to be associatedwith newly-created or modified objects,and changing the rules governing access control.A mandatory access control model uses a strict set of establishedsensitivity levels, otherwise known as classification levels,as well as access controls for integrity and confidentialitybased on the established classifications, most often done by a committee.These are mathematical models used in high-security environmentslike military, government agencies,and enterprises involved with sensitive data and sensitive activities,such as government or military contractors.The state machine, in other words, every possible statefrom startup to shutdown and information flow models are designedby a security team or steering committee,as opposed to an individual administrator, group, or asset owner.The next model is the discretionary access control model, or DAC.The DAC policy is enforced over all entities so that a subjectbeing granted access can pass the information to othersubjects or objects, for example, web browsers running onworkstations or laptops to web servers.The policy can grant its privileges to other subjects.For example, the owner of a directory or a particulardocument can grant permissions to the folder or the document.Change the security attributes on subjects, objects,information systems, or system components if they havethe rights and privileges.Choose the security attributes to be associated with newly-created or revisedobjects, such as read-write, write, read-only, or change.Change the rules governing access control at their discretion.The DAC models involve control and management by the owneror creator of the object, for example, a Word document,an object in a PowerPoint document library, a PDF file stored on a file server,or information in a database or object storage in the cloud.DAC leaves a certain amount of access controlto the discretion of the object's owner or creatoror anyone else who is authorizedto control the object's access, for example, a keyand a key management service at a cloud provider.The DAC model is often considered the opposite of a MACmodel that we talked about in the previous slideand that the owner can determine who should have access rights to an objectand what those rights should be.The most common discretionary access control model would be WindowsNetwork Operating System, for example 2022and possibly using Active Directory.Role-based access control models or RBACis an access control based on user roles, for example,a collection of access authorizations a user receivesbased on an explicit or implicit assumption of a given role or responsibility.It could also be a job title.Role permissions may be inherited through a role hierarchyand typically reflect the permissions needed to perform defined functionswithin an organization.A given role may apply to a single individual or to several individualsand realize that these models are not mutually exclusive.For example, you could have in a DAC modela global database administrators group,but when the users are acting on the SQL databasethey can be subject to a more granular role-based access control,separation of duties or least privilegedprinciples on role-based access to the database.Rule-based access or rules-based is permitted or deniedto resource objects based on a set of rules or ACEs, access control entriesthat are defined by a system, for example a file systemor a network administrator.As with discretionary access control, access properties are storedin access control lists or ACLs associated with each resource object.When a certain group or user account attempts to access a resource,the operating system or cloud-based service or hypervisorwill check the rules contained in the ACL for that object.Examples of rule-based access controls are historicaltime-based ACLs where you can control access based on the IPv4address, the ports or services that are being used,and certain times of day either based on the system clock on the deviceor the network access device or against a centralizednetwork time protocol server.They can also be router infrastructure ACLs often deployedon the customer premises equipment to permit and deny certain addressesand ports going through that edge device.They are static or stateless firewallssuch as early daemons running on Unix and Linux machines and cloud providers,for example Amazon Web Services network ACLs or NACLs.Here we see a simple inbound access rule.Often these are going to be numbered rules starting with 100and the lower numbers are processed first.We can see that we are allowing or denying protocols, port numbers.And remember, protocols can use more than one port numberor they can be dynamic port numbers.The source IP address, which could be anyor an actual specific prefix or address,or it could be some type of token or placeholderthat represents a range of addresses.For example, IT_Admin_IP_Range,then the destination IP address, some type of name or description,and the allow action also referred to as permit or deny.One thing that all access lists have in commonis there's an implied or implicit deny all at the end.Here we see a WebACL that would be used on a web application firewallor a web security gateway where you create access control list rulesfor web traffic, specifically HTTP and HTTPS,either in a virtual server, in your own data center hypervisor environment,or in this example Amazon Web Services up in the cloud.This list of rules can block not just on addressesbut on your geographic location.It can do deep packet inspectionof the HTTP request headers and response headers,typically looking for certain types of attacks like SQL injection,request forgeries, cross-site scripting,buffer overflows, and other attacks.

#### Multi-Factor Authentication (MFA)

Multi-factor Authentication (MFA).Multi-factor authenticationtypically involves adding an additional authenticationmechanism to the initial origin authentication or credential presentation.If it just adds one more factor, we often call this 2FA,two-factor authentication.Officially, multi-factor authentication involves two or moreof the following, something you know, something you have,something you are, or somewhere you are.Let's break these down.Examples of something you know would be a password, a personalidentification number or PIN, a long phrase, or a secret word or secret phrase.Examples of something you have would be hard or softauthentication tokens like YubiKey, Authy or Gemalto,a badge or a smart card.It could be a government or military issued CAC card, X509v3certificates, or you have a security key which could bestored on a FOB or an SD card with micro HSM.Something you are would be a fingerprint, an ocular biometricsuch as retina, facial recognition, and speech patterns or voice recognition.Somewhere you are could be a remote client-based and clientlessvirtual private network, a Remote Software Defined Perimeter (SDP)using a cloud controller, an 802.1x wiredor wireless network, or a cloud identity management IdM-managednetwork using single sign-on or Federated access.In the next lesson, we'll dig deeper into different biometricmechanisms, for example, like fingerprints and ocular scanning.

#### Biometric Authentication

In the previous lesson, I discussed biometrics as something you are.In this lesson, we're going to go a little bit deeper, starting withfingerprint biometrics, which is one of the oldest and most commonbiometrics and they vary from person to person, even withidentical twins and do not change over time.Now there is a caveat there.There could be some injury or some other phenomenonthat causes the fingerprint to be corrupted.It's integrated into mobile devices and laptop computersusing hardware and/or software.Some of the first laptops to use fingerprint biometrics were earlyToshiba laptops.Now a fingerprint scanner has two functions.One, it gets an image of the finger and then secondly,you determine whether the outline of ridges and valleys in the imagematches the patterns in pre-scanned images.Facial recognition.This was one of the fastest growing mechanisms pre-pandemic.It's commonly used to identify or verify an individual in stillor video images.The main applications of face recognition are in areasof security biometrics and human-to-computer interaction(including robotics).The primary method for modeling facial images is called PCAPrincipal Component Analysis.This is simpler, has a high learning capability, and itprocesses vigorous sensitivity to small changes in the face image.An iris scan is a form of optical biometric.The iris is the thin, circular structure or the "color" partof the eye and it controls the diameter and size of the pupilsand therefore the amount of light reaching the retina.Muscles attached to the iris expand or contract the pupilso that the larger the pupil, the more light can enter.Iris scanners use camera technology to get images of theintricate and detailed structures of the iris usingdelicate infrared illumination.Another form of ocular biometric is a retina scan.The retina is the thin tissue composed of neural cells locatedin the back portion of the eye.Due to the complex make-up of the capillaries, every person's retinais distinctive and a retina scan is considered a formof vein-based biometric.The scanner sends a beam of low-energy infrared light into an eyewhen the user looks through the scanner's eyepiece.A beam of light traces a standardized path on the retina and the patternof variations are converted to code and then stored in a database.Retinal scanning is categorized as invasive.In fact, it's more invasive than the iris scan because theeye must be very close to the eyepiece, often for a longer period of time.Voice recognition.Now there is a difference between speaker recognition and speech recognition."Voice recognition" can be used for both terms.Speaker recognition leverages the aural aspects of speechthat diverge among people, and this can have a very benign use,for example, in software that converts speech into text.Traits include human physical structure learned socialcommunication patterns.And voice recognition is classified as a "behavioral biometric"and a non-invasive biometric.Remember on the exam, you need to understand which biometrics are behavioraland which are invasive and non-invasive because itdepends on the environment, environmental factors as to whichbiometric is better than others.Mobile biometrics.This could be fingerprint scanning, facial recognition becoming common,and ocular, voice, and swipe patterns.On the exam, there's a couple of ways to measure biometrics.The false acceptance rate or the FAR measures the probability orlikelihood that the biometric system, for example, a fingerprintscan or a more elaborate palm scan, willincorrectly accept an access effort by an unauthorized user.A system's FAR is often represented as a ratio of thenumber of false acceptances divided by theamount of authentication attempts.The false rejection rate FRR is the likelihood that the system,for example, the iris scan incorrectly rejects access to anauthorized person, due to failing to match the biometric input with a template.The crossover error rate is a value of FAR and FRRthat's plotted and visible when the sensitivity is setup so thatFAR and FRR are the same.This is an excellent metric for quantitative comparisonof differing biometrics.

#### Privileged Access Management Tools

In this lesson, we're going to talk about privileged access management, or PAM.Privileged access management is an identity security initiativethat helps organizations counter cyberthreats through monitoring,detecting, and stopping unauthorized access to critical resources.PAM works as a collaboration of people, processes,and technology to offer visibility into subjects or principalswho are using privileged accounts, and to get visibility intowhat they're doing all the time.System security is enhanced by limiting the number of subjectsor end users that have access to administrative functions.Additional layers of protection mitigate data breaches by threat actors.Here we see the components of PAM, just-in-time permissions.This is a practice where the privilege granted toapplications or systems is limited to predetermined periods of timeon an as-needed basis.This has historically been called time of day permissionsor time of day rights and this could be accomplished early onwith special access control lists.Today, we typically do this with initiatives like PAMand attribute-based access control.Just-in-time permissions minimizes the risk of standingprivileges that attackers can easily exploit.Standing privileges being persistent 24/7 privileges or rights.Password vaulting.This is a program that securely stores credentials for multipleapplications in an encrypted format.Users can access the vault via a single "master" passwordand the vault then presents it for the account they need access to.Ephemeral credentials.These are dynamically generated credentials that are created as neededor ad hoc, then discarded afterwards.Like persistent credentials, these credentials offer the subjecta temporary token or assertion needed to gain access.The three main goals of PAM are to aggregate based on differentusers and different systems and services, to correlate &analyze, and then deliver reports.You've got privileged user data, for example, in MicrosoftSharePoint or in file servers, either in your own data centeror maybe in Microsoft Azure cloud.And you, of course, have asset data, metadata about databases,servers and services, network and security infrastructure,virtual servers, applications, and desktop environments.One of the main goals of PAM is least privilege for systemadministrators and high-level service desk employeesor special third-party service providers,select business users and power users,and to separate thosewith least privileged principles from other types of users.We can get visibility into anomalous behavior,users who have unlimited access, users who are doing it for thefirst time, which could be an attack,malware, and anomalies in various assets,isolating high-risk accounts and getting visibilityinto your high-risk accounts in generating meaningful reports.

#### Course Summary

In this course, you learned about provisioning and deprovisioning,user accounts and password concepts.Federation, single-sign on, and access control models.Multi-factor authentication, biometric authentication, andprivileged access management.Coming up in the next course, we'll explore automation,orchestration, and incident response.

### Automation, Orchestration, & Incident Response

#### Course Overview

Automation, Orchestration, and Incident Response.In this course, we'll examine automation and scripting use cases,benefits, and considerations, explore the incident responseprocess and life cycle, learn about threat hunting and hunt teams,and learn about root cause analysis or problem management,digital forensics, and investigation of data sources.

#### Automation and Scripting Use Cases

Automation versus orchestration.IT automation involves generating a single task to runautomatically without any or little human intervention.Automation could involve sending alerts to a security informationand event management (SIEM) system, dynamically triggering a serverless functionat a cloud provider, for example Azure Functions or AWS Lambda,or adding a record to a databasewhen a batch job is run, just to name a few.Enterprises often automate both cloud-based and on-premises tasks.Orchestration involves managing several or many automated tasks or processes.As opposed to focusing on one task, orchestration combinesor aggregates all of the individual tasks for the entire life cycle.Orchestration occurs with various technologies,applications, containers, and microservices, datasets,middleware systems, and more.Let's look at some use cases for automation and its predecessor, scripting,user and resource provisioning.Most modern enterprises have tightly integratedJoiner and Mover onboarding and provisioning processesthat involve automation and integrationbetween the human resources department, legal, the directory services,identity management, and inventory engines,inventory of both physical and virtual assets. Guard Rails.Cloud providers use JSON JavaScript Object Notation policiesand Infrastructure as Code IaC to enforceleast privilege policies and separation of duties to remove the abilityto make certain API calls from privileged groups and users.For example, at Amazon Web Services, you can use new languages likeCedar C-E-D-A-R to create custom API callsto your applications and services to consumer-constructedapplications and services.Creating guard rails is just one use case.Security group firewalls.These firewalls are layer 3 network layer and layer 4transport layer stateful or static packet filters applied to either subnetsor virtual instances in hypervisors or in the cloud.You can use automation for creating tickets and for elevating or escalatingas part of a Service Desk deploymentrunning scripts and automated workflows.This can be a huge enabler for software-defined networks,software-defined local area networks, and software-defined wideand metropolitan area networks.In modern LANs and datacenters,enabling or disabling services and access controls is a commonfunction of scripts, automation, and Infrastructure as Code.For example with JSON or YAML files.As part of the DevOps life cycle,for example, if you're using Agile, CI/CDcontinuous integration and continuous deployment,automation is leveraged for continuous integration and testing.Any API call or request can be automated.You can use Python, Postman, or something else in orderto run tests to help quality assurancecontinuously check a product's quality.Generate light orchestrations that involve several API callsto perform a particular task on a microservice backend.Use preformed snippets to run Functions as a Service in the cloud,for example, Google Cloud Functions.In the next lesson, we'll break down 8 benefits of automation.

#### Benefits of Automation

In this lesson, we're going to look at the benefits of automation,and this is a good opportunity for you to maybe take some notesand get interactive.So if you want to pause and jot some things down,that should help you in your exam preparation.So first off, we have efficiency and productivity.Automated processes can finish tasks and processes fasterand more effectively than a manual process.Automation tends to increase productivity levels,and when you're working with the manual workforce,you'll discover that the production rate will vastly increase.Another big advantage of automation is the capacity to save time.By automating manual tasks, businesses can remove or greatlyreduce the requirement for repetitive and time-consuming tasks.Of course, in our context, we're thinking about security tasks,for example, dealing administratively with things like layer 2 switchesand routers and other infrastructure devicesthat have a lot of manual tasks and configuration.This is going to let your security force focus on more criticaland strategic actions in the security operations center.A popular approach to security is enforcing baselines.These are often standardized security images,and we can get these from organizations like CIS,the Center for Internet Security, or from vendorswho are government and military contractors.Traditionally, systems administrators use a combination of scriptsand manual processes to set up their infrastructure environments.Let's say, for example, your wireless local area network.Well, automation supports Infrastructure as Code.They go hand in hand and this can be utilized for new physicaland virtual network and data center environments or creating those baselines.It's also widely used for software development to build,test, and deploy applications.Creating secure, scalable network automationfor example, using JSON and or YAML in the cloud enhances security,calibration, supports versioning control,documentation, error handling, testing, contributes to compliance,provides better visibility, enhances collaboration,and contributes overall to your ongoing continual maintenance.What we call due care, providing a more resilient environmentas you scale up to more robust processors and more RAM,the higher-end platforms, or as you scale out,for example, auto-scaling out virtual machines at Microsoft Azure.Automation can free up employees from everyday mundane processes,providing them with more time and energy to engage in more creativeand meaningful tasks and projects.It also helps prevent critical errors that might jeopardizea job performance review.It also helps increase productivity, employee morale,and overall job satisfaction.Quick reaction times increase safety, operational performance,and for example, incident response security with fine-tunedSOAR playbooks and runbooks.And then finally, automation is a force multiplier.A force multiplier is actually a military term.But for our organizations, there would be tools, automation, and orchestrationthat help amplify efforts to produce more output.A simple example of a force multiplier would be, let's say,if you're roofing your home and using a nail gun as opposed to a manual hammer.Investing in force multipliers means that you'll accomplish morewith the same or less effort.So, automation can enhance things like patch management,backup and restore policies, compliance scanning,and of course, software development life cycles.

#### Automation Considerations

In this next brief lesson, we'll look at automation considerations.Developers can reduce the complexity of automationby using established toolsets, kits, and integrated solutions.For example, between the on-premise and the cloud,in a hybrid or edge computing environment.Automation and scripting can reduce costsfor provisioning/onboarding users and devices reducing human interaction andpotential configuration errors and further troubleshooting.In this context, a single point of failure (SPOF) is a flaw in the design,configuration, or implementation of the automation solution.In other words, all the problems that automation solves,you'll lose the benefit if the automation system or toolis a single point of failure and not highly available or resilient.If the automation solution is not redundant and reliable,then one loses the overall benefits of automation.Automation systems can also be a technical debtif implemented in a rush or without proper testingor even understanding completely how the automationand orchestration system works.Ongoing supportability of automation and orchestration is another key factor.That is a cost, an ongoing outlay that the organization must consider.We call that operational maintenance or due care.In the next lesson, we'll look at a process that is commonly automated,at least partially, and that is incident response.

#### Incident Response Process

In this lesson, we're going to explore the incident response processand realize that incident responseis actually an information technology practice,otherwise known as incident response management.Early responders to the incident must quickly classify the negative eventto determine the actions to be taken.Hopefully predetermined actions.Is it a benign event, for example, a false positive?Or is it an actual incident or a negative occurrencethat can be done by a personsince incident response is an art and a scienceand we get better with experience?Or it could be done automatically or in an automatedfashion, for example, with a SOAR runbook or playbook.What is the immediate impact on operations?How severe is it?In other words, an incident could easily escalate to disaster recovery.What is the scope of the impact?Is it just the CEO's workstation or laptop?Is it a single mobile device that is lost or stolen?Or is it a server farm or a datacenter or a DDoS attackon an entire call center or local area network?Is the scope on one floor, multiple floors, or multiple buildings?How prioritized or critical is the target?This can be done qualitatively,for an example 1 to 5 or 1 to 10, or quantitatively.And then as a bonus,can the root cause of the incident be performed quickly and easily?The reason why I say that's a bonusis because that's not the goal of incident response.Root cause analysisis actually a different practice we refer to as problem management.If based on experience, the first responder can determinethe root cause, that's always a good thing.And again, does the incident trigger some type of escalation or elevationor does it trigger disaster recovery?The main goals of incident response are to reduce the immediate impact,prevent the spread to other systems, other VLANs,other zones, to protect and maintain ongoing operationsthrough due care and improving the incident response processand learning from incidents to prevent the recurrence.To support forensic investigations, an aspect of e-discovery,and continuity of operationsotherwise known as business continuity and to provide information.Remember, data becomes information, information becomes knowledge,and knowledge becomes wisdom.So, after-action reporting and a lessons learned database being populated.Next we're going to look at the incident response life cycle.And one thing I want to tell you about life cycles on the Security+ (Plus) examis by no means do you have to memorizethe various steps of life cycles.You won't get a question, for example, what is phase threeor step three of the incident response life cycle.But to understand life cycles, there's some fundamental common elements.So for example, the first stage or the first phase is always preparationor information gathering or reconnaissance or planning, correct?And then the final phaseis the lessons learned phase or the disposition of the life cycle.Also remember that although the steps are typically linear,they can happen simultaneously.So for example, analysis and containmentor detection and analysis could happen simultaneously.Also, sometimes you don't have to even have one of the steps,like you may not even contain.You may go from right from escalation to eradication or recovery.Also, realize that recovery or the phase right before the lessons learnedor the life cycle is wrapped up can be a very long persistent phase.In other words, your system or application or servicecan be in a state of containment and recovery for a long period of timebefore you reached that recovery point, which is basicallywhat was the state of the system or the application or servicebefore the incident ever happened.Let's look at the phases, detection, also referred to as identification.First responders or possibly your SIEM and source system mustseparate an event, a benign event from an incident or a breach immediatelyand preferably using pre-definedmetrics combined with your experience and automated if possible.Responders will implement techniques,pre-defined and pre-tested techniquesfor categorizing and prioritizing the incidentbased on an established risk registerfor a small to medium-sized business or for larger organizations,a SOAR runbook.In this first phase, how are you alerted?And by the way, you can be alerted by several different sources.It could be a log, for example syslog, or an SNMP trap,or an alert from an intrusion prevention system or a firewall.It could be a feed, for example from a cloud service.You could get a phone call to the security operations center,manual automated text messages, logical and physical alarms.Or you're notified because you're doing real-timeinteractive monitoring in a security operations centerand it shows up on the dashboard.Next is analysis and escalation.The analysis of incidents is a combination of an art and a science.And I mentioned this earlier.There are some important questions to answerin this phase or combination of phases.What is the scope of the incident?Does it qualify for escalation or disaster recovery?Are there obvious artifacts or indicators of compromise?Elements of the kill chain that are obviousor observable on systems or servers?Can you discover the actions in the kill chain based on your experience,or by doing threat modeling in a sandbox or a hypervisor environment,or threat modeling in the cloud?We'll explore the kill chain in an upcoming lesson.And if possible, can you quickly identify the root cause?And remember, that's not the main goal of incident response.Escalation or elevation.Basically, this is a pre-established workflowfor escalating the incident to a higher service desk tierand that must be establishedespecially in a medium-sized or larger organization.If you're a medium-sized to large enterprise and you don't have a service desk,you wouldn't be considered a mature organization,for example, on the capability maturity model.Does the incident need to be passed from the first respondersto an incident response team IRT?Realize you may not have a separate first responderand a separate incident response team.Many organizations have a security operations center,often off-site, and a service deskalong with an emergency Change Advisory Board.Containment.This is implementing short-term processessuch as disconnecting devices from the network.Malware can be quarantined by antivirus programs and security suites.You might want to leverage quarantine compartments, sandbox locations,detonation chambers, private cloud deployments, andthreat modeling environments often running in the cloud.Managed security service providersMSSPs can help maintain separation, containment, and segregation.For example, Fortinet, Fortigate, Cisco, Juniper, and Palo Alto Networks.Eradication. Potentially unwanted programs(PUPs) can be eradicated by advanced antivirus and antimalware suitesrunning on-premises or in the cloud or in a hybrid cloud environment.Some artifacts may need to be moved to detonation chambers,for example in a type 1 hypervisor for further analysisand running machine learning algorithms.All the findings should be reported to cloud partnersand added to vulnerability repositories and shared reputation databases,for example, the National Vulnerability Database.Advanced wiping tools may be neededto completely remove all malware footprints and artifact remnants.Incident response recovery and lessons learned.Recovery involves getting back to an acceptable stateto continue to deliver the value proposition.That's a key issue.An acceptable state.Full recovery would be getting back to a recovery pointas if the incident never happened.But sometimes an organization has to function in a perpetual state of recovery.Complete remediation may not be possible even for an extended period.For example, the damage done by a ransomware attackor a fire that breaks out in the data center.After-action reports will be generatedafter the exercises and after the actual incidents.And there should be a Lessons Learned database.The success of redress and recovery depends upon the level of testingand exercise performed.For example, one of your main countermeasures or protectionsfrom ransomware attacks is a backup and recovery policy.You may be doing automated backups on a daily or weekly basis,but if you never test the recovery or restoration,you will not be successful in recovering from the ransomware attack.Notice here on bullet point 3 againwhere I mentioned after-action reports will be generatedafter exercises and actual incidents.In the next lesson, we'll break down the different types of testsand exercises you will do not just for incident responsebut also for your disaster recovery planning.Because remember, an incident can quickly escalateto a disaster or catastrophe.

#### Training and Testing Incident Response (IR)

Well, obviously the members of your incident response teamdon't have this knowledge or skill set.They weren't born with it.They had to learn it from somewhere.So let's talk about training your IRT.A prime example of incident response trainingwould be the Cybersecurity Infrastructure Security Agency (CISA).This is an important entity that helps organizations across the nationprotect their IT enterprises and enable their cybersecurity talent.The CISA, among other organizations,offer incident response training courses free to the government employeesand contractors across federal, state, local, tribal,and territorial government, educationaland critical infrastructure partners, as well as the general public.To test the incident response, either based on your trainingor actual experience, you'll initially have a plan review or a read-through.This could be a group discussion, plan auditing.It could be a Delphi session starting with anonymous surveysand questionnaires or brainstorming sessionswith all the applicable stakeholders.This is often called a checklist, and it's one of the earliest tests.You can build on that with a tabletop test.Here you're going to examine documented plans, diagrams, andlogical and virtual walkthroughs to eliminate gaps and errors.Your first initial drill or exercise that's actually a realworld experience would be a walkthrough.This is a planned rehearsal and/or drill.However, it's limited.It's performed in stages, maybe by department or building only.It should have no impact whatsoever on delivering the value proposition.You're looking for additional gaps, other areas that you can strengthenthe incident response plan.A simulation is a more elaborate walkthrough.It focuses on specific scenarios and areas.For example, active shooter, chemical leak, ransomware attackin your data center, fire or flooding.You're going to use your real business continuity planand disaster recovery plan and your recovery sites.If you don't have a full blown incident response team,you'll go through simulations with your swarm team.This is a more elaborate test than the walkthrough,and it will actually have some impact on delivering the value proposition.You'll test your snapshot recoveries in hot spares,for example, in the cloud or your hybrid cloud scenario.This may be the highest-level test that most organizations conductbecause it will have an impact on delivering the value proposition,an impact on the productivity of your employees.It'll affect your customers, your vendors, and your partners to some degree.A parallel test is a real-world drill while still operating your businessand this is a more elaborate test than a simulation.It's more resource-intensive.In fact, many organizations will bring in what are called crisis actorsto replace the employees, and then they'll video or they'll film itand then their employees as part of their trainingwill go back and they'll watch the crisis actorsgo through, for example, the active shooter drill.A parallel test will definitely take advantage of the recovery sites,warm site, hot site, mobile site, and cloud site.And the full interruption test is the most elaborate.This is a real-world drill where you'll actually cease conductingany business or operational activities.So basically, the organization just goes darkand you're going to go to the cold site or the warm siteor the hot or the cloud site,fully interrupting the delivery of your value proposition.This is often cost-prohibitive for most commercial organizations.Perhaps only a very well-funded company like a Google or Amazon could do this.Or maybe an organization has lots of taxpayer dollarslike a government agency or a military site.But this would be the other extreme to just the checklistor the read-through plan that you begin with.

#### Threat Hunting and Root Cause Analysis

All right, in this lesson, we're going to look at root cause analysis.Root cause analysis (RCA)is a function of the Problem Management IT service practice.A root cause is defined as a factor that introduced a non-conformancein an application, a service, or a system.It is the core causative issue, the highest-level triggerthat sets in motion the entire cause-and-effect reactionthat ultimately leads to the problem or problems.RCA is defined as a collective term that describes a wide rangeof approaches, tools, and techniques used to uncover the cause of problems.Let's look at the steps of root cause analysis.First, we define the problem.Often this is obvious, for example, in the early phase of incident responsewhere we're notified through an alert or an alarm or a log, an API call,a function, a phone call or something that shows upon the dashboard in our security operations center.Hopefully, we've already categorized the mostlikely types of problems and potential issues.If not, step 2 will take longer and involve more collection of data.The more meaningful data and indicators that we collect or gatherin step 3, the easier it is to identify the possible causal factors.Now remember, root cause analysis or problem managementis also a combination of an art and a science.So often based on the experience of the first responderor the incident response team,they will quickly be able to identify the causal factor.If not, it may involve an elaborate forensic investigation or e-discovery.In step 4, we successfully identify the root cause or causesand then, of course, in step 5, based on our after-action reporting,we'll recommend and implementsolutions to reduce the likelihood of the problem occurring again.Threat hunting is also referred to as a "Hunt Team".Threat hunting involves groups of skilled, experienced,and well- trained cyber investigatorswho are aggressively seeking out threats on a network or a system.They're often auditors for compliance or regulations.They may be a pen testing team sent from your insurance company.They attempt to quickly recognize anomalies and discover historicpatterns in data, files, and Indicators of Compromise, for example,cyber artifacts to counter cybercriminals and mitigate threats.Threat hunting can also be part of a Red Team vs. Blue Team exercisewhere Red Team are the attackers and the Blue Team are the defenders.Experts in threat hunting or cyber teamswill have a solid understanding of the cyber kill chain.Let's officially define a cyber kill chain.A kill chain is the succession of steps and/or phasesused during a structured external or internal cyberattack.Kill chains are also used and analyzed by penetration testersand threat hunting teams to better understand advanced persistentthreats from exploits and malware attacks.The original kill chain was developed by Lockheed-Martin,and we'll look at that here in a moment.But first, let's define an advanced persistent threat,which is a generic phrase for exploits that use a kill chain.A means it's targeted, it's coordinated, it's planned ahead of time,it's purposeful, often with cost-benefit analysis and other analyticsbefore the advanced persistent threat is launched, it's persistent.In other words, it's repetitive.It's iterative.If not successful in the early phases or, for example, from a long distance,the attacker will attempt to get closer and closer to the target.It also involves remnants where the malware may be able to survivea reboot of a system or hide from antivirus or anti-malware tools.And T is the threat the person or persons which we refer to as threat actorsor threat agents that have the intent, opportunity, and capability.Here we see the 7-step Lockheed-Martin cyber kill chain.The first phase is reconnaissance or information gathering.If the attacker or the pen testers know nothing about your organizationor your target, this phase can take a long period of time, days, weeks,even months, and the various reconnaissance toolsare actually considered attacks.For example, fingerprinting, evaluating the posture of systems,delineating the operating systems and builds and versions,weaponization, choosing the modality or the malicious code or the exploit.Delivery is the vector through email, spam, malvertisements,email attachments, phishing attacks, business email compromise,putting code on USB fobs and leaving them in the parking lot or in the foyer,penetrating firewall systems or internal compromised users,introducing rootkits or remote access Trojanson systems and services, to name a few.Then the actual exploitation taking advantageof the weakness or the vulnerability in the application, service, or system.Installation of the malware, often involving lateral movementor escalation or elevation on the server or the workstation.Often the malware is a remote access Trojan, so it communicates backeither to the command and control server as part of a DDoS or a botnetor an intermediate bot in the stealthy polymorphic advanced persistent threat.And then typically exfiltration of data is the primary goal.

#### Digital Forensics

Now we've established the factthat forensics is part of root cause analysis or problem management.So why did we perform digital forensics or cyber forensics?Laws have been violated, organizational policies have been violated,systems have been attacked, either externally or internally.Data and identity have been breached, intellectual propertieshave been exfiltrated or stolen, privileged insiders are suspected of crimes,or it's simply the next incident response phase,as mentioned, root cause analysis or problem management.Cyber forensics is a part of e-discovery.It's a main category.E-discovery is an innovative technology that has emerged over the last decadeor longer to lower the risks and costs associated with big data,especially in litigation and internal corporate and government investigations.The e-discovery process includes four phases,identifying and collecting documents or artifacts,sorting through the data by relevance,creating production sets, and data management.The cyber forensics process involves 1. identification of the crime,2. collection of evidence, 3. examination of the evidence,4. analysis of the evidenceand finally reporting on the findings of the analysis.As mentioned previously in this course, when talking about phases,for example, the incident response life cycle, sometimesthese phases or stages can happen simultaneously.For example, the collection and examination of evidencemay happen in a single phase or stage.When collecting and handling evidence in a forensic investigation,you want to employ forensic kits and laptops.Typically a very powerful Linux laptop and something like FTK Imager.Collect network traffic in various logs, for example using your SIEM system.Capture and hash system images and memory dumps.Make sure you're using a write-blockerso you leave no fingerprint on the target device.You'll document the timeline of event sequences.You need that for billing and accounting and maintaining the chain of custody.Record time offsets, create screenshots,or even use digital cameras, or conduct interviews or depositions.When gathering evidence on the exam,you want to remember the order of volatility.The most volatile is the CPU, its registers, and any caches or buffers.Next you have kernel statistics, tables, and caches of workstations,operating systems, specialty devices like controllers, infrastructure devices,and server operating systems.Next is volatile RAM memory.Then temporary file systems and swapor slack space would be on a hard disk drive or a solid-state drive.Now remember at this point during the incident response,if you decide to turn off the system or shut down the systemthat's been attacked, you will not have any of these four things.They're extremely volatile and they only exist when the system is running.So, otherwise, you will be able to get data, data at rest off of disks drivesand volumes, RAID arrays, attached removable drives like USB and FireWire,or directly attached storage.Log data to a remote location.For example, a cluster of syslog servers or SIEM and SOAR system.Next, in the order of volatility would be copies of data that are part ofarchived or backed-up media or moved to the cloud.In processing forensic evidence, you want to detect encrypted filesand encrypted volumes and then attempt to decrypt them.You'll discover compressed files and folders,perform validation and pattern matching, often using your forensic kit,such as EnCase by Guidance Software,leverage regular expressions in metacharacters,filter for suspected user data.For example, if you suspect an internal user of using their corporate emailin an unauthorized way, you want to filterjust for the security identifier of that particular user.In a Microsoft system, we would call that a SID.You want to perform discovery of hidden data stored in slack space and again,extract only meaningful data, data that has utilityin the forensic process, and conduct traces and calibrated estimatesto determine your suspect or suspects.It's critical in a forensic investigation to maintain the chain of custody.This follows the evidence throughout the entire life cycleuntil the possible court date or deposition.It involves strict procedures for collecting,handling, and tagging or labeling the evidence.You want to provide a history and timeline of the evidence handling,maintaining the integrity of the evidence.Therefore, any image you create, file, volume, disk image,or memory dump must be cryptographically hashed,preferably with SHA-2 or higher.You want to provide accountability, prohibit tampering,and make sure you anticipate any admissibility issuessuch as a legal hold on that evidence.Let's define a legal hold.A legal hold refers to a process which an organization uses to retain all formsof pertinent data and information when it reasonably expectssome type of litigation against it, or some need for future utilityin a court of law, administrative, criminal, civil.It can be a restriction placed on a database or a set of records,electronic or physical, because of existing or anticipated litigation,an audit, a government investigation, for example, in the USby the Justice Department or the SEC, or other such activitythat suspends the regular usage, processing, or disposition of the data.Chain of custody involves different types of labels.You could use special bagsthat have areas to mark information or document information on the bag itself.It could be a sticker that you attach to a box.You could also correspond to information that you put intoa software application, designating the form of registered mailor certified mail, maybe using a delivery service, date and time stamp,who released it, who received it, and a description or reason.Like any activity, you want to have after-action reporting.Reports should have as much information as necessary but not a "data overload".You may need to express things in simpler termsor have a different report for a different target audience.So, for example, for your forensic team,it may be a more elaborate technical report,but for the C-Suite or the C-team members may be a more simpler report.Make sure you provide electronic and physicaldocumentation of all of your findings.Meet with the proper authorities, local, regional, state,federal law enforcement, and possibly prepare to offer expert testimonyand provide any needed clarification for all parties involved.Identify the overall impact on businessand then recommend any countermeasures and, of course, the result.Who, what, when, and how?Who had the motivation, opportunity, and means?This is important for the court or other proceedings.

#### Investigation of Data Sources

In this final lesson of this course, we're going to look at investigatingthe source of data, starting with various logs.Firewall logs can provide traffic data in layer 2 framesup to deep packet inspection, for example, of HTTP, FTP, DNS,and SMTP traffic, using various outputs to syslog servers, SIEM systemsand Netflow collectors, application logs for email, web, Microsoft SharePoint,file servers, directory services, database servers, and more.There's endpoint logs, such as logs created by Palo Alto Cortex XDRor logs stored in a cloud provider in an object bucketfrom various managed services.For example, configuration logs and logs showing API activity,operating system-specific security log for Windows,Unix, Linux, macOS, and Solaris.Intrusion prevention system and intrusion detection systems logs,alerts, dumps, traps, and informs, and network logsfrom infrastructure devices, security appliances,data activity monitors, and more.One of the most common services historicallyis the Simple Network Management Protocol traps and informs.An inform just means when it gets sent,the sender gets a notification it was received.Version 3 being the most popular.There's also NetFlow collections.Version 5 is traditional with metadata of the layer 3 and layer 4IP headers and version 9 which is extensible,but you can create your own tags in metadata.Syslog trap messages, SIEM system events, SOAR analysis output,cloud-based machine learning and AI visibility and analysis,IPS sensor dumps and alerts, and endpoint detection and response EDR logs.For example, Palo Alto Traps.Let's talk more about a SIEM system.This is a combination of security information management(SIM) and security event management (SEM).The main goals are to centralize the storage and analysis of logsand other security-related documentation to perform near real-time analysis,to send filtered data to data mining, Google big query,AWS Redshift data warehousing servers in a data center or at a CSP,and to allow security and network professionalsto take countermeasures, perform rapid defensive actions,and conduct incident handling.One of the main goals of a SIEM system is to collect, aggregate,correlate, and deduplicate information from a wide variety of devices.Then you can use the SIEM system, for example,Cisco's Splunk running on Windows Server 2022 to perform log analysis,contribute to forensics, IT compliance, monitoring applications,auditing object access, automating your real-time alerts, monitoringuser activity, synchronizing your time,generating reports, conducting FIM, file integrity monitoring,and even storing them on the SIEM systemor a database attached to the SIEM system.One of the most common outputs from a SIEM system is a security orchestration,automation, and response system.SOAR is an assortment of software services and tools, for example,cloud-based Azure Sentinel, Splunk, Chronicle SOAR(part of the Google Cloud umbrella).SOAR allows organizations to simplify and aggregate security operationsin three core areas, threat and vulnerability management, incident response,and security operations automation.You can deploy SOAR to do 1,2, or all of these.Security automation involves performing security-related taskswithout the need for human intervention.It includes defensive detection, response, and remediation,or offensive vulnerability assessment and penetration testing.SIEM uses cases, categories, and rules that are mapped to incident categories.Then these categories get sent to be handled by three types of SOAR playbooks.Manual playbooks, which is a series of manual tasks.Semi-automated playbooks, which is a hybridof automated and manual subtasks, and fully-automated playbooks,completely automated.There's four types of automation.Defensive automation.Basically, anything that tries to prevent the threat or a risk.In other words, a countermeasure.Forensic automation, anything that tries to retrieve additionalevidence or a later phase of incident response.Offensive automation, anything pro-active that tries to investigatean asset or counterattack an attacker or a command and control server.Or deception automation, anything that retrieves or adjusts deception tools,often automating sending traffic to a honeynet.Then there's three different categories of SOAR action enrichment(adding additional configuration management,database (CMDB), or environmental data).Escalation, escalating through email, ticket escalation, service desk,Simple Notification Services running in the cloud, or chat or messagingcommunication. And mitigation, actually modifying the device configuration.

#### Course Summary

In this course, you learned about automation and scripting use cases,benefits, and considerations, incident response life cycle and threat hunting,and root cause analysis, digital forensics,and the investigation of data sources.Coming up in the next lesson, we'll explore effective security governance.

### Effective Security Governance

#### Course Overview

Effective security governance.In this course, we’ll define security governance and governance structures;examine roles and responsibilities;learn about considerations of external governance; and explore securitygovernance guidance, best practices, standards, policies, and procedures.

#### Security Governance Defined

In the first lesson of this course, let’s define security governance.A security practitioner must align all security functions toa business’s strategy, value proposition,charters, goals, mission, and published objectives.This alignment must permeate through all organizational processes,including global governance, the steering committeecharters, and corporate initiatives, to name a few.Realize, however, as a security practitioner,our concern is with these security and privacy aspects of governance:decisions of steering committees or C-suites or C-teams,charters, missions, and objectives.Security strategists must accountfor any major changes to organizational operations or activity.More than ever, there must bea synergistic relationship between security managersand practitioners and the value proposition.The need for governance existsanytime a group of people come together to accomplish an end.Security governance typically focuses on three attributes or characteristics:authority, decision-making, and accountability.It’s focused on the structure and processes for sounddecision-making, accountability, management,and conduct as it relates to security andprivacy at the top of an organization.Security governance directs how anorganization's security objectives are determined and achieved,how risk is controlled and addressed, and how the delivery of value is improved.Security governance is broadly defined as the rules thatprotect the assets and continuity of an organization.It includes mission statements, charters, declarations of value propositions,security policies, standards, processes, and standard operating procedures.Security governance guides the course and control of organizational securityoperations, initiatives, and activities.And optimally, the security practitioner’s strategyand tactics will be derived from effective security governance.Let’s look at some of the activities of security governance.For the small to medium-sized business,at the very least, creating a risk register or ledger starting out based ona template or a spreadsheet, and then moving to a database;aligning security strategy with the overall organizational goals;publishing all compliance and regulatory requirements;performing a vital role in risk assessment and risk management,for example, offering guidance into acquiring security controls to reduce risk;tracking, recording all compliance and remediation initiatives,for example, HIPAA, Sarbanes-Oxley, GDPR, or PCI DSS;documenting stakeholder interactions and reporting related workflows.

#### Types of Governance Structures

On the Security+ exam, you want to be able to compare centralized governancestructures with decentralized governance structures.If they're centralized, the higher positions of management such as executives,president, vice presidents, and/or the C-suite, which would be the CEO, CFO,CIO, hold the decision-making authority.C-suites are also called the C-team.It relies heavily on top-down decision-making.With decentralized, management distributesthe decision-making authority throughout the organization.You see that more often in smaller tomedium-sized businesses that are highly projectized.With decentralized, decisions are made closerto the source of action and information.And as mentioned, these are used in flatter, more projectized organizations.Security board governance refers to a security frameworkor architecture that provides structure toa group of decision-making stakeholders,otherwise known as The Board or The Board of Directors, and how it functions.Board governance defines the roles and responsibilities of board membersand executives in the form of a working board, governing board,or often referred to as an advisory board.Board of Directors or BOD.A board of directors is the governing body of an organizationor company, whose members are elected by shareholders.That's in the case of public companies.The duties include setting strategy,overseeing executive management like the C-team,and protecting the interests of stakeholderssuch as shareholders, bondholders, and others.Remember, on the exam,every public company must have a board of directors.A steering committee is a group of key organizational stakeholders that makesdeterminations regarding an organization’spriorities or their order of business.In other words, their value proposition, service, and/or product,and it manages its operations at general council, in other words, legal.The goal of steering committees are to overseeand support a project from the management level.The Information Security Committee exists to offerrecommendations to executive managementand team leads regarding security effortsthat are undertaken or new initiatives.The committee may also coordinate and communicatethe direction, current state, and ongoing oversight,and continual improvement of information security initiatives and programs.For example, an enterprise mobility management initiative,cloud computing adoption or migrating,deploying a Wireless WPA3, or a Zero Trust initiative.Common responsibilities of security steering committees would be to frame,review, and recommend information security policies;evaluate the effectiveness of implemented policies;offer clear guidance and management support,for example, resources for security initiatives;and to ensure that security activities are executed in compliance with policyor what we would call security or global governance;initiate security awareness and training programs;identify and recommend non-compliance responses, for example,non-compliance to GDPR; approved methodologies andprocesses or programs for information security;and identify significant threat changes and vulnerabilities.Some relevant government agencies,specifically from the standpoint of the United States,would be the Cybersecurity and Infrastructure Security Agency (CISA).This leads the national effort to understand, manage,and reduce risk to our cyber and physical infrastructure.For example, a security steering committee could relyheavily on information and guidance from the CISA.United States Customs and Border Protection (CBP).The CBP has the mission to keep terrorists and their weapons out of the U.S.,along with securing trade and travel while enforcing regulations,and including immigration and drug laws.Office of Homeland Security Situational Awareness (OSA).This group provides operations coordination, information sharing,situational awareness, common operating picture or posture, and executesthe Secretary’s responsibilities across the homeland security enterprise.And the Office of Intelligence and Analysis (I&A).The I&A assists the Homeland Security Enterprise with the timely intelligenceand information necessary to keep the homeland safe, secure, and resilient.Now, keep in mind, although these entities and agencies are specific tothe United States, most first-world countries will have similar agencies withjust different names but the same types of responsibilities and roles.

#### Security Governance Roles and Responsibilities

In this lesson, let's talk more about roles and responsibilities.Security initiatives require a broad awarenessof organizational roles and responsibilities.Especially if you’re using a Role-Based Access Control model.Companies are organized in different ways,such as top-down, flat, or highly outsourced.Directory services are often and closely alignedand mapped to organizational duties and job titles.And as mentioned, roles and responsibilities will often directly affect accesscontrol methodologies as well as sensitivity or classification levels,for instance, with mandatory access architectures.First, we have owners, and the owner role could bea owner of a physical and/or virtual asset like data.With Role-based Access Control,access decisions typically rely on organizational charts, roles,responsibilities, or locations in a user database.The role is often set based on evaluating the essential objectivesand architecture of the enterprise,aligned with the subject's job title and responsibilities.Physical and logical asset owners may: determine the classification level;they may conduct labeling and tagging;they may grant additional shares and rights, for example,in a discretionary or Attribute-Based Access Control model.Next, we have custodians.Custodians are also referred to as “controllers”.They should maintain the assets from a technical and operational perspective.Custodians often interact directly with ownerstakeholders and possibly answer to executive managers or the C-suite.Custodians are often responsible for ensuring confidentiality, integrity,authenticity, availability, and non-repudiation of assets.Stewards have a different role.They manage assets from more of a business or organizational perspective.When dealing with assets, they often interfacewith other departments such as legal, human resources,the mobile application managers or digital asset managers.Stewards are more likely to deal directly with internaland external customers and stakeholders.They often ensure compliance, for example,standards and various administrative controls, and possibly data quality.Next, we have officers.The buck should stop here,although it’s not uncommon that the custodians/controllers will takethe hit when goals are not accomplished or, for example,due diligence or due care has not been implemented.The C-suite or the C-team would include the Chief Executive Officer,the Chief Information Officer, Chief Information Security Officer,Chief Privacy Officer, Chief Technology Officer,and other new forms of chief officers, for example, Chief People Officer.These should be totally responsible for due diligenceand adherence to security governance.Officers often answer to steering committees and various boards,such as the Board of Directors.One thing that's helpful when determining rolesand responsibilities is what's referred to as a RACI chart.Now I call this a RA'K'I chart, but that’s just me,because the C is a hard C for Consulted.But project managers often use the term 'RACI'.Notice there’s four different roles here-responsible, accountable, consulted, and informed.In this use case we’re looking at, we’re going to do a cloud migration.Let’s say, we’re going to migrate our Oracle database up toAmazon Web Services’ Relational Database Service for Oracle.This is a streamlined initiative; there's only five tasks.But, when doing the RACI chart, realize that one and only one party—which in this case we have four parties: the governance,risk and compliance, GRC department; the legal department;the security team; and IT operations—one and only one party or element or group is accountable.So for each one of these five tasks, we have one and only one accountable party.Now responsible- you will have at least one responsible party,but you can have more than one responsible party.So for example, take a look at ‘Build the architecture’, the fourth row.In building the architecture, both the securityteam and the IT operations team are responsible.So you have at least one.But notice that only the security team is ultimately accountable.‘Consulted’ infers there’s some type of two-way relationship;or when you consult the subject,you expect some feedback or some answer or some approval.So remember, ‘Consulted’ is a two-way communication.For example, here, when you establish the provider requirements,you're going to consult the legal department and you're going to consultthe security team, and you expect feedback.“I” or “Informed” is simply a notification.So, when you inform someone it could be, let's say a blind copy in an email;it could be just some notification service in the cloud,when you run a Function as a Service,or a notification in Microsoft SharePoint to an interested or informed party.

#### External Governance Considerations

In this short lesson, we'll touch on external governance considerations.Despite the size of your business or your organization,or the industry or business sector that you're in,every organization must adhere to specific lawsand regulations at various levels.Regulatory compliance, which isan external governance consideration to remember on the exam,describes the actions an organization takes to comply with those rulesand policies, directives, and mandates that relate to its operations.When it comes to data, there are rulesfor handling sensitive information.To be in regulatory compliance,organizations set up internal processes to keep data safe and secure-otherwise, they may be fined, sued, or even face criminal prosecution.Some laws and regulations may be driven bythe applicable business sector or industry.For example, HIPAA and HITECH- HITECH beingthe most recent aspect of HIPAA-from 2009, the Health Information Technology for Economicand Clinical Health Act, Sarbanes-Oxley, or PCI DSS for payment cards.The scope can be global; international, involving treaties; national;it could be state/provincial or a parish or some other entity;it could be even the county; or local level.Remember, guidance such as ISO/IEC,or NIST, cannot or will not supersede or overwrite governmental lawsand regulations that apply at any level, even local.

#### Exploring Organizations for Security Standards and Best Practices

Okay, in this brief lesson,I want to give you an idea of some of the different best practicesand guidance organizations for securitythat you should be aware of on this exam.Now, realize you won’t get any deep-dive questionson any of these, but, the security professional,the security practitioner, should be aware of these different organizationsand the fact that they’re very important forsecurity architecture, for security initiatives,for security governance, and driving the initiatives, and for best practices,and for steering that ship when it comes to security.So probably first and foremost is ISO,the International Organization of Standardization.Now, the reason why the acronym doesn't match up to what I just said isbecause ISO is actually in French, okay, that's where the acronym comes from.But you can go and you can find out about ISO- bringing global experts togetherto agree on the best way to do things, and they cover a wide variety.It's one of the oldest non-governmental international organizations,and it’s been enabling trade and cooperationwith people and companies since 1946.So, this is also where the OSI model comes from.You know, your seven-layer model?So the application, presentation, session, transport, Internet or network,and then data link, and physical, comes from the ISO.Another big one is the National Institute of Standards and Technology, NIST.In fact, a lot of the things that we look at as far as definitionson this exam come from NIST.A lot of organizations like CompTIA and ISC2and others like to use NIST for their information.So, founded in 1901, it’s part of the U.S. Department of Commerce,one of the oldest physical science laboratories.Everything- smart electric power grid,electronic health records to atomic clocks, nanomaterials, computer chips.In fact, I’d like to recommend to you that you go to this glossary-and you can see the link here, but you can just do a search for it-you know, the NIST glossary.This is an excellent resourcefor every security practitioner when they want to definesomething, you know, Access Control, for example,the process of granting or denying specific requests to one:obtain and use information or related systems or services,or two: enter specific physical facilities.Save it in your browser, okay?Because this is going to be an excellent resource for you,not just on the Security+ exam, but moving forward.Another organization that’s used for a widevariety of security needs and governance is the ISACA.They also have other certifications.CIOs and CISOs will often get the Certified Information Security Manager, CISM.They also have a Certified Information Security Auditor exam.So those are very popular.And if we go to ABOUT US,we can see that Information Security/ITfor over 50 years, a professional membership organization, skill sets,knowledge in auditing, cybersecurity, emerging technology and more.So that’s the ISACA.And here’s the CIS- Center for Internet Security.This is a very important organization.Their mission is to improve the overall security posture ofthe United States— State, Local, Tribal, Territorial, government organizations;coordination, collaboration, cooperation, and increased communication.They have the CIS top 18 Critical Security Controls.Now, this is not something to memorize, but it's definitely very important.And it used to be the top 20, but for your exam, it's the top 18.But while I’m here, let me just show you Control number 1 and number 2and make a very serious point as far as the CISand other governance is concerned or guidance: unless you have determined,evaluated, assessed the assets you have,so, for example, unless you have an inventoryand control of your enterprise's physical assets and an inventoryand control of your software and virtual assets,don't even bother with the remaining 16 controls.In other words, you have to know what you have before you can deal withthe risk: treat risk; have a risk management and risk assessment.So number 1 and number 2 is just simply, “you have to know what you have”.And so the top 18 is a very popular set of controls from the CIS,and that's something to be aware of as well.Now for cloud-based security which is more specific,because CIS is just basically anything on the Internet, but CSA is cloud.So, the Cloud Security Alliance,the world's leading organization to help defineand raise awareness of best practicesfor working in secure cloud computing environments.They have a STAR Program.So this is a registry that Software as a Service providersand other vendors will fill out a questionnaireand they will submit it to the STAR registry,and you, as a potential customer,can go and evaluate these different Softwareas a Service providers to see how they answered the questions.They also have their own certificates and their own training.They have a Certificate of Cloud Security Knowledge,a security of Auditing Knowledge, anda relatively new Certificate of Competence in Zero Trust (CCZT).Now something else they have-let me see if I can search for this- is the Cloud Controls Matrix.Now by no means do you have to memorize this for the exam,but realize, this is a matrix that is composedof 197 control objectives in 17 different domains.So this is used as a powerful auditing tool to look atthe maturity of your organization as you work withcloud providers. Or if you are a cloud provider-if you’re a Platform as a Service, Infrastructure, or Software asa Service provider, you can use this matrix yourselfto evaluate your own maturity, your own posture.And the CAIQ is actually the questionnaire that gets filled out,that gets put into the STAR registry.And you can do that too, to benefit, you know,working with your potential customers and vendors.And recently they actually have a new lite version,a streamlined version of this because, you know, there's a lot of controls.So this just streamlines it down to just 91 controls.For small businesses, right?Small to Medium-sized Businesses, small startup companies,things like that- CCM Lite.Another organization is OWASP.And OWASP is used also for securing websites.It's the Open Worldwide Application Security Project,a nonprofit foundation to help improve the security of software.And not just web applications, but mobile applications,Internet of Things (IoT), application programming interfaces,all different types of software.A community-led open-source project, OWASP.Very popular- they have OWASP cheat sheets that you can use tomanually configure your web application firewalls or your web security gateways.They have a threat modeling tool or a testing tool,a vulnerability assessment tool called OWASP ZAP, Z-A-P, that you canuse to test your websites and web applications. So that’s OWASP.The SANS Institute is a co-opfor information security to empower cybersecurityprofessionals with practical skills and knowledge.They have Webcasts, they have Free Cybersecurity Events,they have the Internet Storm Center.You can also go look at the security policy site,which is a really excellent place to get templatesfor your written security policy.So you don’t have to do this in a vacuum,you can just download the free templates, modify them the way you want them.And by the way, in an upcoming lesson in this course,we're going to talk about security policies.And you may want to come here and look at some of these templates.For example, for your acceptable use policy, mobile policy, remote accessuser policy, social media usage policies, things like that.Last but certainly not least is MITRE ATT&CK matrix,a globally accessible knowledge base of adversary tacticsand techniques based on real-world observations.A knowledge base which represents the foundation for threat modelingand methodologies in a wide variety of different sectors.So we come down here, we can see thatthis has its own different set of categories of attacks.And this can be used by blue teams,by penetration testers, by vulnerability assessment.So, it’s a repository for both red team and blue team.And you can see the different categories here on Reconnaissance; developingresources, credential harvesting, figuring out what the rolesand responsibilities are of the employees;you have your Initial Access, the most common being like phishing attacks,and you see Supply Chain Compromise there; there’s Execution;there’s Persistence, so surviving a boot or logon autostartor initialization script; Privilege Escalation or elevation; Defense Evasion;Credential Access like Credentials from Password Stores,brute force attacks, modifying the authentication process in an on-path attack;there’s Discovery or reconnaissance; and then Lateral Movement,a very important aspect of the kill chain.So, this is another very important area to be familiar with.Don’t worry, on the exam there’s no deep dive,but, as a security professional and a practitioner,these should be security guidanceand best practices that you should be familiar with.

#### Security Governance Standards and Policies

Now that we've taken a survey of a wide variety of sitesand organizations that can help provide guidance and guidelines as wellas best practices, let’s go ahead and define this.Guidelines provide a list of suggestions on howone can do things more effectively.In our case, how we can secure our assets more effectively.Guidelines and practices are like standards,but they're more flexible and typically not mandatory.Remember, guidelines and practices do not supersede local laws and regulations.Guidelines are used to define how standards should be developedor to guarantee adherence to an establishedsecurity policy or security governance.In the previous lesson, we looked at NIST, but we didn't talk about the NSA.The National Security Agency inthe U.S. does have security configuration guides specificallyfor military, government agencies, or organizationswho have contracts with the military or certain agencies.We also looked at the CIS top 18 in the previous lesson,but we didn’t talk about the CISA.But the Cybersecurity and Infrastructure Security Agency,CISA, has a set of best practices as well.And then we did look at the Cloud Security Alliance in the previous lesson.And remember they have their Cloud Controls Matrix, the CCM.Standards allow an information technologystaff to be consistent and systematic.Standards specify the use of specific technologies ina uniform way, because realistically,no one individual practitioner can know everything,and because of separation of duties,no single practitioner should have access and control of everything.Standards also help provide consistency in the enterprise,because it's unreasonable to support multiple versions of hardwareand software unless absolutely necessary.Standards are usually mandatory, and the most successfulIT organizations, the most mature, have standards,and they adhere to the standards to improve efficiency,but also keeping things as simple and direct as possible.Policies, specifically security policies,establish a general framework within which to workand guiding direction to take in the future.The function of a policy is to classifyguiding principles, to direct user, poweruser and administrative or root user behavior,as well as offering stakeholder guidance anda security control implementation roadmap.An information security policy is a directive that outlines howan enterprise plans on protecting its data, applications, services, and systems.Policies help ensure compliance with legal and regulatory requirements,as well as preserve an environment that sustains security principles.We call that security hygiene.Policy documents are high-level overview publications that guidethe way in which various controls and initiatives are implemented.In developing an effective security policy,there’s really six aspects or six characteristics: sanctioned, applicable,realistic, flexible, comprehensive, and enforced.Sanctioned- it means the policy has the supportof executive management or the steering committee.It requires visible participation and action;ongoing communication, having a champion on the C-suite of the C-team,somebody who’s on your side;investments in resources; and prioritization of the security policy.Applicable means it’s applicable to your organization,it’s pertinent; it’s relevant.Strategically, the information security policy mustsupport the guiding principles and goals of the organization.Tactically, it must be relevant to those who must comply.Now remember in the previous lesson, I showed you sans.org,and they actually have security policy templates that you can download.Remember, to have an effective security policy and for it to be applicable,you'll have to customize those templates to fit your organization,your use case, your value proposition.We don't just download templates and just use them in their existing state.Remember, the policy must be applicable to your organization.Realistic- the policy can be effectively executed.Policies must represent the actual environment in which they will be deployed.Again, modify those templates.Information security policiesand procedures should only express what is achievable.Don't set your goals too high.Remember, you have a finite amount of resources- money, practitioners, and time.If the policy is to advance the organization’s guiding principles,one can also assume that a positive outcome is anticipated.Therefore, a policy should never set up constituents, in other words,end users or principles for failure, but insteadshould offer a clear track, a clear path for success.Policies should be flexible and agile.The policy can accommodate change and be adapted if necessary.An adaptable or agile information security policy recognizes that informationsecurity is not a static, point-in-time endeavor, but rather it's organic.It's an ongoing process designed to support the organizational mission.It has to be flexible to respond to new technologies, changes in technology,changes in regulation and in security governance,or to react to competitors or vendors or disruptions in the supply chain.It has to be comprehensive.The policy scope must include all relevant parties.In other words, it's inclusive.An information security policy must consider the organization's objectives allthe way up to corporate governance;international law, for example; cultural norms of its employees and customers;business partners, suppliers, and customers; environmental impacts;and global cyber threats, to name a few.And finally, the information security policy must be enforceable.The policy is statutory and will be enforced.Enforceable means that administrative,physical, or technical controls can beand will be put in place to support this policy.How do we deal with those who violate acceptable use policies, for example?Compliance can be measured and, if necessary, appropriate sanctions applied.The enforcement stages should be well-documented:often beginning with a verbal reprimand, then a written warning,punitive actions, followed by a temporary suspension or permanent termination.And then finally, legal actions if necessary.Some examples of standards and their related policies would be incategories such as password policies; access control policies that aredriven by your architecture: discretionary, role-based,attribute-based or mandatory; physical security policies; encryption;general information security, protecting data at rest and data in transit;business continuity policies, for example, the disaster recovery policy;incident response, standards, and policies that supportthe incident response team;security policies that support the software development life cycleso that your environment is a DevSecOps environment;and a change management policy;and last but not least, the most popular policy, the Acceptable Use Policy.AUPs are considered one of the most important sections ofa written security policy.It identifies how employees are expected to use resources in the organization.It can define rules of behaviorand code of conduct in concert with human resources and legal.For example, using proper and acceptable language, avoiding illegal activities,avoid disturbing or disrupting other systems,do not reveal personal information or corporate intellectual property,do not reveal confidential information.Some sample categories: a mobile device policy for end users;a VPN or software-defined perimeter usage policy;how you can use operating systems and software; social media policies;removable media; and policies for generative AI, and machine learning;personal cloud storage policies; the clean desk policy-putting everything in a locked drawer orcabinet or locker at the end of the day.

#### Security Governance Procedures

Procedures- Procedures are usually requiredand are the lowest level of the policy chain.Procedure documents are typically longerand definitely more detailed thanstandards and guidelines or guidance documents.These include implementation details with step-by-stepinstructions, often accompanied by graphics and diagrams.Established practices are very importantfor helping especially large organizations achievethe consistency of deployment necessary to have a secure environment.Procedures are often called SOPs, Standard Operating Procedures,which are step-by-step instructions thatdefine how workers carry out their routine tasks.SOPs can greatly improve efficiency, quality, performance,communication, and compliance with regulations,which should probably be at the top of the list.Some things to consider with standard operating procedures would be: workersand end users need to know the purpose and the limits of the procedures.You must offer all of the steps needed to complete the process.If you've ever put together a piece of furniture or perhaps a bicycleor some other piece of equipment and the vendor leaves out steps, well,we know how frustrating that can be.You must clarify concepts and terminology, possibly with a glossary.Consider all health and safety issues.In the United States, we have OSHA, O-S-H-A, to deal with.And list the location of all necessary supplemental resources,including more detailed configurationguides on web servers or up on the Internet.The change management practice is also calledthe change control practice.The change control process reduces risk in security policy by deliveringa systematic approach to assess and manage all proposed and subsequent changes.These could be normal changes,such as regularly changing your password in Active Directory.That could be a standard change,for example, every 48 months being provisioneda new mobile device or a new laptop.Or it could be an emergency change,for example, when Microsoft BitLocker locks you out of your laptop ora mobile device is lost or stolen.Change management assures that changes are carefully assessedfor possible impacts on project scope, schedule, and resources,also allowing for informed decisions.Onboarding, also referred to as provisioning,refers to more than just bringing on a new employeewho we call a joiner and providing their assets.It also includes guidance, knowledge, skills,and training them on the behavior they need for associated job roles.Doing this through videos,possibly going through training delivered from your Intranet; printed material;computer-based training; lectures; formal and informal meetings;and hopefully finding a mentor in the organization.Onboarding includes introductions to their team members and direct supervisors,and explaining standards and practices,including their standard operating procedures,clearly define their role and responsibility.It involves provisioning all devices and equipment.And delivering security awareness,not just in the weeks or months after their newly hired or joining,but on an ongoing basis, because they must know theexpectations of the acceptable use policy,which is an organic document that can change based on new technology,new threats, new competitors, and a changing organizational environment.And onboarding and provisioning will includeadditional human resources activities.It’s critical that you remove any ambiguity or any uncertaintyfor the joiner or the mover in the onboarding process.The reverse of this would be offboarding or deprovisioning.One other point- onboardingand provisioning is typically much more automated than the offboardingor deprovisioning process.So let's talk about automating onboarding and other processes.Enterprises often deploy systems that involve self-service onboarding ofpersonal devices and even software,especially with so many remote employees and teleworkers today.These processes can be fully and semi-automated with runbooks and playbooks.For example, SIEM and SOAR systems are emerging solutions.The Joiner or Mover registers a new device andthe native supplicant is automatically provisioned for that user and deviceand installed using a supplicant profile that is preconfigured onthe server side, connecting the device to the corporate network.For example, in an 802.1X port-based network access control environment.The proper usage of various visibility tools will result in comprehensivemonitoring and proper revisions and improvements,not just of standard operating procedures, configurationand change management, and onboarding and offboarding,but for virtually every program and project in the enterprise.We’ll use SIEM systems; we can monitor using IDS and IPS;we can rely on application logs,for example, in a Windows 2022 server, system, security, and application logs;we get monitoring and visibility from firewall logs, both physical and virtual;SNMP traps and informs; NetFlow records;reports from database activity monitors for data loss prevention;and a wide variety of Software as a Service solutions in the cloud.

#### Course Summary

In this course, you learned about securitygovernance and governance structures;roles and responsibilities and considerations of external governance;and security governance guidance, best practices, standards,policies, and procedures.Coming up in the next course, we'll explore risk management.

### Risk Management

#### Course Overview

In this course we'll define risk management, identification, risk assessment,and risk analysis. We'll compare risk treatment and handling approaches.We'll explore risk registers and ledgers.Describe risk reporting and learn about the business impact analysis(BIA) aspect of business continuity.

#### Risk Management Defined

Let's start out with a short lesson to define risk management.Risk management is the continuous process of handling risksor threats to organizational operations,including and starting with mission-critical services and functions,physical and logical assets and let’s don’t forget people.The results of this risk management might be establishing the contextfor risk-related activities. For example the scope,is it a floor of a multi floor building? Is it a single building on a campus?Is it a certain zone like the data center or the server farm,or the call center, or the corporate land conducting an assetand risk assessment of logical and physical assets.Implementing a risk mitigation strategy which includes controls, administrative,technical and physical, based on your established risk treatmentor risk handling. Employing techniques and procedures forthe continuous monitoring of the security state of information systems,for example, a SIEM and SOAR system in your security operations center.Inherent or total risk consists of the vulnerabilitiesand risks that your organization faces right now.Your present posture, the state machine of your security initiatives before anynew or additional safeguards are implemented, this is the where are you now.The present baseline or system/application state before your formal assessmentbegins. Residual risk is the vulnerability or risk that remains afterthe mitigating controls are introduced.So residual risk = the inherent risk - your safeguards or controls.And one last point, the type of risk that we’re talking about here is not whatwe called positive risk. For example,if you go to a casino and you go to the blackjack table and you wager $10or betting on a sporting event, we call that positive risk.That is not the form of risk that we're talking about when we're doing riskmanagement for our organization.

#### Risk Identification and Assessment

Risk identification and assessment,according to the Center for Internet Security (CIS) Top 18 Controls,two initiatives must be conducted before the most critical assets, being dataand people can be protected.First, you have to have an Inventory and Control of all Enterprise Assets,specifically physical assets.And then secondly, an Inventory and Control of all of your softwareor logical virtual assets.These initiatives will contribute greatly to risk identificationand assessment activities.In other words, before the security practitioneror security manager can identify and assess risk,they must “know what they have”.You cannot protect what you don't know about.This involves actively tracking,labeling and robust inventory of all enterprise assets,including end-user devices, both stationary, portable, mobile,on-premises and off-site network infrastructure devices like layer 2 switches,multilayer switches, leaf and spine switches in the data center,routers and other devices. Security devices and appliancesfor example firewalls, IPS sensors, proxies, gateways,and email and web security appliances.Also hardware security modules, servers and hypervisors,non-computing devices and specialty IoT devices.Pretty much any physical component that's connected virtually, remotely,and to cloud environments.The security practitioner should accurately knowthe entirety of assets that need to be monitored and protected withinthe enterprise. The security practitioner must vigorously manage,which includes inventory, track, and repair,all operating system software and applications on all networks,production networks, management networks, storage area networks,and hypervisor networks, to name a fewso that only authorized software is installed and run.This includes virtual assets at cloud providers in infrastructure,platform, and Software as a Service deployments.Any unauthorized and unmanaged software must be locatedand prevented from installation or execution,that unauthorized software is often referred to as ghost or shadow IT,and it must be done in accordance with established policiesand tested procedures. Risk identification involves the qualitative,in other words, relative or on a particular scale or quantitative.In other words, mathematical evaluation beginning with the most probable risksand threats to mission-critical assets.The practitioner should determine the potential impact (magnitude)and likelihood (probability) against the mission-critical assets first.There's four major ways you can perform identification and assessment of risk.One would be continuous, this would be the optimal method.We would call that continual due care, continuous operational maintenanceor security hygiene. It could be done on an ad hoc basis or an as needed basis,it could be done on a recurring fashion, such as a schedule, weekly, monthly,quarterly, biannual or annually, fiscal year or calendar year.The least optimal method would be the one-time risk assessmentand risk identification. Why? Because so many things can change.New technology, new policies, new regulations, new competitors.Here's a nice diagram, you may want to pause thisand take some time to go through this at your own pace.This is from the Department ofHomeland Security in the United States, ready.gov.In the first box on the left,we see a wide variety of potential hazards.So, the organization has to decide which of these are the most likelyor probable and realize that things can change.For example, about two-thirds down the list, we see pandemic disease.Well, it's quite likely that because of the last five years or so,that has moved up higher on the list for a lot of organizations.Then after we identify our hazards,we'll perform vulnerability assessmentand we'll look at our assets that are at risk.For some organization, people are at the top of the list,but not all organizations consider people to be the most valuable asset.It may be property or the infrastructure,it may be the supply chain,it may be business operations or environmental aspectsand then the impact analysis.And there's basically primary and secondary loss,primary loss being the immediate loss such as a casualty to a personor property damage or business interruption due to a ransomware attack ora DDoS attack, that's a primary loss. A secondary loss is a subsequent lossbut realize the secondary loss can actually be more costly thanthe primary loss. For example, loss of confidence, your stock price going down,fines and penalties, or civil lawsuits.Those could be much more costly,although it's considered a secondary or subsequent loss.

#### Risk Analysis

In this lesson, we're going to look atthe fundamentals of risk analysis beginning with the five key elements.First, you must know your assets or your asset class to analyze.Secondly, under some particular incident or scenario,for example web servers and a fire or the CEO’s laptop with ransomware,a natural disaster affecting your site or facility.Third is some timeframe, maybe a fiscal/calendar year.Fourth is the impact or magnitude of the event,and fifth is the likelihood or probability,whether you're doing qualitative or quantitative analysis,which we'll look at here in a moment,you must have at least these five elements to analyze risk properly.The most established and historical methodfor risk analysis is qualitative risk analysis.This is probably the most common method used in risk and security.It's a descriptive approach using subjective opinions, historical events,and various scenarios to determine risk levels.To have successful qualitative risk analysis,you should have expert judgment either on-siteor through third-parties, established best practices and guidanceexperienced analysts, intuition, in other words, this is an art and a science.It often involves interviews, questionnaires, surveys, for example,the Delphi method, where you begin with anonymous surveysand then move from there,perhaps conducting brainstorming sessionsand ongoing workshops to address your assets,known risks, known vulnerabilities, performing gap analysis,and reviewing historical impacts and incidents.Generally, qualitative analysis will generate a heat map.In this heat map, let’s focus on the five key elements.First, this heat map is going to apply to an asset or an asset class,for example the hypervisors in your data center.Secondly, under a certain scenario, for example,a highly privileged user installing a rootkit or a remote access trojan inthe hypervisors, we call that hyperjacking. Third, a timeframe.It could be quarterly, it could be semiannual or most often a fiscalor annual year. And then the final two elements we see here in the diagramon the horizontal, we have impact or magnitude.On the vertical, we have likelihood or probability.Then we’re going to have a scale, we can have a scale of 1 to 5,it could be 1 to 8, 1 to 10,however you want to do your heat map, you want to use the same scale typicallyfor likelihood and for impact in this case 1 to 5. Then we have labelsand if we determine that the particular risk to our hypervisors is likelyand causing critical damage, then we would have high,which means we're going to focus our resources on protecting this asset basedon our risk analysis.Realize that a qualitative method is effectivegenerally from a more macro standpoint,but it is more subjective,and there is a little bit more guessworkand calibrated estimation that goes into qualitative.A more modern emerging method would be the quantitative risk analysis.This is a scientific and/or mathematical approach to getting monetaryand numeric results, often in the form of percentages based on asset values,the actual cost and depreciated value,the impact or severity of the incident expressed numerically.The probability or likelihood of occurrence expressed numerically,as opposed to just on a scale of 1 to 5 or 1 to 10.The threat frequency, in other words, how often is it likely to happen,or how often has it happened in the past?The cost and effectiveness of various safeguards, physical, tacticaladministrative. The resulting probabilities are based on percentages,mathematical formulas, for example, PERT and calibrated estimation,often driven by many Monte Carlo simulations.If you can't do quantitative,even a semi-quantitative approach would be preferable to purely qualitativeanalysis. On the exam, you should be aware ofthe classic analysis mechanism from Doctor Michael Whitman.First, you have the asset value or AV, this is the value of the assetaccording to the organization, this is easy to determine.It's often based on the cost of the asset or what types of revenuethe asset generates. Next is the EF (exposure factor),this is a percentage of asset loss caused by identified threats.The SLE is the single loss expectancy.The potential loss if attack occurs, expressed as a number,often a dollar value or other currency.The single loss expectancy (SLE) is derived as a product of the asset value,AV * exposure factor (EF),remember that for the exam. The ARO,the annualized rate of occurrence is the estimated frequencythe threat will occur within a single year twice, five times or more.The ultimate goal of Whitman’s model is to get the annualized loss expectancyfor a particular asset or asset classin a certain scenario. We know the timeframe, it’s annualizedhowever, is it a fiscal year or is it a calendar year,you have to know that, remember on the exam that the ALE is also a product,a product of the single loss expectancy * ARO,the annualized rate of occurrence.And so, for example, in your Whitman analysis,you would generate spreadsheets or database reporting based on different assetsand asset classes under different threats or scenarios.And then you can see we've got all of our values here.Asset value (AV), exposure factor (EF), single loss expectancy (SLE),the annualized rate of occurrence in a decimal format, and the ultimate goalthe annualized loss expectancy.Based on that, we will allocate resources to managing or treatingthe risk to our assets under various scenarios.

#### Risk Treatment and Handling

Whatever the type of risk analysis you do,whether it be qualitative or quantitativeor something in between like semi quant,it's all going to be driven by your risk treatment or risk handling approach.The first way to treat risk is to accept the risk,with risk acceptance you do not implement any additional safeguards.In other words, you don't introduce technical,administrative or physical controls to reduce residual risk.Often, justification in writing or to your steering committee or C-suiteor C team is demanded, they’ll want to know why you’re accepting the risk.This can also be a process of "ignoring" the risk for a couple of reasons.For example, your data center is not in a floodplain,or your offices are not in an area where there's tornadoes, hurricanes,or mudslides or it could be that the cost of the control far outweighsthe value of the asset, or the amount of revenue that the asset will generate.Examples of accepting risk would be only having one supplier or vendorfor hardware or services because you're relying upon their uptime reputation,their high availability, for example, a cloud service provider,or you lease a facility that’s in a 100-year flood zone and there hasn’t beena flood in 50 years, or you decide not to add cybersecurity riders to yourexisting business insurance policies because it's cost prohibitivefor your organization, or continuing with a (WPA2)-securedwireless local network,as opposed to upgrading hardware and firmware to support WPA3because it costs more than the value of the assets you’re protecting.Next, we have risk transfer.Risk transference is also referred to as risk sharing,this is passing off risk to a third-party or a shared party.Examples of risk transfer would be purchasing an insurance policy oran additional cyber insurance rider.Leveraging a shared responsibility model (SRM)with a cloud service providerfor example, using their infrastructure as opposed to your own data center,or leasing a warm/cold disaster recovery facility with another similarbusiness several miles away with a reciprocal agreement.Next is to avoid the risk.Risk avoidance involves deciding not to undertake actionsor engage in the activities that introduce or increase the risk.Remember though, being too risk-averse can lead to missing outon opportunities or new technological advantages.If your competitors for example, are willing to assume a little more risk,you may lose market share.Examples of avoidance would be not processingand storing credit card information of your customers on-premises,not using a cloud service provider for DevOps or managed data services.In other words, avoiding that risk by doing all of your software developmenton-site or avoiding the use of any clear-text protocols, such as HTTP, LDAP, FTPor SMTP, or not storing sensitive data in a personal cloud service suchas Dropbox or Google Drive.Remember, risk avoidance may also demand that you answer to decision makers,the steering committee, or your C-suite. Next, we have risk mitigation.This involves the strategic and tactical deployment of an array of technical,administrative and physical controls to reduce risk to an acceptable level.Notice that we're not saying eliminate risk,because from a practical standpoint,it's almost impossible to reduce risk or mitigate risk 100% .The only way to do that is through risk avoidance mechanisms.Enterprises will implement safeguards that will reduce risk exposure.So the risk may still exist,but to an acceptable level the impact is being reduced.Examples of mitigating risk would be implementing endpoint protection nextgeneration such as Palo Alto Cortex,upgrading your edge firewall appliance, using a cloud-based SIEMand SOAR system like Azure Sentinel,or a managed security service provider (MSSP) solution from Fortinet,or hiring armed security guards.Remember, risk treatment in handling may also be referred to as "risk appetite",and any combination of treatments can be used with risk management,there’s no one size fits all.You can use a different approach for a different asset or asset class.Analysts must also consider exemptions or exceptionsfor certain privileged users, air gapped systems,or special use case applications.There's three general approaches to risk handling.Keeping in mind that there are four methodologies,you can take an expansionary approach where the enterprise intends to increasethe number of resources to allocate to treat risk as needed on an ongoing basis.In other words, you're willing to expand your technical, administrative,and physical controls. A conservative method,the enterprise is frugal and extremely careful to spend more money,acquire controls, or add personnel unless absolutely necessary.They would rather find a compensating control or neutral.The enterprise will take a balanced approach to risk treatment.The appetites neither expansionary or conservative unless absolutely necessary.

#### Risk Registers and Ledgers

Risk assessment documents,these assessments will record the processes used to identify probable threatsand propose subsequent action plans if the hazard occurs.The document will declare assets at risk (people, buildings,information technology, utility systems, machinery,raw materials and finished goods, and more).There are many templates and prototypes of risk assessment documents online.They can be spreadsheet templates, word documents, PDFs, and more.These documents will be used to construct risk registers and ledgers.A risk register, also known as a ledger or a log,is a compilation of information related to vulnerabilities, risks,and countermeasures. It's a repository of identified risks, impact, scenarios,and potential responses. It can be populated from after-action reporting,lessons learned, case studies,and qualitative and quantitative risk analysis assessments.It's often represented as a table or a scatter plot derived from a spreadsheetor database view. This may also bean important tool to fulfill regulatory compliance.Here is an examplerisk ledger that’s shown as a matrix, this comes from Tim Casey at Intel.On the horizontal we have the incident or event typeand in this case it’s going to be a nonphysical attack.So for example accidental leak would be a data leak as opposed toa chemical leak. Then on the vertical you have different types of threat actorswith various levels of intent.Notice here, according to Casey,the one that checks off all the box is the hostile, disgruntled insider.It may be preferable to refer to this insider as a compromised insider,because they can be compromised for a wide variety of reasonsand not necessarily be disgruntled.Some other risk document concepts would be the fact that risk owners are oftenthe persons or entities responsible for managing threatsand vulnerabilities that might be exploited.This could be a chief information security officer (CISO), data custodian,a virtual asset manager, or some other technical risk stakeholder.Possibly someone whose role or responsibility is digital rights managementor information rights management.Key risk indicators (KRIs)are meaningful metrics for measuring the likelihood and impact of an incidentand if the results exceed established risk appetite.KRIs are commonly used in quantitative risk analysis and a risk threshold,another term to know for the exam.This is a quantifiable level of uncertainty and impact from risk,below which an organization will accept a risk and above the threshold,the organization will not accept a risk.

#### Risk Reporting

Risk reporting is not unlike other types of reporting,such as after action reports, after incident response or tests,or disaster recovery. Risk reports should have just as much informationas necessary, but never a "data overload" or an information dump onthe recipients. Reports should be concise and yet comprehensive,and that can be a fine balance.Using written reports and summaries, white papers and special publications,for example, NIST SPs reports published toan intranet to be consumed by your end users and stakeholders,live presentations, in-person or using conferencing.Analysts may need to express in simpler terms or have different reportsfor different target audiences,possibly including a glossary of terms for your CEOor your chief financial officer who don't necessarily have security expertiseor are very technical. Dashboards are very effective,and you can customize these using experienced Python and R language programmers.You should understand the optimal aspects of visual communications.Avoid three-dimensional representation, use a palette of sequential colors,consider possible color blindness and sight-impaired audiences.Avoid simple pie charts or histograms, and consider using scatterplots,bars and bubble charts, density plots, and boxplots.Here's an example of a scatterplot,a scatterplot will show the relationship between two different quantitativevariables for the same event or incident, systems, or people.One of the values appears on the horizontal axis,and the other variable is on the vertical axis,and each component appears as a point on the graph.A bubble charts, also called a bubble plotand this is basically extending the scatterplot,but looking at the relationship between variables, often numerical variables.So each dot in the bubble chart is a single data point,and the variables and the variables value for each point are indicated bythe horizontal position, the vertical position, and the dot size.Density plots represent a distribution of a numeric value.So it basically uses what's called kernel density estimates to showthe probability or the likelihood of a particular variable.So consider this a more smooth representation of a histogramand it uses the same concept.A boxplot is also called a whisker plot or a box diagram.This is basically a graph that shows a summary of a data set,so the shape of the boxplot shows how the data is distributed.this can also be represented as a candle.For example, if you're doing technical analysis of stocks or commoditiesor cryptocurrencies, where the bottom of the line representsthe lowest value during the trading session and the top of the line isthe highest value during the trading session.In the next lesson, we'll look ata common example of when these types of metrics and visualizations are usedfor reporting, and that is business impact analysis.

#### Business Impact Analysis

A core component of business continuity planningor continuity of operations is business impact analysis,and the BIA draws upon most of the things we've talked aboutso far in this course. Identifying risk,assessing risk, qualitatively and quantitatively,analyzing risk and through our risk register or ledger or databasecompiling metrics and key risk indicators.A BIA predicts the consequences of a disruption to a business and gathers data,collecting information necessary to develop recovery strategies.Specifically, a disaster recovery plan.Potential loss scenarios should be identified fromthe preexisting risk assessment.Other activities may include developing questionnaires,conducting continuing workshops, distributing surveys to stakeholders,and performing follow-ups to previous assessments and doing a gap analysis.Let's look at some key metrics of business impact analysis.The first one we have here is RTO, recovery time objectivealso notice the diagram on the right-hand side.In this use case, our backup strategy toan array from our servers in our data center or server farm.The Recovery Time Objective is the amount of time needed to recover a resource,a service, an application, or a function.In this example, to recover our server from our backups,the RTO must be less than or equal to the maximum tolerable downtime.MTD, also known as maximum allowable downtime.Any solutions must be completed within this time frame or it's consideredan unacceptable loss. If it is unacceptable,some ways to reduce the RTO to get them toan acceptable level would be to add physical security, add more redundancy,purchase insurance, invest in better generators or more robust backup arrays,for example, going from RAID 5 to RAID 6.Investing in faster recovery solutions,possibly upgrading the network or the storage area network. Next we have MTD,maximum tolerable downtime also maximum allowable downtime.This BIA metric or key risk indicator,represents the absolute maximum amount of time that a resource, service,or function can be unavailable before the entity starts to experiencea catastrophic loss. For example,what's the maximum time our voice over IP can be down?The maximum time our power can be out, for example,12 hours because that's how much diesel fuel we have for our generators.When the MTD is exceeded, that often triggers the disaster recovery plan.For example, if there's no more fuel for your generator,you have to move to a warm site, hot site, or cloud site to keep operational.Notice how closely related the MTD is to the recovery time objective.Next we have the Recovery Point Objective,the RPO is often represented as the target amount of time within whicha process must be restored after disruption.But although there's an element of time,the RPO is actually a point when something happens,some manual or automated task.The activity point, relative to a disaster, is really wherethe recovery process begins.It's the last known good configuration on your server,a database transaction log that you're going to recover from,which would apply to this diagram.Snapshots for example, virtual snapshots or virtual machinesor virtual hard disks possibly stored in cloud object storage likeGoogle Cloud Storage or Azure Blob. Recovery volumes,for example, on the CEO’s laptop,when was the last time the state machine was updated, state machine instances.Next, we have the mean time between failures, MTBF.MTBF is the measurement of the reliability of a hardware system, for example,a Cisco or Juniper router or router.It could be a component or it could be your hot spares.For example, in this diagram,it could be a solid state drive that goes into your RAID array.The MTBF data often comes from the Original Equipment Manufacturer, the OEM,but it could also come from retailers and distributorsor through some third-party consumer report.For example, the MTBF of a solid-state drive is usually rated inthe millions of hours, so an MTBF of1 million hours means that the average lifespan of a device is over 114 years.Industrial SSDs, often used in high-end RAID arrays in storage area networks,typically have ratings between 2 million hours (about 228 years)or 5 million hours or 570 years.And obviously, a solid state drive has a much longer MTBF thana traditional hard disk drive with plates and arms.MTTR, this is the Mean Time to Repair or the Mean Time to Replace.This meaningful metric determines how long it will take in minutes, hours,or days to repair and/or replace a failed system, a component, an application,or a service. The MTTR is often calculated for replacements and hot spares.This BIA measurement has and is heavily affected by supply chain disruptions,backorders, and the dislocation or even dissolution of vendors, wholesalers,and distributors, and it's typically a mathematical average value basedon experience and documentation.One formula would be the MTTR = (The total downtime)/(the number of breakdowns).

#### Course Summary

In this course, you learned about risk management, identification, assessment,analysis and risk treatment and handling, risk registers, ledgers,and risk reporting, and business impact analysis.Coming up in the next course, we'll explore security,compliance and third-party risk.

### Security Compliance & Third-Party Risk

#### Course Overview

Security Compliance and Third-Party Risk.In this course, we'll explore compliance monitoring and reporting,describe the consequences of non-compliance, consider privacy issues,examine vendor assessment and selection, and compare agreement types.

#### Compliance Monitoring

As we begin this course, let's define the word compliance.Compliance is defined as observing a rule, such as a policy, a standard,a specification, or a law.Regulatory compliance outlines the goals organizationsand commercial entities want to accomplishto certify that they understand and are takingthe proper actions to comply with policies, relevant laws, and regulations.For example, companies that provide products and services tothe U.S. federal government must meet certain security directives set by NIST.Specifically, NIST SP 800-53and Special Publication 800-171 are two common mandates with which companiesworking within the federal supply chain may need to comply.Compliance monitoring is a continuous process to ensure that all organizationalsubjects, otherwise known as principals,are adhering to all policies and procedures in the published policiesand procedures documents published to a Intranetor published in physical format, or both.The goals of compliance monitoring include: exposing compliance risk issues inan organization's operations or functionsas they deliver their value proposition, product, and/or service;helping organizations achieve consistent regulatory complianceand avoid areas of non-compliance.From the standpoint of a security practitioner or security manager,remember that compliance monitoring is often consideredan important part of security governance and the overall cybersecurity postureor the security maturity of your organization.Failure to conform with compliance requirements can result in severe finesand business disruptions,and we'll discuss some of those later on in this course.Some of the activities of compliance monitoring would be monitoringfor continuous certification and accreditation.For example, you can actually use compliance vulnerability assessment tools tomake sure that you can be certified and accredited next timethe governing body does an audit.You want to publish all compliance and regulatory requirements,both logical and physical.It involves tracking and recording all compliance and remediation initiativesor programs. Supporting a compliance manager, for example,or a digital asset manager,maybe somebody who's in charge of digital rights management to help themenforce Separation of Duty,or maybe a larger Zero Trust initiative.Or compliance monitoring may involve activities for someone with the role ofa data steward in some organizations.A couple of terms you want to know for the exam would be due diligenceand due care. And these are critical components to compliance and governance.Due diligence relates to the act of performing thorough research beforecommitting to a particular plan of action.It involves proper information gathering, planning, testing,scoping, and strategizing before the development, or production,and/or deployment. So, for example,doing comprehensive background checks before you hire somebody; investigatinga cloud service provider thoroughly before you sign that memorandum ofunderstanding (MOU) or memorandum of action (MOA); testingand evaluating your nonrepudiation techniques, for example,digital signing and digital signatures before you sign the contractsor before you utilize that code.Due care refers to the degree of attention that a reasonable person takesfor a particular entity.It's the level of judgment, attention,and ongoing activities thatan entity would engage in under similar circumstances.It involves all ongoing operational controls.Many organizations would rely on something like ITIL4continual improvement framework to optimize their due careto elevate them to a higher Capability Maturity Model (CMM) level.For example, level 3 to level 4.Another term for the exam is attestation. Complianceattestation is a formal validation document that is used to certifyan organization's status to interested external parties such as vendors,large customers, and potential strategic partners,or a candidate for a merger or acquisition.According to the ISO/IEC, attestation is the issue of a "statement" based ona decision that specific requirements or governance has been met.SOC 2, for an example, isan attestation report that offers in depth information and assurance aboutan entity's availability, processing integrity, confidentiality,and privacy controls.Another term is acknowledgment. Complianceacknowledgment typically involves a statement affirming thatan authorized enterprise understandsand will adhere to their confidentiality obligations anda security, privacy mandate such as Sarbanes-Oxley (SOX), HIPAA/HITECH, SOC 1/SOC 2, PCI DSS, GDPR,the Cloud Control Matrix (CCM), or other regulations and governance.And of course, it always helps to automate the monitoring of your compliance.Compliance processes are time-consuming,and when there's no automation involved,it quickly uses up productive hours. Hours that can be used to better deliverthe value proposition. A manual workflow could take around 150 hours,while an automated compliance tool may only need about 10-12 hours tocomplete. Compliance automation tools ensure that the protection of dataand other assets are governed according to the applicable regulations,such as GDPR. Tasks can include self-assessment,automating your planning and monitoring controls,automated testing, and reporting.Compliance automation tools can assist enterprisesto reduce non-compliance risk,improve efficiency, and attain better visibility,which is a critical aspect of the Zero Trust initiative.

#### Compliance Reporting

In this lesson, we'll takea brief moment to compare internal compliance reporting to external compliancereporting. Internal compliance reporting allows organizationsand commercial entities to institute internal controls, administrative,physical and technical and ongoing operational controls, to monitor employeebehavior to detect potential fraud or illegal activities,misconduct such as violations of the AUPor just simple non-compliant activities to adhere to regulatory requirements,maintain stakeholder trust, mitigate risk,support ethical considerations and corporate social responsibility (CSR),and establish internal governance and performance monitoring.External compliance, on the other hand, refers to following the rules,laws, and standards set by a Government entity.The primary goal of external compliance is to avoid any negative impact onthe organization such as fines, penalties, and the loss of corporate goodwill.In the next lesson, we'll actually look more specifically at some ofthe consequences of non-compliance.It refers to the state or province in which the firm is incorporated,or the organization is located,concerned with defining these compliances state by state, province by province,region by region. External compliance reportsand audits are reviewed by regulatory bodies for determining compliance status,certification, and/or accreditation.And these can vary widely per industry, per business sector, basedon different applicable regulations as well as geographical locations.

#### Consequences of Non-Compliance 

In this brief lesson, we want to be reminded ofthe various consequences of non-compliance: non-compliance to laws,non-compliance to regulations, non-compliance to mandates. First, we have fines.Realize that regulations such as the GDPR,which applies to the EU and their protectorates,and any organization that does business with the EU,can inflict multiple levels of fines for non-compliance.If we look at HIPAA in the United States going back to the 1990s,they have delivered a number of large fines toa wide variety of organizations who have not kept medicaland personal information private and secret.Often, if a company is subject to a criminal case,the result may not be jail time or prison time,but instead it will be fines or restitution.And then we have sanctions.A sanction can be placed on an organization ora business where they're unable to conduct activities,may be for a certain period of time, or conduct activities in a certain area,or the sanction just could be a threat of a penalty for disobeying a law ora particular rule or mandate.But generally speaking, a sanction is an official orderand it places some limitation: a limitation on trade,a limitation on contact for a certain period of time,or a limitation on doing business or delivering a certain value proposition,such as a product or service, usually for a finite amount of time.Non-compliance can lead to reputational damage.We call this a damage to goodwill.Over the course of time, a commercial entity or an organization will build upa reputation. It will build up goodwill with the public, with its customers,with vendors, with strategic partners, with its shareholders or bondholders.And if they fail to comply to laws, mandates, and regulations,this could be not only temporary damage,but it can be permanent damage to their reputation.A license is basically a permit or some certificate granted from some authority.It gives them permissions to own something or use something to conducta particular action or activity, or possibly carry on trade.A license is a permit that endorses an activity.It's an official permission, and there's many licenses throughout the world.If an organization is non-compliant,they can lose their license either temporarily or an extended period of time,or permanently. So, for example, in the United States,if somebody is convicted of driving under the influence and they do it,let's say three times, then they could permanently lose their driver license.And then finally, a consequence of non-compliance could be impacts on contracts.For example, when going back to a vendor after non-compliant,the vendor could charge more for the service.Or if the entity needs to borrow money from a bank or some other lender,they could be charged a higher interest ratebecause of their previous non-compliance.Also, the non-compliance could be a breach of the service-level agreementor the master service agreement.In fact, later on in this training,we'll talk about different types of contracts and agreements.And when we get there, remember that one of the parties,either in the business-to-business or customer-to-business relationship,if they're non-compliant,it can have an effect on the contract and it could lead to legal arbitrationor possibly lawsuits.

#### Privacy Considerations

In this lesson, we're going to look at some considerations when dealing withprivacy. And remember that confidentiality and privacy are closely related.However, privacy is really a subset of confidentialitybecause privacy really relates to keeping information involving peopleconfidential. So it's more specific.There are legal implications when implementing privacy initiatives.We have to consider local laws/regional laws, national, in some cases global,because we're going to be subject to global initiatives such as GDPR, ISO/IEC,and others. We know that violations of privacy can lead to various types oframifications: civil suits, administrative law,for example, if a lawyer breaches the client privileges,they could lose their law license, they could be disbarred.There's also criminal ramifications as well.We have to realize who is going to be in contact with the data.So certain data subjects, like a controller,will generally have more access to data than, let's say, a steward,because the controller's responsible often for confidentiality, integrity,non-repudiation, availability. Also, data processors.How do we protect privateand confidential information when someone's doing data processingor running batch jobs or doing data input?So obviously there's going to be non-disclosure agreements part of their AUP.But we may have to tokenize the data in certain high security environments.Who has ownership of this particular asset?In a discretionary access control model,you can be the owner of the spreadsheet or the word document or the PDF fileor even a database table.And when it's discretionary,the owner has the ability to assign permissions or views or shares of that data.Does that fit into your privacy model?The storage of data, data inventoryand the retention of that data in archiving.Obviously, we want to protect that data at rest with a cryptographic mechanism.And today we're going to gravitate towards AES-256.And again, if we're moving that data,data in transit will typically use some type of IPsec tunnelor we'll use Transport Layer Security 1.2 or higher.Another term on the exam,which really comes from the GDPR is the term right to be forgotten.Let's talk more about this one.According to the EU GDPR: "The data subject,in other words, the principal or the person, shall have the right to obtain fromthe controller the erasure of personal data concerning himor her without undue delayand the controller shall havethe obligation to erase personal data without undue delay if one ofa number of conditions applies."Practically speaking, "Undue delay" is typically about 30 days.Organizations must also take reasonable measures to validate the personrequesting erasure is truly who they say they are or the data subject.So, we would call that step-up authentication and authorizationor possibly proofing.The right to be forgotten merges with people's right to access their ownpersonal information, according to the GDPR.The right to control one's data is meaningless if people cannot act when theyno longer consent to processing,when there are significant errors within the data,or if they believe information is being stored unnecessarily.In these cases, an individual can request that the data be erased.Keep in mind this is not an absolute rightas the GDPR walks a fine line when it comes to data erasureand data disposition.

#### Vendor Assessment and Selection

In this lesson, we want to explore some ofthe techniques that you might use in assessing and selecting or choosinga vendor or a provider, or in some cases, a manufacturer,someone who's developing a software solution for you.It may even be a strategic partner.In some cases, you may be able to do penetration testing.This is uncommon for just ordinary vendors.If you're going to choose, let's say Amazon Web Services,you're not going to get to go doa penetration test in one of their data centers.But if it's part of a larger initiative like an acquisition or a merger,and you're evaluating their security controls,you may be able to do some level of pen testing on their organization beforethe merger is approved. When choosing a vendor in the service-level agreementor the master service agreement,you may or may not have a right-to-audit clause.So, for example, in a B2B or business-to-business relationship,you may be able to audit one of these vendors specifically if they're part ofa mission critical supply chain.You may be allowed to conduct the audit yourself,but more likely you're going to rely on a third-party objective auditor.For example, the vendor will have the audit performed,and they'll deliver the reports to you or the accreditation or certification.In other words, they're going to provide evidence that they've performed theirown internal audit, hopefully highly objectiveor evidence through accreditation ofan auditing firm like Pricewaterhouse or some other accountancy organization.This can also involve a wide variety of independent assessments if they chooseto adhere to PCI DSS because they process credit cards and bank cards,maybe an independent assessment of a cloud provider,and that could be through a questionnaire, for example,in the Cloud Security Alliance, which we'll look at here in a moment.And as mentioned, assessing vendors as part of your supply chainand using supply chain risk management techniquesand assessments to look at every link in the supply chain for both productsand/or services.And of course, the assessment and selectionis going to ultimately dictate which vendoror vendors that you choose. This is also a critical aspect of due diligence.Before you choose the vendor, doing all of your proper information gathering,reconnaissance, assessments, comparison,sometimes even before you enter the negotiation process for the contractsor the agreements. You also must identify any conflicts of interest between youand the potential vendor, and maybe your partners or your customers.At the very least, for a small to medium sized organization,you should rely on pre-established templates and questionnaires or surveys.You're going to stand on the shoulders of giants to make sure there's no gapsor missing pieces when you're assessing your vendors.You're going to rely on other organizations who've already done this before,possibly a number of times. And you're going to leverage that expertise.And the more you conduct the assessments, the more lessons learned you have.This will establish your rules of engagement as you move forward.Establish processes and procedures that you go through every timewhen you're in the lifecycle of choosing a vendor or a provider.So there's usually a pre-engagement meeting and well-defined processesfor assessment and selection.An example of doing this for, let's say,a cloud provider or a software as a service provider would be usingthe Cloud Security Alliance, Cloud Controls Matrix (CCM).The CCM is a cybersecurity control framework for cloud computing,and it aligns to the Cloud Security Alliance (CSA) best practices and guidance.It's considered the de-facto standard for cloud security and privacy.There's also an associated questionnaire that actually populates their "STAR"registry and it offers a set of "yes or no" questions to the potential provider,for example, the platform or software as a service provider based onthe security controls of the CCM.That way, you can look at how they answered the questions,and there's a lot of controls, 197 approximately.And you can compare your potential vendor or a software asa service provider based on their answers to the "yes no" questions acrossa wide variety of domains.You can download the CCM V4.There's also a CCM-Litefor smaller to medium sized organizations that don't want to use all 197controls. It gives you like one-third of the controls,but this is in a spreadsheet format,so it's going to have a listing of all of the controls.It will give you best practices and guidelines for implementing those controls.There's also guidelines for auditors.Remember the CSA also has a certificate for cloud auditors.So they can use those guidelines in evaluating the organization.It also maps to different things like ISO/IEC, NIST, and others.And then you can see here, there'sa Consensus Assessments Initiative Questionnaire thatthe potential vendor can fill out.And then they submit that to the CSA and it gets populated intothe "STAR" registry.

#### Agreement Types

Okay we're going to finish up this course witha survey of different types of agreements that you want to be aware of forthe Security+ exam. Some of these,just as part of your experience in the real-world,will be familiar to you because you've signed these on your own.For example, it's very likely you have signed an NDA ora nondisclosure agreement for one reason or another.This is also called a "confidentiality agreement".NDAs are legally enforceable contracts that generate a confidentialand/or private relationship between an entity,so these could be potential employee or contractor-to-management,they can be consumer-to-business, business-to-business,but the bottom line is there's sensitive information.And access is being granted to that sensitive or confidential information.A confidential relationship means one or both parties hasa duty not to share that information with unauthorized parties.These can be signed at the outset of a pre-engagement meeting.For example, you're going to hire a third-partypenetration testing or threat hunting team. May bean organization that's going to do an audit or a control assessment.It can be early in the interview process.It can be part of the hiring and even post-termination process.For example, if a group of employees are laid off,they may still be legally bound by the NDA. Orthe confidentiality agreements may be signed in anticipation of the next step,which may be a Memorandum of Agreement (MOA)or Memorandum of Understanding (MOU).Let's talk about those in greater detail.A Memorandum of agreement is a written document describing a cooperation ora potential contractual relationship between two entities that want to worktogether on a project or they have a mutual agreed-upon objective.It serves as a legal document that describes the details of a partnershipor potential partnership agreement.It's more formal than a verbal agreement,but it's actually less formal than the final contracts that are signed.Organizations can use this to establish and outline shared agreements,often early on in the negotiation process.An MOA may be used regardless of whether currency will be exchanged as part ofthe agreement.An MOU, memorandum of understanding isa nonbinding agreement that declares each party's objectives in performinga business transaction or initiating a new partnership or reciprocal agreement.This form of agreement is also referred to as a letter of intent(LOI) or often an MOA that we just talked about.For example, if a business is in the beginning phases ofa transaction with another party,the MOU is often the first step towards a formal agreement viaa binding contract. It openly defines how the parties will work togetherand what are the mutual expectations and responsibilities,often including nondisclosure.The goal of the MOU is to attain a mutual understanding, or a level of trust,so that both parties can move forward into an enforceable contract.For example, the vendor might like an MOU or an MOA withthe potential customer before they investa lot of their resources their programmers, their program managers,their project managers into delivering a solution,for example, a modified off-the-shelf software solution to the customer.A provider must realize that the use of contractual agreements suchas hosting agreements/connection agreements,reciprocal agreements and service-level agreements are used to allocate sharedresponsibility as well as risk among both providers and consumers.But this can also be a business-to-business or vendor-to-vendor agreementas well, for example, a vendor with subcontractors.A service-level agreement defines the precise responsibilities of the provider.It also helps to set customer expectations.It also clarifies the support system, for example,the service desk or help desk,the response to problems or outages for an agreed level of service(often based on a chosen support plan).The vendor could have multiple different plans in their portfolio,and each plan in the portfolio can have different SLAs.The liability for the failure of one or more controls andthe realization of risk can be appropriately documentedand understood by all involved parties when using a service-level agreement.An SLA is also called a master service agreement in some business sectors(or MSA). As part of due diligence in the business continuity plan(BCP), one should confirm any/all expectations withthe candidate service provider andensure that they're documented in your MSA/SLA.An MSA is a contract the two parties enter into duringa service transaction. And this agreement detailsthe expectations of both parties.The goal of an MSA is to make the contract process faster,often because you're relying upon pre-established or pre-cooked templates.That also makes future contract agreements simpler.These templates are often provided by your legal department orthe law firm that you're employing.A work order is a document that delivers all the information aboutan ongoing maintenance task or subproject,and outlines a process for completing that activity.Work orders can include details regarding who authorized the job, for example,a member of the C-suite or the C-team,or a steering committee or a supervisor. What's the scope of the work?Who the job or the task is assigned to? A custodian, a data processor,a steward, or an owner? What are all expectations?What's the delivery date and time expectation?Or through your project manager,what are your thresholds and multiphase delivery dates?Speaking of project managers,a term that they're familiar with is Statement of Work (or SOW).The SOW is an agreement that establishes the expectations for a project,program, or initiative and aligning the team or team members involved.Details should clarify price, or cost, timelines, deliverables,the actual process that's going to be used, the expectation of requirements,invoicing schedules, and much more.It depends upon the scope and the breadth of the project or the initiative.Basically, an SOW is a document of agreement between a client and a service,or possibly between some stakeholder and a project manager or some agent,like a cloud access security broker,defining the scope and details of a project.It's among the first documents that you'll use to establish the framework ofa project before entering into the planning and execution stages.Next is the BPA, the Business Partnership Agreement.A BPA establishes rules for two or more parties going intoa business agreement together.So this is not a client-server or consumer-to-vendor.This is B2B, business-to-business.It's a legally binding document that outlines every detail ofthe business operations, ownership stakes, financials, accountabilities,and even the decision-making approachand strategies that are being used by both businesses in the partnership.It's used by general partnerships, limited partnerships,limited liability partnerships, and limited liability limited partnerships.

#### Course Summary

In this course, you learned about compliance monitoring,reporting, and non-compliance handling; privacy issues,vendor assessment, and selection; and comparison of agreement types.Coming up in the next lesson, we'll explore audits, assessments, and awareness.

### Audits, Assessments, & Awareness

#### Course Overview

In this final course of Security+we’ll cover audits, assessments, and awareness.In this course we'll describe internal and external audit and attestation.Explore penetration testing. Define user guidance and training.Examine mock phishing campaigns and explain security trainingmonitoring and reporting.

#### Internal Audit and Attestation

Internal Audit and Attestation.An internal security audit operates by attesting that all organizationalinformation systems are adhering to a set of internalor external criteria regulating data security, network security,and overall infrastructure security, including applications.Internal criteria include the company's IT policies, procedures,and variety of security controls.Internal audit should objectively assess the organization's overall strategyfor handling emerging threats from a governance, architectural, operational,and granular technology standpoint.The Security Audit Committee is responsiblefor assisting independent auditors to examinethe organization's security reporting system ina process independent of management,by offering critical oversight of the corporation's reporting processes,internal controls, and their own independent auditing.Providing checks and balances for example,choosing certain supervisors or security managers who audit other divisionor other areas of the company internally, and vice versa,allowing a forum for discussing security concerns candidly and objectively.This audit committee could also be the steering committee,or it could be a subgroup of the steering committee providing opportunitiesfor workshops, brainstorming sessions, executive summaries,questionnaires and surveys and the like.An audit committee is typically appointed by the boardand is composed of directors who are not part of senior management orthe C-suite or C team. Duties of internal audit committees would be riskoversight, ethics and compliance, oversight of independent auditors,oversight of the internal audit process and managing controls and reporting.More than ever, today, organizations are relied upon to perform self-assessmentor self-assessment auditing.The self-assessment with independent validation, known as SAIV approach,is a more cost-effective assessment solution.The organization's internal audit activities leveragea capable, independent validator who is well-versed in security self-assessmentmethodology, using methods from The Center for Internet Security, CISor NIST, or other solutions.The goal is to deliver an independent validation ofthe internal audit activity's self-assessment.In addition to reviewing the self-assessment,the validator also confirms work completed by the self-assessment teamand interviews senior management.In the next lesson, we'll look at external audit assessment.

#### External Audit and Attestation

You may conclude from the previous lesson that internal auditand attestation would be the best approach, the least costly approach,for sure. However, it's not always the most objective approach,and for many organizations they don't have a choice.They must allow external auditing and attestation in order to be compliant,or to be accredited or certified. In an external audit,an organization often compares itself to some established standard.ISO 27001 is an example of a compliance audit with a certificationas the result. The Cloud Security Alliance, CSA certifies auditorsfor cloud security, and they use the Cloud Controls Matrix, CCM.The level for audits can be further segmented based onthe agreed-upon procedures that are involved in the scope of the audit.The scope of the audit may be simply forthe mobile solution, enterprise mobility management, your wireless guest VLAN,or your data center or call center.Let's compare audits to assessments because there is a difference.It’s a technical difference. But an assessment could be seen as an “audit plus”.So, it’s a more expanded audit.Assessments compare with both standards and industry practices along withthe auditor's knowledge and experience.For example, the PCI DSS has an audit,but organizations are required to go through a penetration test as well,which is an assessment. Therefore, PCI DSS can also be called an assessment,and audits are generally more specific and may have a particular scope.Just like you’re here right now to prepare for a security examination,CompTIA Security+, there's a wide variety of examinationsand certifications in this security industry.Security examinations are used to certify security professionals at variousexperience levels, for example to participate in the auditingand assessment process. Common examples of security examinations areCompTIA Security+, CompTIA Advanced Security Practitioner, CASP+,Certified Information Systems Security Professional,the CISSP from ISC² and the Certified Information Security Manager,CISM from ISACA, and this is by no means an exhaustive list,but this would be a common path.For example, for a Chief Information Officer ora Chief Information Security Officer who has five years or more experiencein the security field.External audits and assessments often rely upon independent third parties.The audit of information security is a comprehensive assessment that evaluates,often with gap analysis, the current state of security controls inthe organizations. The third parties will offer suggestionsand technological solutions,and the planning of timely actions to raise your level of difficultyor resistance to threat agents and their exploits and malware.When applied to DevSecOps, in other words,security throughout the entire development and operational lifecycle,a third-party security audit is an exhaustive assessment of all code,documentation, code repositories,and related processes to a software system by an independent security firm.The goal of an audit is to uncover potential security risks,which can then be patched by the software's developer.We call this software assurance.Once the independent third-party auditorfrom the pre-engagement meeting understandsthe scope of the audit and assessment, it will then evaluate all of the assets.It will understand the requirements for securing those assets andan analysis of the existing technical, physical,administrative or managerial and operational controls,and then do a deep dive into the development process, looking for gaps,weaknesses and vulnerabilities,and then reporting back to the organization with potential countermeasuresor additional controls to raise difficulty or resistance to threat actors.

#### Penetration Testing

One of the most common and critical activities ofa security assessment which could be part of an audit is penetration testing,otherwise known as pentesting.Pentesting is a process used to collect informationand actively expose vulnerabilities in a system, service,or application by actually conducting exploits,and what we call red team attacks.Penetration testing is conducted as a known environment previously knownas white box, partially known environment,previously called gray box oran unknown environment, previously called black box,where the tester assumes the attacker role to discover vulnerabilitiesand weaknesses. Pentesting can be launched against physical, technical,and/or logical controls.Penetration testing can also be usefulfor determining how well the system,application or service tolerates real-world-style attack patterns.The likely level of sophistication thatthe attacker needs to successfully compromise the system.Could it be done by a script kiddyor somebody who just watched a YouTube video?Or do they need advanced Python or Ruby skills?Determining additional countermeasures that could mitigate threats againstthe system. Understand the defender's ability, the blue team to detect attacksand respond appropriately.Some attributes of penetration testing would be the aforementioned: known,partially known, vs. unknown environment,and this is determined in the pre-engagement meeting.Is it credentialed or non-credentialed?In other words, are you giving the pen-testers actual user accountsor administrative accounts or root accounts to conduct their tests?Maybe they start with a guest user credentialand then try to elevate privileges or move laterally.Maybe you give them a privileged user credential from a power user allthe way up to an administrator or root user.Is the test offensive, meaning red team or threat hunters,or are you testing the blue team capabilities, the defensive capabilities,or what we call active defense.Or the test could be integrated with vulnerability assessments,incident response testing, and other decision-making initiatives, for example,an external or internal audit.It could be intrusive vs. non-intrusive.It could be non-intrusive or passive where it has no impact onthe end user's systems. It's just basically passive scanning.Or it could be intrusive where the tester actually leaves their mark,and it could have an effect on productivity.This could also be referred to as active intrusive vs. non-intrusive passive.Typically, a penetration test will have a life cycle.The first phase is the rules of engagement agreement.This is a meeting where the attackers determine the scope,they determine any areas that are off limits or exceptions or exemptions.Is there a bug bounty, are they going to get a cash bonus for findinga large vulnerability, or a huge gap in the security controls? Is it a no all,no some or no nothing type of attack?If it's a no nothing attack which we call opaque,then step two will be a lot longer and much more expensive because the lessthe attacker knows or the information provided inthe rules of engagement meeting,the more reconnaissance and information gathering that they must do.Step three is the privilege escalation.Once they get access to a system, service, or application,they'll try to elevate their privileges or move laterallyor pivot off of other systems that may be have open ports that are being sharedor trust relationships. Lateral movement and pivoting,trying to distribute throughout the VLAN, call center, corporate LAN,wireless VLAN, or data center.Persistence. Installing persistent codethat can evade antivirus and anti-malware tools.Stealthy polymorphic code that can move to RAM memory, or encrypt itself,or compress itself and move to lower levels of a directory.It's also persistent, and thatthe threat actor will continue to try different vectorsand different modes to be successful,and then typically they're going to cleanup during lateral movementand pivoting and persistence.They're probably deleting logs, disabling services.So, at the end of the test, they have to clean up their mess and go back toa recovery point to leave all the systems,applications and services in a state before they began the test.And this also involves making recommendations to the clientfor security solutions to improve their security posture and bea more mature organization,so that next time they'll be more resistant to assessments and tests.The good news is you don't have to do this in a vacuum.We can stand on the shoulders of giants by using existing frameworks.SSAF, from the Open Information Systems Security Group,a not-for-profit organization based in London,there's OSSTMM, open-source security testing by the Institute for Securityand Open Methodologies, ISECOM.OWASP, a popular methodology used widely by security professionals.It's a non-profit organization focused on advancing software security.Maybe you've heard of the OWASP Top 10.They have an OWASP Top 10 web vulnerabilities, API vulnerabilities,IoT vulnerabilities, as well as OWASP ZAP,a popular vulnerability assessment tool against websites and web services.PTES, the popular Penetration Testing Execution Standard,a methodology developed to cover the key aspects of pentesting, and of course,NIST providing a manual that is best suited to improvethe overall cybersecurity of an organization.

#### User Guidance and Training

Let's take a quick look at some of the topics you might cover in user guidanceand user training and awareness.Password policies and self-management of passwords,including the use of tokens and registering biometricsoften automated, awareness of all policy documents and handbooks,those that are published to the Intranet or in human resources sites,or in physical format.Situational awareness, for example how to respond when you see somebodyon your floor without a badge or somebody who's unfamiliar.Guidance on insider threats, for example,reporting to security guards or the security operations centeror supervisors when you're aware of unauthorized use or violations of an AUP.Letting employees know that management may use honey tokens or honey filesas a part of data loss prevention,or to expose illegal activities or violations of the AUP.Instructing employees and contractorson how to work in hybrid cloud environments and what are their expectationsas remote workers or teleworkers.Giving employees the awareness that you might be using user behavioral analysisand other machine learning tools to monitor their activities,both on site and remote, and awareness to different social engineeringcampaigns that may be launched, for example, a mock phishing campaign.Letting employees know what they can do with removable media and cablesand other hardware, and an overall postureand awareness of ongoing operational security withthe goal of continual improvement.

#### Phishing Campaigns

In this brief video, we're going to revisit the concept of the phishing attack, which, as we've established, is one of the most common vectors for delivering a malware payload or for tricking end users into clicking on hyperlinks that take them to drive by download sites or other farmed sites, or hoaxed sites to collect information or deliver malware. We're going to talk about this from the standpoint of security awareness and training, in the fact that most organizations are going to formulate a mock phishing campaign, in other words,to simulate a phishing attack against their own employees to enhance and increase security awareness. So, in this case, the attacker is actually your own security operations center or your own security team.It may be a penetration testing team that sends phishing email to the employees, often asking for them to verify their account. The corporate emails will be strategic and well crafted.They'll look just like a corporate email from somebody else within the organization, or one of your vendors or partners or customers. The end user clicks on the link in the email, which sends them to a first-stage website. This is actually a mock website that you can create, for example, in Rapid7 Metasploit, by simply just using a wizard-driven tool.The second stage will ask for some information,and then the third stage will dynamically generate a PHP script that's the most common. Then the user will fill in some information,offering some personally identifiable information or other intellectual property. If this were a real attacker, the information, the data would be stolen and possibly go through an intermediate bot to a command-and-control server, or to some campaign center on the dark web. A corporate phishing campaign is an email hoax designed to replicate a real attack against employees, both on site and teleworkers, as part of your security awareness training. This is a critical exercise. It's extremely common because cybercriminals use phishing to obtain sensitive information such as intellectual property, credentials, credit card details because they're spoofing a trustworthy organization or a reputable partner in an email communication.They may even create a fake employee within the organization that they're representing.These initiatives are used to support security training for new hires especially,but your ongoing anti-phishing awareness should be done for all of your stakeholders. The goal is not to entrap and punish employees, but rather raise awareness and provide instructionand guidance to prevent future successful advanced persistent threat campaigns.

#### Security Training, Monitoring, and Reporting

Security Training Monitoring and Reporting. Security training monitoring and reporting must be scoped to the specific audience to deliver different types of security training. For example, basic security awareness training for new hires or periodic training for existing stakeholders, technical security training, security management training, and compliance training. Regardless of the training modality. For example, brick-and-mortar, computer-based training, streaming webinars, or conferencing, to name a few, participants should be able to answer surveys and evaluations about all aspects of the experience.Participants should also be provided with an avenue for giving open-ended subjective feedback. The Net Promoter Score, NPS is considered the gold standard customer experience metric. In this context, the NPS score measures participant loyalty by looking at their probability of recommending a given security training experience. NPS scores are measured with a single-question survey and reported with a number ranging from -100 to +100, where a higher score is desirable. The NPS score evaluation would only be a part of the reporting process. Often, peer and supervisory evaluations should be performed to offer valuable critique and reinforcing feedback to the one delivering the training. This evaluation should also include the origin content in the graphical representations, test questions, and the various modalities of the training. All reporting best practices mentioned earlier in this Security+ training should be considered.

#### Course Summary

In the final course of Security+, we covered internal and external auditing, attestation, and penetration testing. User guidance, training, and mock phishing campaigns. And security training, monitoring, and reporting.

